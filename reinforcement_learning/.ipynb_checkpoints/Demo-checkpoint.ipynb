{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "TDL4T4160P-8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21905,
     "status": "ok",
     "timestamp": 1665042566743,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "TDL4T4160P-8",
    "outputId": "f5f0c89a-3413-4a14-b0ac-91798038dadf"
   },
   "outputs": [],
   "source": [
    " #!pip install gym[atari,accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Lab36Cm53hT3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1666077971727,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "Lab36Cm53hT3",
    "outputId": "685e3000-67fa-4bb3-e1ec-7a50773a17f0"
   },
   "outputs": [],
   "source": [
    "# !apt install xvfb\n",
    "# !pip install gym-notebook-wrapper\n",
    "# !pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1d5fad-136c-4376-b5b0-879077510380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 15:02:12.416849: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 15:02:13.502692: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-11-10 15:02:13.503304: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-11-10 15:02:13.562838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-10 15:02:13.563068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1050 Ti computeCapability: 6.1\n",
      "coreClock: 1.4175GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\n",
      "2022-11-10 15:02:13.563084: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-11-10 15:02:13.564465: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-11-10 15:02:13.564493: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-11-10 15:02:13.565716: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-11-10 15:02:13.565960: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-11-10 15:02:13.567321: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-11-10 15:02:13.568023: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-11-10 15:02:13.570878: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-11-10 15:02:13.570978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-10 15:02:13.571237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-10 15:02:13.571476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "# Check that tensor flow is able to use the GPU\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84dfbf-263e-4d9a-ad88-2eaf271e4421",
   "metadata": {
    "id": "ae84dfbf-263e-4d9a-ad88-2eaf271e4421"
   },
   "source": [
    "# Dueling Network Architecture Implementation\n",
    "The Duelling network is an artificial neural network architecture that has improved the state of the art in the DQN area used in combination with Dual DQN and Prioritized Experience Replay. This approach splits the action value calculation using a combination of state value function and advantage function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc578666-0c50-4681-8b90-bc48ece391d6",
   "metadata": {
    "id": "fc578666-0c50-4681-8b90-bc48ece391d6"
   },
   "source": [
    "# Searching for available environments\n",
    "We want to test the performance of our architecture with the Atari game 'Asterix'.\n",
    "Here we check which kind of versions of this game are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d78d38-4889-44d7-a8cd-49d3c9059bad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1539,
     "status": "ok",
     "timestamp": 1665042568266,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "82d78d38-4889-44d7-a8cd-49d3c9059bad",
    "outputId": "9d44fd58-f58f-4b95-cfa9-f00f7d2a04e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALE/Asterix-ram-v5\n",
      "ALE/Asterix-v5\n",
      "Asterix-ram-v0\n",
      "Asterix-ram-v4\n",
      "Asterix-ramDeterministic-v0\n",
      "Asterix-ramDeterministic-v4\n",
      "Asterix-ramNoFrameskip-v0\n",
      "Asterix-ramNoFrameskip-v4\n",
      "Asterix-v0\n",
      "Asterix-v4\n",
      "AsterixDeterministic-v0\n",
      "AsterixDeterministic-v4\n",
      "AsterixNoFrameskip-v0\n",
      "AsterixNoFrameskip-v4\n"
     ]
    }
   ],
   "source": [
    "from gym import envs\n",
    "\n",
    "# Searching for available environments\n",
    "game_name = \"Asterix\"\n",
    "all_envs = envs.registry.values()\n",
    "env_ids = [env_spec.id for env_spec in all_envs]\n",
    "\n",
    "for id in sorted(env_ids):\n",
    "    if game_name in id:\n",
    "        print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5b1aa-6e92-4ac3-9079-c49121fafaef",
   "metadata": {
    "id": "12c5b1aa-6e92-4ac3-9079-c49121fafaef"
   },
   "source": [
    "# Environment Configuration\n",
    "We select the version 4 of the environment with no frame skipping and select as render mode human. The no frame skipping is used to make this environment compatible with the optimization made by *AtariPreprocessing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cecf335-3193-4ce8-90f5-dfe1c3a82e5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1665042568266,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "3cecf335-3193-4ce8-90f5-dfe1c3a82e5f",
    "outputId": "28ea8b12-3721-41e2-a222-156e9e23bd33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.wrappers import AtariPreprocessing\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# Make Parameters:\n",
    "game_name = \"Asterix\"\n",
    "game_mode = \"NoFrameskip\"  # [Deterministic | NoFrameskip | ram | ramDeterministic | ramNoFrameskip ]\n",
    "game_version = \"v4\"  # [v0 | v4 | v5]\n",
    "env_name = '{}{}-{}'.format(game_name, game_mode, game_version)\n",
    "env_render_mode = 'rgb_array'  # [human | rgb_array]\n",
    "env_frame_skip = 4\n",
    "\n",
    "env = gym.make(env_name, render_mode=env_render_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23b68bf0-d09a-432b-9fe9-9991f0d8a84a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1665042568267,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "23b68bf0-d09a-432b-9fe9-9991f0d8a84a",
    "outputId": "1921b05a-a9fa-47f2-9246-26adafe63578"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/core.py:51: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:297: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEoAAAGhCAYAAADGJCKEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAQtElEQVR4nO3dX0zV9R/H8deBczggQxScHk4GQnOiYqVgpjKzYbTl3zUz8x+uLnSJYphKs6a5Cfkn9bdIHMypzRldiGZlKSWCLksFSZESmwxRYUxHRxHl33n/LtTvPIL2Jg8C+nps54Lv9+Phy3OHA7jzPh+TiAjoX3m09wV0FgylxFBKDKXEUEoMpcRQSgylxFBKDKXUrqE2bdqE0NBQeHt7IzIyEocPH27Py3k4aSeZmZlisVgkIyNDiouLJSEhQXx9faWsrKy9LumhTCLt80fxsGHDMGTIEKSlpRnH+vfvj0mTJiElJeWh/9bpdOLy5cvw8/ODyWRyOSciuH79Oux2Ozw83PcNY3bbPbVCfX098vPzkZSU5HI8NjYWv/76a7P1dXV1qKurMz6+dOkSBgwY8NDPUV5ejt69e7vngtFOoa5cuYKmpib06tXL5XivXr1QWVnZbH1KSgo+/fTTZsej8QbMsLgca0QDjmAf/Pz83HrN7fpk3tK3zf3HAOCjjz6Cw+EwbuXl5QAAMywwm+673QnX0v08inZ5RPXo0QOenp7NHj1VVVXNHmUAYLVaYbVaH9fltahdHlFeXl6IjIxEdna2y/Hs7GyMGDGiPS7pX7XLIwoAEhMTMXPmTERFRWH48OFIT0/HhQsXMHfu3Pa6pIdqt1Bvv/02rl69ipUrV6KiogIRERHYt28fQkJC2uuSHqrdfo96FNeuXYO/vz9GYyLMpvt+6kkDDuFbOBwOdO3a1W2fk3/rKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQym5PVRKSgqGDh0KPz8/9OzZE5MmTcLZs2dd1ogIVqxYAbvdDh8fH4wePRpnzpxx96W4ldtD5ebmYt68efjtt9+QnZ2NxsZGxMbG4saNG8aaNWvWYP369UhNTcXx48dhs9nw2muv4fr16+6+HPdp6/GtqqoqASC5ubkiIuJ0OsVms8lnn31mrLl165b4+/vL5s2bVffpcDgEgIzGRBljmuxyG42JAkAcDodbv442f45yOBwAgICAAABAaWkpKisrERsba6yxWq145ZVXWpysAm5PV127ds3l9ri1aSgRQWJiIqKjoxEREQEAxvyLdrIKuP285+/vb9yeffbZtrzsFrVpqPj4eJw6dQpff/11s3PaySrgwdNVj1ObjXjMnz8fe/fuRV5ensuAoc1mA3D7kRUUFGQcf9BkFfCETleJCOLj45GVlYWDBw8iNDTU5XxoaChsNpvLZFV9fT1yc3M77GQV0AaPqHnz5mHnzp349ttv4efnZzzv+Pv7w8fHByaTCQsXLkRycjL69u2Lvn37Ijk5GV26dMG0adPcfTlu4/ZQd4epR48e7XJ869atmD17NgBgyZIluHnzJt5//31UV1dj2LBhOHDggNvHW92J01VK/FtPiaGUGEqJoZQYSomhlBhKiaGUGEqJoZQYSomhlBhKiaGUGEqJoZQYSomhlBhKiaGUGEqJoZQYSomhlBhKiaGUGEqJoZQYSomhlBhKiaGUGEqJoZQYSqnNQ6WkpBivBL5LOF3l6vjx40hPT8fzzz/vcrwzTle1WaiamhpMnz4dGRkZ6N69u3FcRLBx40YsW7YMb775JiIiIrB9+3bU1tZi586dbXU5j6zNQs2bNw9jx47FmDFjXI531umqNpmFyczMREFBAY4fP97s3MOmq8rKylq8vwftXfU4uf0RVV5ejoSEBOzYsQPe3t4PXPfUT1fl5+ejqqoKkZGRxrGmpibk5eUhNTXVmC9+6qerYmJicPr0aRQWFhq3qKgoTJ8+HYWFhQgLC+N0FQD4+fkZ0553+fr6IjAw0DjO6SolTlc9Jpyu6sAYSomhlBhKiaGUGEqJoZQYSomhlBhKiaGUGEqJoZQYSomhlBhKiaGUGEqJoZQYSomhlBhKiaGUGEqJoZQYSomhlBhKiaGUGEqJoZQYSomhlBhKiaGUGEqpTUJdunQJM2bMQGBgILp06YIXX3wR+fn5xnmOoQGorq7GyJEjYbFY8OOPP6K4uBiff/45unXrZqzpjGNobt/ka+nSpRIdHf3A8/9lk69bt26Jw+EwbuXl5Z1/k6+9e/ciKioKb731Fnr27InBgwcjIyPDOP9fxtCeyL2rzp8/j7S0NPTt2xf79+/H3LlzsWDBAnz11VcA/tsmX0/kdJXT6URUVBSSk5MBAIMHD8aZM2eQlpaGWbNmGetaM4b2RE5XBQUFYcCAAS7H+vfvjwsXLgBw3eTrXg8bQ+sI3B5q5MiRzfb8LCkpQUhICABu8mX44IMPMGLECCQnJ2PKlCk4duwY0tPTkZ6eDgCddpOvNtkD9LvvvpOIiAixWq0SHh4u6enpLuedTqcsX75cbDabWK1WGTVqlJw+fVp9/+2xByjH0JT4t54SQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUkttDNTY24uOPP0ZoaCh8fHwQFhaGlStXwul0GmuE01XA6tWrsXnzZqSmpuLPP//EmjVrsHbtWnzxxRfGms44XeX2UEePHsXEiRMxduxY9OnTB5MnT0ZsbCxOnDgBgJt8GaKjo/HLL7+gpKQEAPDHH3/gyJEjeOONNwBwky/D0qVL4XA4EB4eDk9PTzQ1NWHVqlV45513AHCTL8M333yDHTt2YOfOnSgoKMD27duxbt06bN++3WXdU7/J1+LFi5GUlISpU6cCAAYNGoSysjKkpKQgLi7OZbrqqd7kq7a2Fh4ernfr6elp/HrA6ao7xo8fj1WrViE4OBgDBw7EyZMnsX79erz77rsAOF1luHbtmiQkJEhwcLB4e3tLWFiYLFu2TOrq6ow1nK56TDhd1YExlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMptTpUXl4exo8fD7vdDpPJhD179ricF8XkVF1dHebPn48ePXrA19cXEyZMwMWLFx/pC2lrrQ5148YNvPDCC0hNTW3xvGZyauHChdi9ezcyMzNx5MgR1NTUYNy4cWhqavrvX0lbe5SXFAOQ3bt3Gx9r9qX6559/xGKxSGZmprHm0qVL4uHhIT/99JPq87bHy6fd+hylmZzKz89HQ0ODyxq73Y6IiIgOPV3l1lCafakqKyvh5eWF7t27P3DN/Z7ITb6A1k1OadZ0hOkqt4bS7Etls9lQX1+P6urqB665n9VqRdeuXV1uj5tbQ2kmpyIjI2GxWFzWVFRUoKio6MmarqqpqcHff/9tfFxaWorCwkIEBAQgODj4Xyen/P398d5772HRokUIDAxEQEAAPvzwQwwaNAhjxoxx31fmbq39MZmTkyMAmt3i4uJERDc5dfPmTYmPj5eAgADx8fGRcePGyYULF9TXwOkqJU5XdWAMpcRQSgylxFBKDKXEUEoMpcRQSgylxFBKDKXEUEoMpcRQSgylxFBKDKXEUEoMpcRQSgylxFBKDKXEUEoMpcRQSgylxFBKDKXEUEoMpcRQSgylxFBKbp2uamhowNKlSzFo0CD4+vrCbrdj1qxZuHz5sst9PPXTVbW1tSgoKMAnn3yCgoICZGVloaSkBBMmTHBZ99RPV7Xk2LFjAkDKyspEhNNVD+RwOGAymdCtWzcAnK5q0a1bt5CUlIRp06YZr/nmdNV9GhoaMHXqVDidTmzatOlf18vTNF11V0NDA6ZMmYLS0lJkZ2e7TBBwuuqOu5HOnTuHn3/+GYGBgS7nOV0VEAC73Y7JkyejoKAA33//PZqamoznnYCAAHh5eXG6Ki4uTkpLS1s8B0BycnKM++B01WPC6aoOjKGUGEqJoZQYSomhlBhKiaGUGEqJoZQYSomhlBhKiaGUGEqJoZQYSomhlBhKiaGUGEqJoZQYSomhlBhKiaGUGEqJoZQYSomhlBhKiaGUGEqJoZQYSsnte1fda86cOTCZTNi4caPL8ad+uupee/bswe+//w673d7sHKer7rh48aI888wzUlRUJCEhIbJhwwbjHKer7nA6nZg5cyYWL16MgQMHNjvP6ao7Vq9eDbPZjAULFrR4ntNVuP1o+d///odt27b9615V95Onabrq8OHDqKqqQnBwMMxmM8xmM8rKyrBo0SL06dMHAKerAAAzZ87EqVOnUFhYaNzsdjsWL16M/fv3A+B0lbF31f1jZxaLBTabDf369QPQefeuanWoEydO4NVXXzU+TkxMBADExcVh27ZtqvvYsGEDzGYzpkyZgps3byImJgbbtm2Dp6dnay/nseF0lRL/1lNiKCWGUmIoJYZSYiglhlJiKCWGUmIoJYZSYiglhlJiKCWGUmIoJYZSYiglhlJiKCWGUmIoJYZSYiglhlJiKCWGUmIoJYZSYiglhlJiKCWGUmIoJYZSYiglhlJq9cunO4K7L2RuRMPtrS/u0YgGlzXu0ilDXb9+HQBwBPseuObq1avw9/d32+fslK8zdzqduHz5MkQEwcHBKC8vN15T7nA4EBwcjOrqamNjMXfolI8oDw8P9O7d25jba2mQyMPDvU+/fDJXYiilTh3KarVi+fLlsFqtDz3mDp3yybw9dOpH1OPEUEoMpcRQSgyl1KFDbdq0CaGhofD29kZkZCTmzJmDoUOHws/PDz179sSkSZNw9uxZY31ubi4iIyPh7e2NsLAwbN68GYcOHYLJZGp2++uvv1p3MW59AxM3yszMFIvFIhkZGVJcXCwJCQni6ekp69atk6KiIiksLJSxY8dKcHCw1NTUyPnz56VLly6SkJAgxcXFkpGRIRaLRVasWCEA5OzZs1JRUWHcGhsbW3U9HTbUSy+9JHPnznU5Fh4eLklJScbHVVVVAkByc3NlyZIlEh4e7rJ+zpw50r9/fwEg1dXVj3Q9HfJbr76+Hvn5+S5vdAMAsbGxLm9043A4ANzesfbo0aPN1r/++uvGt+bgwYMRFBSEmJgY5OTktPqaOmSoK1euoKmpqdlbkNz7RjcigsTERERHRyMiIgKVlZUtrnc6nVi7di127dqFrKws9OvXDzExMcjLy2vVNXXo/2a5/01t5J43uomPj8epU6dw5MiRh64HgBkzZsBmswEAhg8fjvLycqxbtw6jRo1SX0uHfET16NEDnp6ezd4m6e4b3cyfPx979+5FTk4OevfuDeD2m+O0tN5sNjd7G5SXX34Z586da9U1dchQXl5eiIyMdHmjGwA4cOAAampqkJWVhYMHDyI0NNQ4N3z48BbXR0VFwWJxfbeNkydPIigoqHUX9Ug/CtrQ3V8PtmzZIsXFxbJw4UIxm83i5+cnhw4dkvj4eJk8ebJUVFRIbW2t8evB0KFDZcKECbJlyxaxWCwye/Zs2b17t5SUlEhRUZEkJSUJANm1a1errqfDhhIR+fLLLyUkJES8vLxkyJAhD9x+fOvWrSIicujQIQkICBCTySR9+vSRtLQ0Wb16tTz33HPi7e0t3bt3l+joaPnhhx9afS38/yilDvkc1RExlBJDKTGUEkMpMZQSQykxlBJDKTGUEkMp/R99wmXWXcVFtwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env.reset() \n",
    "state = env.render(mode=\"rgb_array\")\n",
    "name_fig_prep = \"Asterix_start.png\"\n",
    "env.step(0)\n",
    "env.step(0)\n",
    "env.step(0)\n",
    "state = env.step(0)\n",
    "plt.imshow(state[0])\n",
    "plt.plot()\n",
    "plt.savefig(name_fig_prep, format='png', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04326819-e929-40fe-9a26-ecfc317f7882",
   "metadata": {
    "id": "04326819-e929-40fe-9a26-ecfc317f7882"
   },
   "source": [
    "## Enviornment Observations\n",
    "Below we have a first look at the environment characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "143cf671-a4e9-46a5-b06c-5f66f83fdc22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1665042568267,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "143cf671-a4e9-46a5-b06c-5f66f83fdc22",
    "outputId": "ffd6528b-5a7d-4fbf-d9e5-c4aad9150b7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "401a5ee2-bf0f-4721-be41-32c498caf4f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1665042568268,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "401a5ee2-bf0f-4721-be41-32c498caf4f3",
    "outputId": "ef5b5156-1509-47c2-98e8-2ffa96ea7495"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c5513e8-9f39-47bd-b667-a7436383795e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1665042568268,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "2c5513e8-9f39-47bd-b667-a7436383795e",
    "outputId": "f6792f4c-9ef4-485e-ef92-4875a0e28914"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP',\n",
       " 'UP',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'DOWN',\n",
       " 'UPRIGHT',\n",
       " 'UPLEFT',\n",
       " 'DOWNRIGHT',\n",
       " 'DOWNLEFT']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad4449-5a13-4032-9645-53f78265617b",
   "metadata": {
    "id": "39ad4449-5a13-4032-9645-53f78265617b"
   },
   "source": [
    "## Environment Optimization\n",
    "We optimize the environment by adding the frame skipping, changing its observation in greyscale and following the experiment done in the paper we set to at most 30 the no-op actions; to get this we use the AtariPreprocessing wrapper.\n",
    "We use Framestack to create observations of 4 frames to give the idea of movement to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63a89f89-355e-407a-a036-d1d14bfe3fba",
   "metadata": {
    "id": "63a89f89-355e-407a-a036-d1d14bfe3fba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = AtariPreprocessing(env, frame_skip=env_frame_skip, grayscale_obs=True, terminal_on_life_loss=True, noop_max=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43876bf8-8f34-4118-990a-dba1bbbc67fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "executionInfo": {
     "elapsed": 894,
     "status": "ok",
     "timestamp": 1665042569152,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "43876bf8-8f34-4118-990a-dba1bbbc67fd",
    "outputId": "bead8004-41c7-4082-d536-f1d6bce913c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name_fig_prep = \"Asterix_Preprocessed.png\"\n",
    "env = FrameStack(env, 4)\n",
    "state = env.reset()\n",
    "plt.imshow(state[0])\n",
    "plt.plot()\n",
    "plt.savefig(name_fig_prep, format='png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c6620c-52ee-48de-bc6b-4caba64b8f1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1665042569153,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "51c6620c-52ee-48de-bc6b-4caba64b8f1f",
    "outputId": "4c23526d-3fe5-4795-bba8-e84ad50f9222"
   },
   "outputs": [],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0735fbbc-11c3-418f-8026-f3eba9dc5cb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1665042569153,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "0735fbbc-11c3-418f-8026-f3eba9dc5cb9",
    "outputId": "ce6cfa59-1f25-4917-d5c2-b5aa4a49c4c1"
   },
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dccc025-b2ae-48c8-a70d-0e146d5c9d7c",
   "metadata": {
    "id": "9dccc025-b2ae-48c8-a70d-0e146d5c9d7c"
   },
   "source": [
    "# Network configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579e3d9-51f6-4612-bf77-25db75a98d4a",
   "metadata": {
    "id": "e579e3d9-51f6-4612-bf77-25db75a98d4a"
   },
   "source": [
    "## Policy\n",
    "The policy is the component that chooses the action to perform; using an $\\epsilon$-gready policy the action chosen can be random with probability $\\epsilon$ or an action suggested by the ANN with probability $1 - \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3213c2-3be1-4337-bf89-5a32f9dbf2c8",
   "metadata": {
    "id": "7c3213c2-3be1-4337-bf89-5a32f9dbf2c8"
   },
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class EpsilonGreedyPolicy:\n",
    "\n",
    "    def __init__(self, model, action_space_size, episodes=1, min_epsilon=0, decay_rate=1.35):\n",
    "        self.model = model\n",
    "        self.action_space_size = action_space_size\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.episode = 1\n",
    "        self.episodes = episodes\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "    def get_action(self, state):\n",
    "        epsilon_decay = (self.episode / self.episodes)*self.decay_rate\n",
    "        epsilon = max(1 - epsilon_decay, self.min_epsilon)\n",
    "        rnd = random.random()\n",
    "        # print(random, epsilon)\n",
    "        if rnd < epsilon:\n",
    "            action = random.randint(self.action_space_size)\n",
    "            return action\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = self.model(state_tensor)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "            return action\n",
    "\n",
    "    def next_episode(self):\n",
    "        self.episode += 1\n",
    "\n",
    "    def reset_episodes(self):\n",
    "        self.episode = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c4fef-7fd8-4e38-9eac-b7623174d588",
   "metadata": {
    "id": "255c4fef-7fd8-4e38-9eac-b7623174d588"
   },
   "source": [
    "## Replication Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f6dd9-3076-4c01-bf7f-01b9400c6fc2",
   "metadata": {
    "id": "6d9f6dd9-3076-4c01-bf7f-01b9400c6fc2"
   },
   "source": [
    "### Prioritized experience replay\n",
    "The Prioritized experience replay was introduced in the paper \"Prioritized experience replay\" (https://arxiv.org/abs/1511.05952), it consists in an evolution of the replay buffer that orders the experiences to replay by priority. In this experiment we adopt the **rank-based** variant where the experience sampling from the buffer it's done with probability $ P(i) = \\frac{p_{i}^{\\alpha}}{\\sum_k{p_{k}^{\\alpha}}} $ and $p(i)=\\frac{1}{rank(i)}$ where $rank(i)$ is the rank of the transition *i*, $\\alpha$ is called **priority exponent**. It is necessary to compute the importance sampling weights as $w_j = \\frac{(N * P(j))^{-\\beta}}{max_i{w_i}} $ and $w_i = (\\frac{1}{N} . \\frac{1}{P(i)})^\\beta$ to avoid overfitting for the experiences with more priority.\n",
    "\n",
    "In both the paper the parameters are setted as follows: **priority exponent** $\\alpha= 0.7$,  the **importance sampling exponent** $\\beta = [0.5, 1]$.\n",
    "In the paper a heap array structure is proposed to implement the buffer. Due to the particular structure and the amount of property of the replay buffer in the Prioritized Experience Replay we choose to describe it as a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8897eb-7fea-48e8-93d4-cfd169a6278a",
   "metadata": {
    "id": "9e8897eb-7fea-48e8-93d4-cfd169a6278a"
   },
   "outputs": [],
   "source": [
    "# import heapq as heap\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import heapq\n",
    "\n",
    "# from decorator import append\n",
    "\n",
    "\n",
    "class PrioritizedExperienceReplayRankBased:\n",
    "    \"\"\"\n",
    "    replay_buffer       - contains the tuples (TD_error, transaction_id, experience)\n",
    "    max_buffer_size     - it's the max size of the buffer, over which before add an experience one is remove\n",
    "    alpha               - the alpha parameter used to calculate the probability of the i-th element P(i) to be sampled\n",
    "    self.max_td_error   - max td in the buffer\n",
    "\n",
    "    Old\n",
    "    time_to_haepify - time steps before sort the structure\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_buffer_size, alpha):\n",
    "        self.max_buffer_size = max_buffer_size\n",
    "        # (TD, experience)\n",
    "        self.replay_buffer = []\n",
    "        self.alpha = alpha\n",
    "        # The experience added has the maximum priority but once it sampled it will be updated with a more correct\n",
    "        # value.\n",
    "        self.max_td_error = 0\n",
    "        # self.heapify_threshold = step_to_heapify  # here we stock the threshold to sort the buffer\n",
    "        # self.step_to_heapify = step_to_heapify  # number of next steps before heapify\n",
    "\n",
    "    def set_alpha(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # Add experience in the buffer mapping it with its last TD_error.\n",
    "    # Heapq structure try to sort the elements of different tuple comparing from the first element of the tuple and\n",
    "    # continuing with next element until the two tuples have an element different. We are interested in sorting by TD,\n",
    "    # we don't care to sort on states, actions, or rewards. So, we use the transaction id to sort the transaction\n",
    "    # that is older in the buffer.\n",
    "    # The transaction_id of a transaction could be the -ith frame number of the whole training representing\n",
    "    # when the transaction happened.\n",
    "    # NB This approach avoids headppush fails when try to compare two states.\n",
    "    def add_experience(self, transaction_id, experience):\n",
    "        if len(self.replay_buffer) == self.max_buffer_size:\n",
    "            self.remove_experience()\n",
    "\n",
    "        # New experiences where td_error is unknown are set with the max td_error\n",
    "        if len(self.replay_buffer) > 0:\n",
    "            self.max_td_error = self.replay_buffer[0][0]\n",
    "        heapq.heappush(self.replay_buffer, (-self.max_td_error, transaction_id, experience))\n",
    "\n",
    "    # Remove experience from the buffer\n",
    "    def remove_experience(self, index=-1):\n",
    "        self.replay_buffer.pop(index)\n",
    "\n",
    "    @staticmethod\n",
    "    def zip_f_sampling(alpha, n):\n",
    "        x = np.arange(1, n + 1)\n",
    "        weights = x ** (-alpha)\n",
    "        weights /= weights.sum()\n",
    "        zipf = stats.rv_discrete(values=(x, weights))\n",
    "        return zipf.rvs() - 1\n",
    "\n",
    "    # Get batch_size samples from the buffer; using the beta parameter to compute the importance sampling weight\n",
    "    # Beta value can change while training we can delegate its control outside\n",
    "    def sample_experience(self, batch_size, beta):\n",
    "        experiences = []\n",
    "        importance_sampling_weights = []\n",
    "        n = len(self.replay_buffer) - 1\n",
    "        indexes = []\n",
    "        transaction_id = []\n",
    "\n",
    "        for i in range(0, batch_size):\n",
    "            # Sample index and check the experience is not already present in the batch\n",
    "            index = self.zip_f_sampling(self.alpha, n)\n",
    "            while index in indexes:\n",
    "                index = self.zip_f_sampling(self.alpha, n)\n",
    "            indexes.append(index)\n",
    "            # importance sampling weights computation\n",
    "            rank = index + 1\n",
    "            pj = 1 / rank\n",
    "            importance_sampling_weights.append(((n * pj) ** (-beta)))\n",
    "            transaction_id.append(self.replay_buffer[index][1])\n",
    "            experiences.append(self.replay_buffer[index][2])\n",
    "\n",
    "        # Normalization step\n",
    "        max_weight = max(importance_sampling_weights)\n",
    "        importance_sampling_weights_normalized = np.divide(importance_sampling_weights, max_weight)\n",
    "        return indexes, transaction_id, experiences, importance_sampling_weights_normalized\n",
    "\n",
    "    def update_td_error(self, indexes, td_errors):\n",
    "        # TODO update tupla e dopo ordina\n",
    "        for index, td_error in zip(indexes, td_errors):\n",
    "            my_list = list(self.replay_buffer[index])\n",
    "            my_list[0] = -abs(td_error)\n",
    "            self.replay_buffer[index] = tuple(my_list)\n",
    "        heapq.heapify(self.replay_buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb99df-922b-4ef1-9865-8d653c135eb8",
   "metadata": {
    "id": "5ddb99df-922b-4ef1-9865-8d653c135eb8"
   },
   "source": [
    "## Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58cc1cd-e7dc-4f40-ba18-b5127abc5fff",
   "metadata": {
    "id": "e58cc1cd-e7dc-4f40-ba18-b5127abc5fff"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras as krs\n",
    "\n",
    "input_shape = env.observation_space.shape\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d096c-c9d8-4012-9400-25075d3779cc",
   "metadata": {
    "id": "257d096c-c9d8-4012-9400-25075d3779cc"
   },
   "source": [
    "## Neural Network Creation\n",
    "The network architecture proposed follows the structure used in *\"Dueling Network Architectures for Deep Reinforcement Learning\"* https://arxiv.org/abs/1511.06581 composed of 3 convolutional layers and 2 fully connected layers for each stream (advantage, value).\n",
    "It's possible to create a dueling network using the `DQNAgent` of `rl.agents.dqn` setting `enable_dueling_network=True` in the constructor, but the purpose of this experiment is to show how to develop it manually so it is not used.\n",
    "\n",
    "The output of the value stream and the output of the advantage stream are merged to obtain the action-value function in the last module of the network using the following formula:\n",
    "$$ Q(s, a; \\theta, \\alpha, \\beta)^\\pi = V(s; \\theta, \\beta)^\\pi + (A(s, a; \\theta, \\alpha)^\\pi - \\frac{1}{|A|}\\sum_{a'}A(s, a'; \\theta, \\alpha)^\\pi) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a878b21-2040-4ca8-aeff-3563499bf9f9",
   "metadata": {
    "id": "0a878b21-2040-4ca8-aeff-3563499bf9f9"
   },
   "outputs": [],
   "source": [
    "from tensorflow import math\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "\n",
    "backend.set_image_data_format('channels_first')\n",
    "def create_dueling_model(input_shape, number_actions):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    value_stream_1 = layers.Dense(512)(layer4)\n",
    "    value_stream_2 = layers.Dense(1)(value_stream_1)  # scalar output size\n",
    "\n",
    "    advantage_stream_1 = layers.Dense(512)(layer4)\n",
    "    advantage_stream_2 = layers.Dense(number_actions)(advantage_stream_1)  # output size equal to the actions available\n",
    "\n",
    "    # Combination of the streams: a Q value for each state\n",
    "    q_values = value_stream_2 + math.subtract(advantage_stream_2, math.reduce_mean(advantage_stream_2, axis=1,\n",
    "                                                                                   keepdims=True))\n",
    "    # Alternative q_value\n",
    "    # q_value = value_stream_2 + (advantage_stream_2 - backend.max(advantage_stream_2, axis=1, keepdims=True))\n",
    "    return Model(inputs=[inputs], outputs=[q_values])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b2cfe-111c-4afe-a3c5-391066124fe3",
   "metadata": {
    "id": "ff1b2cfe-111c-4afe-a3c5-391066124fe3"
   },
   "source": [
    "# Agent \n",
    "Here we define a custom agent to perfom action in the environment using a DoubleDQN training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391a27a-28ac-4d3d-9c8f-b42bb303bd76",
   "metadata": {
    "id": "e391a27a-28ac-4d3d-9c8f-b42bb303bd76"
   },
   "source": [
    "## Play one step\n",
    "With this function we want to ask the policy what action must be chosen and perform it on the enironment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec3a88-954a-49f5-96b9-a6d34c29a16d",
   "metadata": {
    "id": "bdec3a88-954a-49f5-96b9-a6d34c29a16d"
   },
   "source": [
    "## Gradient \n",
    "In our scenario the gradient that is backpropageted to the last convolutional layer must be rescaled by $\\frac{1}{\\sqrt{2}}$. Furthermore we have to realize by hand the gradient clipping that is not realized by the optimizer since we are using a custom loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bd489-2d61-4334-b692-51a88f5ecb29",
   "metadata": {
    "id": "c45bd489-2d61-4334-b692-51a88f5ecb29"
   },
   "source": [
    "## Double DQN Training\n",
    "Double DQN algorithm uses a second network, beyond the network used for the prediction. So in the training process the main network is used to choose an action and another to evaluate it, this permits to mitigate the overfitting present in the classic DQN. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gg9foUcDcp1r",
   "metadata": {
    "id": "gg9foUcDcp1r"
   },
   "source": [
    "## DQN Agent code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad3e87-4bed-4e08-a34e-8a2e2fc06a83",
   "metadata": {
    "id": "6bad3e87-4bed-4e08-a34e-8a2e2fc06a83"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math as mt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class DuelDQNAgent:\n",
    "\n",
    "    # We keep the creation model outside the agent to ensure a fine-grained control on it\n",
    "    def __init__(self, env, model, policy, model_target=None, optimizer=None, replay_buffer=None):\n",
    "        self.env = env\n",
    "        self.model_primary = model\n",
    "        self.model_target = model_target\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "    def set_policy(self, policy):\n",
    "        self.policy = policy\n",
    "\n",
    "    # Execs one action receiving in input the current state\n",
    "    def play_one_step(self, state):\n",
    "        action = self.policy.get_action(state)\n",
    "        # print(\"action {}\".format(action))\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        return action, reward, next_state, done, info\n",
    "\n",
    "    # Play\n",
    "    def play(self):\n",
    "        state = self.env.reset()\n",
    "        steps = 0\n",
    "        cumulative_reward = 0\n",
    "        while True:\n",
    "            action, reward, next_state, done, info = self.play_one_step(state)\n",
    "            cumulative_reward += reward\n",
    "            if done:\n",
    "                print(\"DONE number of steps: {} reward:  {}\".format(steps, cumulative_reward))\n",
    "                break\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "        return steps, cumulative_reward\n",
    "\n",
    "    # Double DQN Training\n",
    "    #     @tf.function\n",
    "    @staticmethod\n",
    "    def gradient_clipping(gradients, clipping_value):\n",
    "        clipped_gradients = [(tf.clip_by_norm(grad, clipping_value)) for grad in gradients]\n",
    "        return clipped_gradients\n",
    "\n",
    "    # @tf.function\n",
    "    def weighted_gradient(self, best_on_target_q_values, importance_sampling_weights, states, loss_function, mask):\n",
    "        with tf.GradientTape() as tape:\n",
    "            batch_size = len(states)\n",
    "            tape.watch(importance_sampling_weights)\n",
    "            best_on_target_q_values = tf.expand_dims(best_on_target_q_values, 1)\n",
    "            all_q_values = self.model_primary(states)\n",
    "            q_values = tf.reduce_sum(all_q_values * mask, axis=1)\n",
    "            q_values = tf.expand_dims(q_values, 1)\n",
    "            # The target and the predicted values has been expanded following the instruction in:\n",
    "            # https://www.tensorflow.org/api_docs/python/tf/compat/v1/losses/mean_squared_error\n",
    "            loss_values = loss_function(y_true=best_on_target_q_values, y_pred=q_values,\n",
    "                                        sample_weight=importance_sampling_weights)\n",
    "            loss_value = tf.reduce_sum(loss_values) / batch_size\n",
    "\n",
    "        grads = tape.gradient(loss_value, self.model_primary.trainable_variables)\n",
    "        return grads, loss_values.numpy()\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_grad(gradients, rescale_value, index):\n",
    "        tensor_to_scale = gradients[index]\n",
    "        rescaled_tensor = tensor_to_scale * rescale_value\n",
    "        gradients[index] = rescaled_tensor\n",
    "        return gradients\n",
    "\n",
    "    # Collects samples of the previous experiences from the replay buffer\n",
    "    # and use them to improve the weights update of the Neural Network.\n",
    "    def double_dqn_training_step(self, batch_size, loss_function, discount_factor, clipping_value, beta, step_size=1):\n",
    "        indexes, transaction_ids, experiences, importance_sampling_weights = self.replay_buffer.sample_experience(\n",
    "            batch_size, beta)\n",
    "        states, actions, rewards, next_states, dones = [np.array([experience[field_index] for experience in experiences]\n",
    "                                                                 ) for field_index in range(5)]\n",
    "\n",
    "        action_space = self.env.action_space.n\n",
    "        # Predict using the primary network\n",
    "        next_q_values = self.model_primary.predict(next_states)\n",
    "        next_q_values_target = self.model_target.predict(next_states)\n",
    "\n",
    "        # Select the action that lead us to the higher next Q value\n",
    "        best_actions = np.argmax(next_q_values, axis=1)\n",
    "        best_action_mask = tf.one_hot(best_actions, action_space)\n",
    "\n",
    "        next_q_value_target = tf.reduce_sum(next_q_values_target * best_action_mask, axis=1)\n",
    "        best_on_target_q_values = (rewards + (1 - dones) * discount_factor * next_q_value_target)\n",
    "        # Punishment\n",
    "        best_on_target_q_values = best_on_target_q_values * (1-dones) - (dones)\n",
    "\n",
    "        mask = tf.one_hot(actions, action_space)\n",
    "        importance_sampling_weights = tf.convert_to_tensor(importance_sampling_weights, tf.float32)\n",
    "        weighted_gradient, loss_values = self.weighted_gradient(best_on_target_q_values, importance_sampling_weights,\n",
    "                                                                states, loss_function, mask)\n",
    "        self.replay_buffer.update_td_error(indexes, loss_values)\n",
    "\n",
    "        # We rescale the last convolutional layer to 1/sqrt(2) to balance the double backpropagation\n",
    "        # The index of the last sequential layer\n",
    "        #rescale_value = (1 / mt.sqrt(2))\n",
    "        #index_gradient_to_rescale = 4\n",
    "        #rescaled_grads = self.rescale_grad(weighted_gradient, rescale_value, index_gradient_to_rescale)\n",
    "\n",
    "        # Since we are in a custom loop we have to clip the gradient by hand, we can't delegate it to the optimizer\n",
    "        clipped_gradients = self.gradient_clipping(weighted_gradient, clipping_value)\n",
    "        # Application gradient descent trough optimizer\n",
    "        self.optimizer.apply_gradients(zip(clipped_gradients, self.model_primary.trainable_variables))\n",
    "\n",
    "    # We use the training step just when there is enough samples on the replay buffer\n",
    "    def double_dqn_training(self, batch_size, loss_function, discount_factor, freq_replacement, training_freq,\n",
    "                            clipping_value, beta_min, beta_max, max_episodes=600, max_steps=10800):\n",
    "        rewards_stock = []\n",
    "        steps_stock = []\n",
    "        cumulative_steps = 0\n",
    "\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            state = self.env.reset()\n",
    "            rewards = 0\n",
    "            steps = 0\n",
    "            beta = max(beta_min, (beta_max * episode / max_episodes))\n",
    "\n",
    "            while True:\n",
    "                action, reward, next_state, done, info = self.play_one_step(state)\n",
    "                experience = [state, action, reward, next_state, done]\n",
    "                rewards += reward\n",
    "                self.replay_buffer.add_experience(cumulative_steps, experience)\n",
    "                cumulative_steps += 1\n",
    "                steps += 1\n",
    "\n",
    "                if len(self.replay_buffer.replay_buffer) > batch_size and (cumulative_steps % training_freq) == 0:\n",
    "                    self.double_dqn_training_step(batch_size, loss_function, discount_factor, clipping_value, beta)\n",
    "                if (cumulative_steps % freq_replacement) == 0:\n",
    "                    self.model_target.set_weights(self.model_primary.get_weights())\n",
    "                if steps == max_steps:\n",
    "                    print(\n",
    "                        \"ABORTED episode = {} number of steps = {} reward = {}\".format(episode, steps, rewards))\n",
    "                if done:\n",
    "                    print(\n",
    "                        \"DONE episode = {} number of steps = {} reward = {}\".format(episode, steps, rewards))\n",
    "                if done or steps == max_steps:\n",
    "                    rewards_stock.append(rewards)\n",
    "                    steps_stock.append(steps)\n",
    "                    break\n",
    "                state = next_state\n",
    "\n",
    "            self.policy.next_episode()\n",
    "\n",
    "        return steps_stock, rewards_stock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538fe86-67f4-4292-b80c-0225a088b271",
   "metadata": {
    "id": "9538fe86-67f4-4292-b80c-0225a088b271"
   },
   "source": [
    "## Result plots\n",
    "We use this function to generate the plot representing the rewards or the steps for each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6ec3d-002c-4d30-9052-4e05db6839a0",
   "metadata": {
    "id": "6ef6ec3d-002c-4d30-9052-4e05db6839a0"
   },
   "outputs": [],
   "source": [
    "def plot_result(x_label, y_label, x, y, name):\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.plot(x, y)\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364dba26-35cc-4d53-878a-6bf0496b5a24",
   "metadata": {
    "id": "364dba26-35cc-4d53-878a-6bf0496b5a24"
   },
   "source": [
    "# Training\n",
    "The learning step is executed with the **Double Deep Q-networks** algorithm presented in the paper *\"Deep reinforcement learning with double Q-learning\"*.https://arxiv.org/pdf/1509.06461.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e16f49-345d-460c-922d-f529d9291a07",
   "metadata": {
    "id": "23e16f49-345d-460c-922d-f529d9291a07"
   },
   "source": [
    "## Training parameters\n",
    "We adopt as optimizer the **Adam** implementation setting the learning rate equal to $6.25x10^{-5}$ and **clipping the gradient** norm at most to 10; the parameters are specified in the paper \"*Deep reinforcement learning with double Q-learning*\" (https://arxiv.org/pdf/1509.06461.pdf)\n",
    "To evaluate the loss score we use the `mean_squared_error`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742cf2f-c7c6-403e-9da9-32d47e7717da",
   "metadata": {
    "id": "4742cf2f-c7c6-403e-9da9-32d47e7717da"
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Environment info\n",
    "input_shape = env.observation_space.shape\n",
    "actions_number = env.action_space.n\n",
    "\n",
    "# Model persistent file\n",
    "primary_model_file_name = \"{}_dueling_model\".format(game_name)\n",
    "DuelDQNAgent\n",
    "# Training Parameters\n",
    "loss_function = losses.MeanSquaredError(reduction=losses.Reduction.NONE)\n",
    "batch_size = 32 # @param {type:\"integer\"}\n",
    "discount_factor = 0.95 # @param {type:\"number\"}\n",
    "learning_rate = 6.25e-5 # @param {type:\"number\"}\n",
    "episodes = 3000 # @param {type:\"integer\"}\n",
    "clipping_value = 10 # @param {type:\"number\"}\n",
    "training_freq = 4 # @param {type:\"integer\"}\n",
    "\n",
    "# Dual DQN Training\n",
    "freq_replacement = 1000 # @param {type:\"integer\"}\n",
    "\n",
    "# Replay buffer parameters\n",
    "buffer_size = 100000 # @param {type:\"integer\"}\n",
    "# step_to_heapify = 200 # @param {type:\"integer\"}\n",
    "alpha = 0.7 # @param {type:\"number\"}\n",
    "beta_max = 1 # @param {type:\"number\"}\n",
    "beta_min = 0.5 # @param {type:\"number\"}\n",
    "\n",
    "# Policy parameters\n",
    "min_epsilon = 0.01 # @param {type:\"number\"}DuelDQNAgent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf7e90-0940-497c-8dc3-cafa5366bc3f",
   "metadata": {
    "id": "5fbf7e90-0940-497c-8dc3-cafa5366bc3f"
   },
   "source": [
    "## Model creation / loading \n",
    "In this step we check whether there is an already saved model and load it in that case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e76e0d-1130-47d8-b055-dbaa73907926",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3653,
     "status": "ok",
     "timestamp": 1665042575453,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "44e76e0d-1130-47d8-b055-dbaa73907926",
    "outputId": "49ca5eb3-67e4-44d7-a61a-02e830afe86d"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Model creation\n",
    "file_primary = Path(primary_model_file_name)\n",
    "if file_primary.exists():\n",
    "    print(\"Found an existing model\")\n",
    "    model = load_model(primary_model_file_name)\n",
    "else:\n",
    "    print(\"Model not found, a new one will be crate\")\n",
    "    model = create_dueling_model(input_shape, actions_number)\n",
    "\n",
    "# Print a summary about the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8325f-d204-47a9-84d0-e0532a453eb5",
   "metadata": {
    "id": "3cf8325f-d204-47a9-84d0-e0532a453eb5"
   },
   "source": [
    "## Training\n",
    "Here we ran the training operation. After a training session we save two plot episodes - rewards, episodes - steps. Also we save a csv with two columns: steps and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef554953-be5f-4c07-9537-7f61a548629a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 32149848,
     "status": "error",
     "timestamp": 1665074725297,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "ef554953-be5f-4c07-9537-7f61a548629a",
    "outputId": "d26f332c-d658-4041-b41b-06a1d68fdd00"
   },
   "outputs": [],
   "source": [
    "def training():\n",
    "    model_target = create_dueling_model(input_shape, actions_number)\n",
    "    model_target.set_weights(model.get_weights())\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    policy_training = EpsilonGreedyPolicy(model, actions_number, episodes=episodes, min_epsilon=min_epsilon)\n",
    "    replay_buffer = PrioritizedExperienceReplayRankBased(buffer_size, alpha)\n",
    "    agent = DuelDQNAgent(env, model, policy_training, model_target, optimizer, replay_buffer)\n",
    "    steps, rewards = agent.double_dqn_training(batch_size, loss_function, discount_factor, freq_replacement,\n",
    "                                               training_freq, clipping_value, beta_min, beta_max, episodes)\n",
    "\n",
    "    ext = \"png\"\n",
    "    name_plot_eps_steps = \"{} Training Episodes Steps.{}\".format(game_name, ext)\n",
    "    name_plot_eps_rewards = \"{} Training Episodes Rewards.{}\".format(game_name, ext)\n",
    "    file_plot_1 = Path(name_plot_eps_steps)\n",
    "    i = 1\n",
    "    while file_plot_1.exists():\n",
    "        i += 1\n",
    "        name_plot_eps_steps = \"{} Training Episodes Steps_{}.{}\".format(game_name, i, ext)\n",
    "        name_plot_eps_rewards = \"{} Training Episodes Rewards_{}.{}\".format(game_name, i, ext)\n",
    "        file_plot_1 = Path(name_plot_eps_steps)\n",
    "\n",
    "    plot_result(\"Episode\", \"Steps\", range(1, episodes + 1), steps, name_plot_eps_steps)\n",
    "    plot_result(\"Episode\", \"Rewards\", range(1, episodes + 1), rewards, name_plot_eps_rewards)\n",
    "\n",
    "    csv_name = \"{}.csv\".format(game_name)\n",
    "    dict = {'steps': steps, 'rewards': rewards}\n",
    "    df = pd.DataFrame(dict)\n",
    "    df.to_csv(csv_name, mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a05b7-819e-4b6d-ad07-bce40bd6fe01",
   "metadata": {},
   "source": [
    "# Play\n",
    "Here we play a game (one episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96580b5d-b9c7-418c-bdbe-a780b87ce2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play():\n",
    "    policy_play = EpsilonGreedyPolicy(model, actions_number, min_epsilon=min_epsilon)\n",
    "    env\n",
    "    agent = DuelDQNAgent(env, model, policy_play)\n",
    "    steps, reward = agent.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0525ad1-afdc-4a71-9bba-0041a3abad5d",
   "metadata": {
    "id": "b0525ad1-afdc-4a71-9bba-0041a3abad5d"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vi51G7gJfU0N",
   "metadata": {
    "id": "vi51G7gJfU0N"
   },
   "outputs": [],
   "source": [
    "let_training = True # @param {type:\"boolean\"}\n",
    "let_play = False # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b03c22-dc33-4db8-b3e1-a4408d017e01",
   "metadata": {
    "id": "28b03c22-dc33-4db8-b3e1-a4408d017e01"
   },
   "outputs": [],
   "source": [
    "\n",
    "if let_training:\n",
    "    training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c509d-475a-46f4-ad95-b556fd1e7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "if let_play:\n",
    "    play()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NtaGEQz9kh2h",
   "metadata": {
    "id": "NtaGEQz9kh2h"
   },
   "outputs": [],
   "source": [
    "#!rm -r ./sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9-PYk6Yj2-c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 820,
     "status": "ok",
     "timestamp": 1665074732809,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "b9-PYk6Yj2-c",
    "outputId": "e1565445-ca51-4d71-fee1-c565a81dda82"
   },
   "outputs": [],
   "source": [
    "#!zip -r /content/Asterix_dueling.zip /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77KRHlFFhhNG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1665074736231,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "77KRHlFFhhNG",
    "outputId": "dba6b184-6bec-447d-abef-369969bbf0ad"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#files.download('Asterix_dueling.zip')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
