{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "TDL4T4160P-8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21905,
     "status": "ok",
     "timestamp": 1665042566743,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "TDL4T4160P-8",
    "outputId": "f5f0c89a-3413-4a14-b0ac-91798038dadf"
   },
   "outputs": [],
   "source": [
    " #!pip install gym[atari,accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Lab36Cm53hT3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1666077971727,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "Lab36Cm53hT3",
    "outputId": "685e3000-67fa-4bb3-e1ec-7a50773a17f0"
   },
   "outputs": [],
   "source": [
    "# !apt install xvfb\n",
    "# !pip install gym-notebook-wrapper\n",
    "# !pip show tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84dfbf-263e-4d9a-ad88-2eaf271e4421",
   "metadata": {
    "id": "ae84dfbf-263e-4d9a-ad88-2eaf271e4421"
   },
   "source": [
    "# Dueling Network Architecture Implementation\n",
    "The Duelling newtorwk is an artificial neural network architecture that has improved the state of the art in the DQN area used in combination with Dual DQN and Prioritized Expirience Replay. This approach splits the action value calculation using a combination of state value function and advantage function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc578666-0c50-4681-8b90-bc48ece391d6",
   "metadata": {
    "id": "fc578666-0c50-4681-8b90-bc48ece391d6"
   },
   "source": [
    "# Searching for available environments\n",
    "We want to test the performance of our architecture with the Atari game 'Phoenix'.\n",
    "Here we check wich kind of versions of this game are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d78d38-4889-44d7-a8cd-49d3c9059bad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1539,
     "status": "ok",
     "timestamp": 1665042568266,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "82d78d38-4889-44d7-a8cd-49d3c9059bad",
    "outputId": "9d44fd58-f58f-4b95-cfa9-f00f7d2a04e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALE/Phoenix-ram-v5\n",
      "ALE/Phoenix-v5\n",
      "Phoenix-ram-v0\n",
      "Phoenix-ram-v4\n",
      "Phoenix-ramDeterministic-v0\n",
      "Phoenix-ramDeterministic-v4\n",
      "Phoenix-ramNoFrameskip-v0\n",
      "Phoenix-ramNoFrameskip-v4\n",
      "Phoenix-v0\n",
      "Phoenix-v4\n",
      "PhoenixDeterministic-v0\n",
      "PhoenixDeterministic-v4\n",
      "PhoenixNoFrameskip-v0\n",
      "PhoenixNoFrameskip-v4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ste/anaconda3/lib/python3.9/site-packages/requests/packages/urllib3/_collections.py:2: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Mapping, MutableMapping\n"
     ]
    }
   ],
   "source": [
    "from gym import envs\n",
    "\n",
    "# Searching for available environments\n",
    "game_name = \"Phoenix\"\n",
    "all_envs = envs.registry.values()\n",
    "env_ids = [env_spec.id for env_spec in all_envs]\n",
    "\n",
    "for id in sorted(env_ids):\n",
    "    if game_name in id:\n",
    "        print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5b1aa-6e92-4ac3-9079-c49121fafaef",
   "metadata": {
    "id": "12c5b1aa-6e92-4ac3-9079-c49121fafaef"
   },
   "source": [
    "# Environment Configuration\n",
    "We select the version 4 of the enviroment with no frameskipping and select as render mode human. The no frameskipping is used to make this enviroment compatible with the optimization made by *AtariPreprocessing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cecf335-3193-4ce8-90f5-dfe1c3a82e5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1665042568266,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "3cecf335-3193-4ce8-90f5-dfe1c3a82e5f",
    "outputId": "28ea8b12-3721-41e2-a222-156e9e23bd33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.wrappers import AtariPreprocessing\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# Make Parameters:\n",
    "game_name = \"Phoenix\"\n",
    "game_mode = \"NoFrameskip\"  # [Deterministic | NoFrameskip | ram | ramDeterministic | ramNoFrameskip ]\n",
    "game_version = \"v4\"  # [v0 | v4 | v5]\n",
    "env_name = '{}{}-{}'.format(game_name, game_mode, game_version)\n",
    "env_render_mode = 'human'  # [human | rgb_array]\n",
    "env_frame_skip = 4\n",
    "\n",
    "env = gym.make(env_name, render_mode=env_render_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b68bf0-d09a-432b-9fe9-9991f0d8a84a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1665042568267,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "23b68bf0-d09a-432b-9fe9-9991f0d8a84a",
    "outputId": "1921b05a-a9fa-47f2-9246-26adafe63578"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/core.py:51: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:297: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1e8ee56e20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env.reset() \n",
    "state = env.render(mode=\"rgb_array\")\n",
    "plt.imshow(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04326819-e929-40fe-9a26-ecfc317f7882",
   "metadata": {
    "id": "04326819-e929-40fe-9a26-ecfc317f7882"
   },
   "source": [
    "## Enviornment Observations\n",
    "Below we have a fist look to the environment characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "143cf671-a4e9-46a5-b06c-5f66f83fdc22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1665042568267,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "143cf671-a4e9-46a5-b06c-5f66f83fdc22",
    "outputId": "ffd6528b-5a7d-4fbf-d9e5-c4aad9150b7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "401a5ee2-bf0f-4721-be41-32c498caf4f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1665042568268,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "401a5ee2-bf0f-4721-be41-32c498caf4f3",
    "outputId": "ef5b5156-1509-47c2-98e8-2ffa96ea7495"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c5513e8-9f39-47bd-b667-a7436383795e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1665042568268,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "2c5513e8-9f39-47bd-b667-a7436383795e",
    "outputId": "f6792f4c-9ef4-485e-ef92-4875a0e28914"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'DOWN', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad4449-5a13-4032-9645-53f78265617b",
   "metadata": {
    "id": "39ad4449-5a13-4032-9645-53f78265617b"
   },
   "source": [
    "## Environment Optimization\n",
    "We optimize the enviroment adding the frame skipping, changing its observation in greyscale and following the experiment did in the paper we set to at most 30 the no-op actions; to get this we use the AtariPreprocessing wrapper.\n",
    "we use Framestack to create observation of 4 frames to give the idea of moviment to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63a89f89-355e-407a-a036-d1d14bfe3fba",
   "metadata": {
    "id": "63a89f89-355e-407a-a036-d1d14bfe3fba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = AtariPreprocessing(env, frame_skip=env_frame_skip, grayscale_obs=True, terminal_on_life_loss=True, noop_max=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43876bf8-8f34-4118-990a-dba1bbbc67fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "executionInfo": {
     "elapsed": 894,
     "status": "ok",
     "timestamp": 1665042569152,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "43876bf8-8f34-4118-990a-dba1bbbc67fd",
    "outputId": "bead8004-41c7-4082-d536-f1d6bce913c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1e8e8c4190>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = FrameStack(env, 4)\n",
    "state = env.reset()\n",
    "plt.imshow(state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51c6620c-52ee-48de-bc6b-4caba64b8f1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1665042569153,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "51c6620c-52ee-48de-bc6b-4caba64b8f1f",
    "outputId": "4c23526d-3fe5-4795-bba8-e84ad50f9222"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0735fbbc-11c3-418f-8026-f3eba9dc5cb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1665042569153,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "0735fbbc-11c3-418f-8026-f3eba9dc5cb9",
    "outputId": "ce6cfa59-1f25-4917-d5c2-b5aa4a49c4c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dccc025-b2ae-48c8-a70d-0e146d5c9d7c",
   "metadata": {
    "id": "9dccc025-b2ae-48c8-a70d-0e146d5c9d7c"
   },
   "source": [
    "# Network configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579e3d9-51f6-4612-bf77-25db75a98d4a",
   "metadata": {
    "id": "e579e3d9-51f6-4612-bf77-25db75a98d4a"
   },
   "source": [
    "## Policy\n",
    "The policy is the component that choose the action to perform; using an $\\epsilon$-gready policy the action chosen can be a random with probability $\\epsilon$ or an action suggested by the ANN with probability $1 - \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c3213c2-3be1-4337-bf89-5a32f9dbf2c8",
   "metadata": {
    "id": "7c3213c2-3be1-4337-bf89-5a32f9dbf2c8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class EpsilonGreedyPolicy:\n",
    "\n",
    "    def __init__(self, model, action_space_size, episodes=1, min_epsilon=0):\n",
    "        self.model = model\n",
    "        self.action_space_size = action_space_size\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.episode = 1\n",
    "        self.episodes = episodes\n",
    "\n",
    "    def get_action(self, state):\n",
    "        epsilon = max(1 - self.episode / self.episodes, self.min_epsilon)\n",
    "        random = np.random.random()\n",
    "        # print(random, epsilon)\n",
    "        if random < epsilon:\n",
    "            action = np.random.randint(self.action_space_size)\n",
    "            return action\n",
    "        else:\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = self.model(state_tensor, training=False)\n",
    "            \n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "            return action\n",
    "\n",
    "    def next_episode(self):\n",
    "        self.episode += 1\n",
    "\n",
    "    def reset_episodes(self):\n",
    "        self.episode = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c4fef-7fd8-4e38-9eac-b7623174d588",
   "metadata": {
    "id": "255c4fef-7fd8-4e38-9eac-b7623174d588"
   },
   "source": [
    "## Replication Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f6dd9-3076-4c01-bf7f-01b9400c6fc2",
   "metadata": {
    "id": "6d9f6dd9-3076-4c01-bf7f-01b9400c6fc2"
   },
   "source": [
    "### Prioritized experience replay\n",
    "The Prioritized experience replay was introduced in the paper \"Prioritized experience replay\" (https://arxiv.org/abs/1511.05952), it consists in an evolution of the replay buffer ordering the experiences to replay by priority. In this experiment we adopt the **rank-based** variant where the experience sampling from the buffer it's done with probability $ P(i) = \\frac{p_{i}^{\\alpha}}{\\sum_k{p_{k}^{\\alpha}}} $ and $p(i)=\\frac{1}{rank(i)}$ where $rank(i)$ is the rank of the transition *i*, $\\alpha$ is called **priority exponent**. It is necessary to compute the importance sampling weights as $w_j = \\frac{(N * P(j))^{-\\beta}}{max_i{w_i}} $ and $w_i = (\\frac{1}{N} . \\frac{1}{P(i)})^\\beta$ to avoid overfitting fot the experiences with more priority.\n",
    "\n",
    "In both the paper the parameters are setted as follow: **priority exponent** $\\alpha= 0.7$,  the **importance sampling exponent** $\\beta = [0.5, 1]$.\n",
    "In the paper is proposed a **heap array** structure to implement the buffer. Due to the particular structure and the amount of property of the replay buffer in the Prioritized Experience Replay we choose to describe it as a class. The heap array structure is implemented as a list sorted every *T* step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e8897eb-7fea-48e8-93d4-cfd169a6278a",
   "metadata": {
    "id": "9e8897eb-7fea-48e8-93d4-cfd169a6278a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# We set a time to haepify to sort the buffer every K time step.\n",
    "class PrioritizedExperienceReplayRankBased:\n",
    "    \"\"\"\n",
    "    contains the tuples (TD_error, experience)\n",
    "    replay_buffer --- it's the max size of the buffer, over which before add an experience one is remove\n",
    "    max_buffer_size --- time step before sort the structure\n",
    "    time_to_haepify --- the last time step\n",
    "    mod_curr_step = 0  --- the alpha parameter used to calculate the probability of the i-th element P(i) to be sampled\n",
    "    alpha -- alpha parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_buffer_size, step_to_heapify, alpha):\n",
    "        self.max_buffer_size = max_buffer_size\n",
    "        # (TD, experience)\n",
    "        # Probably list is not the most efficient structure to use np array ?\n",
    "        self.replay_buffer = []\n",
    "        self.alpha = alpha\n",
    "        self.heapify_threshold = step_to_heapify  # here we stock the threshold to sort the buffer\n",
    "        self.step_to_heapify = step_to_heapify  # number of next steps before heapify\n",
    "        self.max_td_error = 0\n",
    "\n",
    "    def set_alpha(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # Add experience in the buffer mapping it with its last TD_error\n",
    "    def add_experience(self, experience):\n",
    "        if len(self.replay_buffer) == self.max_buffer_size:\n",
    "            self.remove_experience()\n",
    "\n",
    "        # New experience where td_error is unknown are set with the max td error\n",
    "        # NB we are considering the max td error as the error of the experience in first position, but the buffer may\n",
    "        # not have been sorted yet\n",
    "        if len(self.replay_buffer) > 0:\n",
    "            self.max_td_error = self.replay_buffer[0][0]\n",
    "\n",
    "        self.replay_buffer.append((self.max_td_error, experience))\n",
    "        self.step_to_heapify -= 1\n",
    "        if self.step_to_heapify == 0:\n",
    "            self.replay_buffer.sort(key=lambda el: el[0], reverse=True)\n",
    "            self.step_to_heapify = self.heapify_threshold\n",
    "\n",
    "    # Remove experience from the buffer\n",
    "    def remove_experience(self):\n",
    "        self.replay_buffer.pop(-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def zip_f_sampling(alpha, n):\n",
    "        x = np.arange(1, n + 1)\n",
    "        weights = x ** (-alpha)\n",
    "        weights /= weights.sum()\n",
    "        zipf = stats.rv_discrete(values=(x, weights))\n",
    "        return zipf.rvs() - 1\n",
    "\n",
    "    # Get batch_size samples from the buffer; using the beta parameter to compute the importance sampling weight\n",
    "    # Beta value can change while training we can delegate its control outside\n",
    "    def sample_experience(self, batch_size, beta):\n",
    "        experiences = []\n",
    "        importance_sampling_weights = []\n",
    "        n = len(self.replay_buffer) - 1\n",
    "        indexes = []\n",
    "\n",
    "        for i in range(0, batch_size):\n",
    "            # Sample index and check the experience is not already present in the batch\n",
    "            index = self.zip_f_sampling(self.alpha, n)\n",
    "            while index in indexes:\n",
    "                index = self.zip_f_sampling(self.alpha, n)\n",
    "            indexes.append(index)\n",
    "            # importance sampling weights computation\n",
    "            rank = index + 1\n",
    "            pj = 1 / rank\n",
    "            importance_sampling_weights.append(((n * pj) ** (-beta)))\n",
    "            experiences.append(self.replay_buffer[index][1])\n",
    "\n",
    "        # Normalization step\n",
    "        max_weight = max(importance_sampling_weights)\n",
    "        importance_sampling_weights_normalized = np.divide(importance_sampling_weights, max_weight)\n",
    "        return indexes, experiences, importance_sampling_weights_normalized\n",
    "\n",
    "    def update_td_error(self, index, td_error):\n",
    "        self.replay_buffer[index] = [td_error, self.replay_buffer[index][1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb99df-922b-4ef1-9865-8d653c135eb8",
   "metadata": {
    "id": "5ddb99df-922b-4ef1-9865-8d653c135eb8"
   },
   "source": [
    "## Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e58cc1cd-e7dc-4f40-ba18-b5127abc5fff",
   "metadata": {
    "id": "e58cc1cd-e7dc-4f40-ba18-b5127abc5fff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 10:55:38.866962: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:513: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object,\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:521: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool,\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object:\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/tensor_util.py:110: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool:\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:22: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/numpy_ops/np_random.py:95: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def randint(low, high=None, size=None, dtype=onp.int):  # pylint: disable=missing-function-docstring\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/h5py/__init__.py:46: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras as krs\n",
    "\n",
    "input_shape = env.observation_space.shape\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d096c-c9d8-4012-9400-25075d3779cc",
   "metadata": {
    "id": "257d096c-c9d8-4012-9400-25075d3779cc"
   },
   "source": [
    "## Neural Network Creation\n",
    "The network architecture proposed follows the structure used in *\"Dueling Network Architectures for Deep Reinforcement Learning\"* https://arxiv.org/abs/1511.06581 composed by 3 convolutional layers and 2 fully connected layer for each stream (advantage, value).\n",
    "It's possible to create a dueling network using the `DQNAgent` of `rl.agents.dqn` setting `enable_dueling_network=True` in the constructor, but the perpouse of this experiment is to show how to develop it manually so that is not used.\n",
    "\n",
    "The output of the value stream and the output of the advantage stream are merged to obtain the action-value function in the last module of the network using the following formula:\n",
    "$$ Q(s, a; \\theta, \\alpha, \\beta) = V (s;\\theta, \\beta) + ( A(s, a; \\theta, \\alpha) − max_{a' ∈|A|} A(s, a'; \\theta, \\alpha)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a878b21-2040-4ca8-aeff-3563499bf9f9",
   "metadata": {
    "id": "0a878b21-2040-4ca8-aeff-3563499bf9f9"
   },
   "outputs": [],
   "source": [
    "from tensorflow import math\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "\n",
    "\n",
    "def create_dueling_model(input_shape, number_actions):\n",
    "    backend.set_image_data_format('channels_first')\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    value_stream_1 = layers.Dense(512)(layer4)\n",
    "    value_stream_2 = layers.Dense(1)(value_stream_1)  # scalar output size\n",
    "\n",
    "    advantage_stream_1 = layers.Dense(512)(layer4)\n",
    "    advantage_stream_2 = layers.Dense(number_actions)(advantage_stream_1)  # output size equal to the actions available\n",
    "\n",
    "    # Combination of the streams: a Q value for each state\n",
    "    q_values = value_stream_2 + math.subtract(advantage_stream_2, math.reduce_mean(advantage_stream_2, axis=1,\n",
    "                                                                                   keepdims=True))\n",
    "    # Alternative q_value\n",
    "    # q_value = value_stream_2 + (advantage_stream_2 - backend.max(advantage_stream_2, axis=1, keepdims=True))\n",
    "    return Model(inputs=[inputs], outputs=[q_values])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b2cfe-111c-4afe-a3c5-391066124fe3",
   "metadata": {
    "id": "ff1b2cfe-111c-4afe-a3c5-391066124fe3"
   },
   "source": [
    "# Agent \n",
    "Here we define a custom agent to perfome action in the enviroment using a DoubleDQN training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391a27a-28ac-4d3d-9c8f-b42bb303bd76",
   "metadata": {
    "id": "e391a27a-28ac-4d3d-9c8f-b42bb303bd76"
   },
   "source": [
    "## Play one step\n",
    "With this function we want to ask to the policy what action must be choosen and perform it on the eniroment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec3a88-954a-49f5-96b9-a6d34c29a16d",
   "metadata": {
    "id": "bdec3a88-954a-49f5-96b9-a6d34c29a16d"
   },
   "source": [
    "## Gradient \n",
    "In our scenario the gradient that is backpropageted to the last convolutional layer must be rescaled by $\\frac{1}{\\sqrt{2}}$. Furthermore we have to realize by hand the gradient clipping that is not realized by the optimizer since we are using a custoom loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bd489-2d61-4334-b692-51a88f5ecb29",
   "metadata": {
    "id": "c45bd489-2d61-4334-b692-51a88f5ecb29"
   },
   "source": [
    "## Double DQN Training\n",
    "Double DQN algorithm uses a second network, beyond the network used for the prediction. So in the training process the main network is used to choose an action and an other to evaluate it, this permit to mitigate the overfitting present in the classic DQN. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gg9foUcDcp1r",
   "metadata": {
    "id": "gg9foUcDcp1r"
   },
   "source": [
    "## DQN Agent code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bad3e87-4bed-4e08-a34e-8a2e2fc06a83",
   "metadata": {
    "id": "6bad3e87-4bed-4e08-a34e-8a2e2fc06a83"
   },
   "outputs": [],
   "source": [
    "import math as mt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DuelDQNAgent:\n",
    "\n",
    "    ## We keep the creation model outside the agent to ensure a fine-grained control on it\n",
    "    def __init__(self, env, model, policy, model_target=None, optimizer=None, replay_buffer=None):\n",
    "        self.env = env\n",
    "        self.model_primary = model\n",
    "        self.model_target = model_target\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "    def set_policy(self, policy):\n",
    "        self.policy = policy\n",
    "\n",
    "    # Execs one action receiving in input the environment, its state, the current episode.\n",
    "    # If training its true add the experience in the replay buffer\n",
    "    def play_one_step(self, state):\n",
    "        action = self.policy.get_action(state)\n",
    "        # print(\"action {}\".format(action))\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        return action, reward, next_state, done, info\n",
    "\n",
    "    # Play\n",
    "    def play(self):\n",
    "        state = self.env.reset()\n",
    "        steps = 0\n",
    "        cumulative_reward = 0\n",
    "        while True:\n",
    "            action, reward, next_state, done, info = self.play_one_step(state)\n",
    "            cumulative_reward += reward\n",
    "            if done:\n",
    "                print(\"DONE number of steps: {} reward:  {}\".format(steps, cumulative_reward))\n",
    "                break\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "        return steps, cumulative_reward\n",
    "\n",
    "    ## Double DQN Training\n",
    "    @staticmethod\n",
    "    def gradient_clipping(gradients, clipping_value):\n",
    "        clipped_gradients = [(tf.clip_by_norm(grad, clipping_value)) for grad in gradients]\n",
    "        return clipped_gradients\n",
    "\n",
    "    def weighted_gradient(self, best_on_target_q_values, importance_sampling_weights, states, loss_function, mask,\n",
    "                          step_size=1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(importance_sampling_weights)\n",
    "            all_q_values = self.model_primary(states)\n",
    "            q_values = tf.reduce_sum(all_q_values * mask, axis=1, keepdims=True)\n",
    "            loss_value = loss_function(best_on_target_q_values, q_values)\n",
    "            loss_corrected = tf.multiply(loss_value, importance_sampling_weights, step_size)\n",
    "        grads = tape.gradient(loss_corrected, self.model_primary.trainable_variables)\n",
    "        return grads, loss_value\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_grad(gradients, rescale_value, index):\n",
    "        tensor_to_scale = gradients[index]\n",
    "        rescaled_tensor = tf.multiply(tensor_to_scale, rescale_value)\n",
    "        gradients[index] = rescaled_tensor\n",
    "        return gradients\n",
    "\n",
    "    # Collects samples of the previous experiences from the replay buffer\n",
    "    # and use them to improve the weights update of the Neural Network.\n",
    "    def double_dqn_training_step(self, batch_size, loss_function, discount_factor, clipping_value, beta, step_size=1):\n",
    "        indexes, experiences, importance_sampling_weights = self.replay_buffer.sample_experience(batch_size, beta)\n",
    "        states, actions, rewards, next_states, dones = [np.array([experience[field_index] for experience in experiences]\n",
    "                                                                 ) for field_index in range(5)]\n",
    "\n",
    "        action_space = self.env.action_space.n\n",
    "        # Predict using the primary network\n",
    "        next_q_values = self.model_primary.predict(next_states)\n",
    "        next_q_values_target = self.model_target.predict(next_states)\n",
    "        \n",
    "        # Select the action that lead us to the higher next Q value\n",
    "        best_actions = np.argmax(next_q_values, axis=1)\n",
    "        best_action_mask = tf.one_hot(best_actions, action_space)\n",
    "\n",
    "        next_q_value_target = tf.reduce_sum(next_q_values_target * best_action_mask, axis=1)\n",
    "        best_on_target_q_values = (rewards + (1-dones)*discount_factor*next_q_value_target)\n",
    "        # Add a negative reward when the agent die\n",
    "        best_on_target_q_values = best_on_target_q_values * (1-dones) - dones\n",
    "\n",
    "        mask = tf.one_hot(actions, action_space)\n",
    "        importance_sampling_weights = tf.convert_to_tensor(importance_sampling_weights, tf.float32)\n",
    "        weighted_gradient, loss_value = self.weighted_gradient(best_on_target_q_values, importance_sampling_weights,\n",
    "                                                               states, loss_function, mask, step_size)\n",
    "\n",
    "        for index, td_error in zip(indexes, loss_value):\n",
    "            self.replay_buffer.update_td_error(index, abs(td_error))\n",
    "\n",
    "        # We rescale the last convolutional layer to 1/sqrt(2) to balance the double backpropagation\n",
    "        rescale_value = (1 / mt.sqrt(2))\n",
    "        # The index of the last sequential layer\n",
    "        index_gradient_to_rescale = 4\n",
    "        rescaled_grads = self.rescale_grad(weighted_gradient, rescale_value, index_gradient_to_rescale)\n",
    "\n",
    "        # Since we are in a custom loop we have to clip the gradient by hand, we can't delegate it to the optimizer\n",
    "        clipped_gradients = self.gradient_clipping(rescaled_grads, clipping_value)\n",
    "        # Application gradient descent trough optimizer\n",
    "        self.optimizer.apply_gradients(zip(clipped_gradients, self.model_primary.trainable_variables))\n",
    "\n",
    "    # We use the training step just when there is enough samples on the replay buffer\n",
    "    def double_dqn_training(self, batch_size, loss_function, discount_factor, freq_replacement, training_freq,\n",
    "                            clipping_value, beta_min, beta_max, max_episodes=600):\n",
    "        rewards = []\n",
    "        steps = []\n",
    "        \n",
    "\n",
    "        for episode in range(1, max_episodes+1):\n",
    "            state = self.env.reset()\n",
    "            cumulative_reward = 0\n",
    "            step = 0\n",
    "            beta = max(beta_min, (beta_max * episode / max_episodes))\n",
    "\n",
    "            while True:\n",
    "                action, reward, next_state, done, info = self.play_one_step(state)\n",
    "                experience = [state, action, reward, next_state, done]\n",
    "                cumulative_reward += reward\n",
    "                self.replay_buffer.add_experience(experience)\n",
    "                if done:\n",
    "                    print(\n",
    "                        \"DONE episode = {} number of steps = {} reward = {}\".format(episode, step, cumulative_reward))\n",
    "                    rewards.append(cumulative_reward)\n",
    "                    steps.append(step)\n",
    "                    break\n",
    "                if len(self.replay_buffer.replay_buffer) > batch_size and (step % training_freq) == 0:\n",
    "                    self.double_dqn_training_step(batch_size, loss_function, discount_factor, clipping_value, beta)\n",
    "                if (step % freq_replacement) == 0:\n",
    "                    self.model_target.set_weights(self.model_primary.get_weights())\n",
    "                state = next_state\n",
    "                step = step + 1\n",
    "\n",
    "            self.policy.next_episode()\n",
    "        return steps, rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538fe86-67f4-4292-b80c-0225a088b271",
   "metadata": {
    "id": "9538fe86-67f4-4292-b80c-0225a088b271"
   },
   "source": [
    "## Result plots\n",
    "We use this function to generate the plot representing the rewards or the steps for each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ef6ec3d-002c-4d30-9052-4e05db6839a0",
   "metadata": {
    "id": "6ef6ec3d-002c-4d30-9052-4e05db6839a0"
   },
   "outputs": [],
   "source": [
    "def plot_result(x_label, y_label, x, y, name):\n",
    "    plt.ylabel(x_label)\n",
    "    plt.xlabel(y_label)\n",
    "    plt.plot(x, y)\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364dba26-35cc-4d53-878a-6bf0496b5a24",
   "metadata": {
    "id": "364dba26-35cc-4d53-878a-6bf0496b5a24"
   },
   "source": [
    "# Training\n",
    "The learnig step is executed with **Double Deep Q-networks** algorithm presented in the paper *\"Deep reinforcement learning with double Q-learning\"*.https://arxiv.org/pdf/1509.06461.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e16f49-345d-460c-922d-f529d9291a07",
   "metadata": {
    "id": "23e16f49-345d-460c-922d-f529d9291a07"
   },
   "source": [
    "## Training parameters\n",
    "We adopt as optimizer the **Adam** implementation setting the learning rate equal to $6.25x10^{-5}$ and **clipping the gradient** norm at most to 10; the parameters are specified in the paper \"*Deep reinforcement learning with double Q-learning*\" (https://arxiv.org/pdf/1509.06461.pdf)\n",
    "To evaluate the loss score we use the `mean_squared_error`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4742cf2f-c7c6-403e-9da9-32d47e7717da",
   "metadata": {
    "id": "4742cf2f-c7c6-403e-9da9-32d47e7717da"
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Environment info\n",
    "input_shape = env.observation_space.shape\n",
    "actions_number = env.action_space.n\n",
    "\n",
    "# Model persistent file\n",
    "primary_model_file_name = \"{}_dueling_model\".format(game_name)\n",
    "\n",
    "# Training Parameters\n",
    "loss_function = losses.mean_squared_error\n",
    "batch_size = 32 # @param {type:\"integer\"}\n",
    "discount_factor = 0.95 # @param {type:\"number\"}\n",
    "learning_rate = 6.25e-5 # @param {type:\"number\"}\n",
    "episodes = 1000 # @param {type:\"integer\"}\n",
    "clipping_value = 10 # @param {type:\"number\"}\n",
    "training_freq = 4 # @param {type:\"integer\"}\n",
    "\n",
    "# Dual DQN Training\n",
    "freq_replacement = 1000 # @param {type:\"integer\"}\n",
    "\n",
    "# Replay buffer parameters\n",
    "buffer_size = 10000 # @param {type:\"integer\"}\n",
    "step_to_heapify = 200 # @param {type:\"integer\"}\n",
    "alpha = 0.7 # @param {type:\"number\"}\n",
    "beta_max = 1 # @param {type:\"number\"}\n",
    "beta_min = 0.5 # @param {type:\"number\"}\n",
    "\n",
    "# Policy parameters\n",
    "min_epsilon = 0.01 # @param {type:\"number\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf7e90-0940-497c-8dc3-cafa5366bc3f",
   "metadata": {
    "id": "5fbf7e90-0940-497c-8dc3-cafa5366bc3f"
   },
   "source": [
    "## Model creation / loading \n",
    "In this step we check wheter there is an already saved model and load it in that case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44e76e0d-1130-47d8-b055-dbaa73907926",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3653,
     "status": "ok",
     "timestamp": 1665042575453,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "44e76e0d-1130-47d8-b055-dbaa73907926",
    "outputId": "49ca5eb3-67e4-44d7-a61a-02e830afe86d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found an existing model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 10:55:39.855260: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-10-19 10:55:39.855980: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-10-19 10:55:39.882479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-19 10:55:39.882604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1050 Ti computeCapability: 6.1\n",
      "coreClock: 1.4175GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\n",
      "2022-10-19 10:55:39.882628: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-10-19 10:55:39.884094: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-10-19 10:55:39.884133: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-10-19 10:55:39.885376: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-10-19 10:55:39.885602: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-10-19 10:55:39.887072: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-10-19 10:55:39.887841: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-10-19 10:55:39.890829: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-10-19 10:55:39.890957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-19 10:55:39.891132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-19 10:55:39.891218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-10-19 10:55:39.891608: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-19 10:55:39.892333: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-10-19 10:55:39.892471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-19 10:55:39.892585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1050 Ti computeCapability: 6.1\n",
      "coreClock: 1.4175GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\n",
      "2022-10-19 10:55:39.892624: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-10-19 10:55:39.892651: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-10-19 10:55:39.892672: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-10-19 10:55:39.892693: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-10-19 10:55:39.892716: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-10-19 10:55:39.892736: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-10-19 10:55:39.892758: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-10-19 10:55:39.892779: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-10-19 10:55:39.892843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-19 10:55:39.892981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-19 10:55:39.893076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-10-19 10:55:39.893112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-10-19 10:55:40.254125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-10-19 10:55:40.254147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-10-19 10:55:40.254153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-10-19 10:55:40.254328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-19 10:55:40.254493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-19 10:55:40.254619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-19 10:55:40.254716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3578 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4, 84, 84)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 20, 20)   8224        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 9, 9)     32832       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 7, 7)     36928       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3136)         0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          1606144     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 8)            4104        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          1606144     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean (TFOpLambda (None, 1)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            513         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda)   (None, 8)            0           dense_3[0][0]                    \n",
      "                                                                 tf.math.reduce_mean[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 8)            0           dense_1[0][0]                    \n",
      "                                                                 tf.math.subtract[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 3,294,889\n",
      "Trainable params: 3,294,889\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Model creation\n",
    "file_primary = Path(primary_model_file_name)\n",
    "if file_primary.exists():\n",
    "    print(\"Found an existing model\")\n",
    "    model = load_model(primary_model_file_name)\n",
    "else:\n",
    "    print(\"Model not found, a new one will be crate\")\n",
    "    model = create_dueling_model(input_shape, actions_number)\n",
    "\n",
    "# Print a summary about the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8325f-d204-47a9-84d0-e0532a453eb5",
   "metadata": {
    "id": "3cf8325f-d204-47a9-84d0-e0532a453eb5"
   },
   "source": [
    "## Training\n",
    "Here we ran the training operation. After a training session we save two plot episode - rewards, episode - steps. Also we save a csv with two column: steps and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef554953-be5f-4c07-9537-7f61a548629a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 32149848,
     "status": "error",
     "timestamp": 1665074725297,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "ef554953-be5f-4c07-9537-7f61a548629a",
    "outputId": "d26f332c-d658-4041-b41b-06a1d68fdd00"
   },
   "outputs": [],
   "source": [
    "def training():\n",
    "    try:\n",
    "        model_target = create_dueling_model(input_shape, actions_number)\n",
    "        model_target.set_weights(model.get_weights())\n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "        policy_training = EpsilonGreedyPolicy(model, actions_number, episodes=episodes, min_epsilon=min_epsilon)\n",
    "        replay_buffer = PrioritizedExperienceReplayRankBased(buffer_size, step_to_heapify, alpha)\n",
    "        agent = DuelDQNAgent(env, model, policy_training, model_target, optimizer, replay_buffer)\n",
    "        steps, rewards = agent.double_dqn_training(batch_size, loss_function, discount_factor, freq_replacement,\n",
    "                                                   training_freq, clipping_value, beta_min, beta_max, episodes)\n",
    "\n",
    "        ext = \"png\"\n",
    "        name_plot_eps_steps = \"{} Training Episodes Steps.{}\".format(game_name, ext)\n",
    "        name_plot_eps_rewards = \"{} Training Episodes Rewards.{}\".format(game_name, ext)\n",
    "        file_plot_1 = Path(name_plot_eps_steps)\n",
    "        i = 1\n",
    "        while file_plot_1.exists():\n",
    "            i += 1\n",
    "            name_plot_eps_steps = \"{} Training Episodes Steps_{}.{}\".format(game_name, i, ext)\n",
    "            name_plot_eps_rewards = \"{} Training Episodes Rewards_{}.{}\".format(game_name, i, ext)\n",
    "            file_plot_1 = Path(name_plot_eps_steps)\n",
    "\n",
    "        plot_result(\"Episode\", \"Steps\", range(1, episodes + 1), steps, name_plot_eps_steps)\n",
    "        plot_result(\"Episode\", \"Rewards\", range(1, episodes + 1), rewards, name_plot_eps_rewards)\n",
    "\n",
    "    finally:\n",
    "        model.save(primary_model_file_name)\n",
    "\n",
    "        csv_name = \"{}.csv\".format(game_name)\n",
    "        if steps is not None and rewards is not None:\n",
    "            dict = {'steps': steps, 'rewards': rewards}\n",
    "            df = pd.DataFrame(dict)\n",
    "            df.to_csv(csv_name, mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a05b7-819e-4b6d-ad07-bce40bd6fe01",
   "metadata": {},
   "source": [
    "# Play\n",
    "Here we play a game (one episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96580b5d-b9c7-418c-bdbe-a780b87ce2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play():\n",
    "    policy_play = EpsilonGreedyPolicy(model, actions_number, min_epsilon=min_epsilon)\n",
    "    agent = DuelDQNAgent(env, model, policy_play)\n",
    "    steps, reward = agent.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0525ad1-afdc-4a71-9bba-0041a3abad5d",
   "metadata": {
    "id": "b0525ad1-afdc-4a71-9bba-0041a3abad5d"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "vi51G7gJfU0N",
   "metadata": {
    "id": "vi51G7gJfU0N"
   },
   "outputs": [],
   "source": [
    "let_training = True # @param {type:\"boolean\"}\n",
    "let_play = False # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b03c22-dc33-4db8-b3e1-a4408d017e01",
   "metadata": {
    "id": "28b03c22-dc33-4db8-b3e1-a4408d017e01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 10:55:43.411482: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-10-19 10:55:43.426863: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200660000 Hz\n",
      "2022-10-19 10:55:43.487627: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-10-19 10:55:43.590356: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE episode = 1 number of steps = 174 reward = 140.0\n",
      "DONE episode = 2 number of steps = 77 reward = 40.0\n",
      "DONE episode = 3 number of steps = 81 reward = 80.0\n",
      "DONE episode = 4 number of steps = 78 reward = 80.0\n",
      "DONE episode = 5 number of steps = 79 reward = 80.0\n",
      "DONE episode = 6 number of steps = 75 reward = 40.0\n",
      "DONE episode = 7 number of steps = 79 reward = 40.0\n",
      "DONE episode = 8 number of steps = 79 reward = 40.0\n",
      "DONE episode = 9 number of steps = 145 reward = 200.0\n",
      "DONE episode = 10 number of steps = 78 reward = 120.0\n",
      "DONE episode = 11 number of steps = 78 reward = 80.0\n",
      "DONE episode = 12 number of steps = 79 reward = 40.0\n",
      "DONE episode = 13 number of steps = 139 reward = 80.0\n",
      "DONE episode = 14 number of steps = 98 reward = 60.0\n",
      "DONE episode = 15 number of steps = 78 reward = 60.0\n",
      "DONE episode = 16 number of steps = 81 reward = 40.0\n",
      "DONE episode = 17 number of steps = 78 reward = 20.0\n",
      "DONE episode = 18 number of steps = 78 reward = 60.0\n",
      "DONE episode = 19 number of steps = 97 reward = 60.0\n",
      "DONE episode = 20 number of steps = 1548 reward = 100.0\n",
      "DONE episode = 21 number of steps = 79 reward = 40.0\n",
      "DONE episode = 22 number of steps = 128 reward = 80.0\n",
      "DONE episode = 23 number of steps = 81 reward = 40.0\n",
      "DONE episode = 24 number of steps = 178 reward = 120.0\n",
      "DONE episode = 25 number of steps = 177 reward = 140.0\n",
      "DONE episode = 26 number of steps = 77 reward = 60.0\n",
      "DONE episode = 27 number of steps = 77 reward = 60.0\n",
      "DONE episode = 28 number of steps = 75 reward = 60.0\n",
      "DONE episode = 29 number of steps = 80 reward = 40.0\n",
      "DONE episode = 30 number of steps = 79 reward = 40.0\n",
      "DONE episode = 31 number of steps = 80 reward = 60.0\n",
      "DONE episode = 32 number of steps = 555 reward = 200.0\n",
      "DONE episode = 33 number of steps = 80 reward = 80.0\n",
      "DONE episode = 34 number of steps = 77 reward = 40.0\n",
      "DONE episode = 35 number of steps = 78 reward = 60.0\n",
      "DONE episode = 36 number of steps = 76 reward = 40.0\n",
      "DONE episode = 37 number of steps = 1038 reward = 120.0\n",
      "DONE episode = 38 number of steps = 190 reward = 100.0\n",
      "DONE episode = 39 number of steps = 78 reward = 40.0\n",
      "DONE episode = 40 number of steps = 990 reward = 140.0\n",
      "DONE episode = 41 number of steps = 77 reward = 80.0\n",
      "DONE episode = 42 number of steps = 79 reward = 40.0\n",
      "DONE episode = 43 number of steps = 174 reward = 80.0\n",
      "DONE episode = 44 number of steps = 93 reward = 80.0\n",
      "DONE episode = 45 number of steps = 76 reward = 60.0\n",
      "DONE episode = 46 number of steps = 223 reward = 140.0\n",
      "DONE episode = 47 number of steps = 75 reward = 40.0\n",
      "DONE episode = 48 number of steps = 75 reward = 40.0\n",
      "DONE episode = 49 number of steps = 76 reward = 60.0\n",
      "DONE episode = 50 number of steps = 80 reward = 120.0\n",
      "DONE episode = 51 number of steps = 76 reward = 60.0\n",
      "DONE episode = 52 number of steps = 174 reward = 120.0\n",
      "DONE episode = 53 number of steps = 174 reward = 120.0\n",
      "DONE episode = 54 number of steps = 80 reward = 40.0\n",
      "DONE episode = 55 number of steps = 78 reward = 40.0\n",
      "DONE episode = 56 number of steps = 974 reward = 160.0\n",
      "DONE episode = 57 number of steps = 396 reward = 400.0\n",
      "DONE episode = 58 number of steps = 175 reward = 180.0\n",
      "DONE episode = 59 number of steps = 75 reward = 40.0\n",
      "DONE episode = 60 number of steps = 80 reward = 80.0\n",
      "DONE episode = 61 number of steps = 76 reward = 200.0\n",
      "DONE episode = 62 number of steps = 82 reward = 80.0\n",
      "DONE episode = 63 number of steps = 80 reward = 120.0\n",
      "DONE episode = 64 number of steps = 76 reward = 40.0\n",
      "DONE episode = 65 number of steps = 127 reward = 120.0\n",
      "DONE episode = 66 number of steps = 1934 reward = 160.0\n",
      "DONE episode = 67 number of steps = 76 reward = 60.0\n",
      "DONE episode = 68 number of steps = 621 reward = 240.0\n",
      "DONE episode = 69 number of steps = 75 reward = 40.0\n",
      "DONE episode = 70 number of steps = 77 reward = 60.0\n",
      "DONE episode = 71 number of steps = 81 reward = 60.0\n",
      "DONE episode = 72 number of steps = 123 reward = 40.0\n",
      "DONE episode = 73 number of steps = 78 reward = 40.0\n",
      "DONE episode = 74 number of steps = 107 reward = 40.0\n",
      "DONE episode = 75 number of steps = 97 reward = 40.0\n",
      "DONE episode = 76 number of steps = 78 reward = 40.0\n",
      "DONE episode = 77 number of steps = 76 reward = 40.0\n",
      "DONE episode = 78 number of steps = 94 reward = 80.0\n",
      "DONE episode = 79 number of steps = 81 reward = 60.0\n",
      "DONE episode = 80 number of steps = 1217 reward = 440.0\n",
      "DONE episode = 81 number of steps = 94 reward = 60.0\n",
      "DONE episode = 82 number of steps = 77 reward = 40.0\n",
      "DONE episode = 83 number of steps = 75 reward = 40.0\n",
      "DONE episode = 84 number of steps = 78 reward = 80.0\n",
      "DONE episode = 85 number of steps = 76 reward = 40.0\n",
      "DONE episode = 86 number of steps = 76 reward = 40.0\n",
      "DONE episode = 87 number of steps = 80 reward = 40.0\n",
      "DONE episode = 88 number of steps = 75 reward = 40.0\n",
      "DONE episode = 89 number of steps = 76 reward = 60.0\n",
      "DONE episode = 90 number of steps = 75 reward = 60.0\n",
      "DONE episode = 91 number of steps = 178 reward = 140.0\n",
      "DONE episode = 92 number of steps = 75 reward = 40.0\n",
      "DONE episode = 93 number of steps = 80 reward = 60.0\n",
      "DONE episode = 94 number of steps = 75 reward = 20.0\n",
      "DONE episode = 95 number of steps = 78 reward = 40.0\n",
      "DONE episode = 96 number of steps = 75 reward = 60.0\n",
      "DONE episode = 97 number of steps = 81 reward = 40.0\n",
      "DONE episode = 98 number of steps = 81 reward = 60.0\n",
      "DONE episode = 99 number of steps = 74 reward = 0.0\n",
      "DONE episode = 100 number of steps = 75 reward = 20.0\n",
      "DONE episode = 101 number of steps = 95 reward = 60.0\n",
      "DONE episode = 102 number of steps = 81 reward = 40.0\n",
      "DONE episode = 103 number of steps = 299 reward = 300.0\n",
      "DONE episode = 104 number of steps = 369 reward = 120.0\n",
      "DONE episode = 105 number of steps = 172 reward = 220.0\n",
      "DONE episode = 106 number of steps = 79 reward = 60.0\n",
      "DONE episode = 107 number of steps = 77 reward = 60.0\n",
      "DONE episode = 108 number of steps = 129 reward = 220.0\n",
      "DONE episode = 109 number of steps = 77 reward = 80.0\n",
      "DONE episode = 110 number of steps = 177 reward = 240.0\n",
      "DONE episode = 111 number of steps = 129 reward = 120.0\n",
      "DONE episode = 112 number of steps = 81 reward = 80.0\n",
      "DONE episode = 113 number of steps = 92 reward = 80.0\n",
      "DONE episode = 114 number of steps = 78 reward = 40.0\n",
      "DONE episode = 115 number of steps = 81 reward = 40.0\n",
      "DONE episode = 116 number of steps = 77 reward = 40.0\n",
      "DONE episode = 117 number of steps = 91 reward = 60.0\n",
      "DONE episode = 118 number of steps = 126 reward = 100.0\n",
      "DONE episode = 119 number of steps = 95 reward = 120.0\n",
      "DONE episode = 120 number of steps = 75 reward = 40.0\n",
      "DONE episode = 121 number of steps = 79 reward = 60.0\n",
      "DONE episode = 122 number of steps = 80 reward = 60.0\n",
      "DONE episode = 123 number of steps = 171 reward = 120.0\n",
      "DONE episode = 124 number of steps = 75 reward = 40.0\n",
      "DONE episode = 125 number of steps = 95 reward = 40.0\n",
      "DONE episode = 126 number of steps = 96 reward = 80.0\n",
      "DONE episode = 127 number of steps = 92 reward = 80.0\n",
      "DONE episode = 128 number of steps = 81 reward = 80.0\n",
      "DONE episode = 129 number of steps = 80 reward = 20.0\n",
      "DONE episode = 130 number of steps = 76 reward = 20.0\n",
      "DONE episode = 131 number of steps = 77 reward = 40.0\n",
      "DONE episode = 132 number of steps = 76 reward = 40.0\n",
      "DONE episode = 133 number of steps = 77 reward = 60.0\n",
      "DONE episode = 134 number of steps = 78 reward = 40.0\n",
      "DONE episode = 135 number of steps = 82 reward = 20.0\n",
      "DONE episode = 136 number of steps = 78 reward = 60.0\n",
      "DONE episode = 137 number of steps = 82 reward = 20.0\n",
      "DONE episode = 138 number of steps = 78 reward = 60.0\n",
      "DONE episode = 139 number of steps = 204 reward = 160.0\n",
      "DONE episode = 140 number of steps = 76 reward = 60.0\n",
      "DONE episode = 141 number of steps = 79 reward = 60.0\n",
      "DONE episode = 142 number of steps = 75 reward = 40.0\n",
      "DONE episode = 143 number of steps = 76 reward = 40.0\n",
      "DONE episode = 144 number of steps = 78 reward = 60.0\n",
      "DONE episode = 145 number of steps = 77 reward = 60.0\n",
      "DONE episode = 146 number of steps = 79 reward = 40.0\n",
      "DONE episode = 147 number of steps = 222 reward = 200.0\n",
      "DONE episode = 148 number of steps = 77 reward = 40.0\n",
      "DONE episode = 149 number of steps = 74 reward = 60.0\n",
      "DONE episode = 150 number of steps = 76 reward = 20.0\n",
      "DONE episode = 151 number of steps = 75 reward = 60.0\n",
      "DONE episode = 152 number of steps = 257 reward = 100.0\n",
      "DONE episode = 153 number of steps = 74 reward = 40.0\n",
      "DONE episode = 154 number of steps = 75 reward = 40.0\n",
      "DONE episode = 155 number of steps = 507 reward = 120.0\n",
      "DONE episode = 156 number of steps = 79 reward = 60.0\n",
      "DONE episode = 157 number of steps = 79 reward = 60.0\n",
      "DONE episode = 158 number of steps = 127 reward = 60.0\n",
      "DONE episode = 159 number of steps = 75 reward = 60.0\n",
      "DONE episode = 160 number of steps = 82 reward = 80.0\n",
      "DONE episode = 161 number of steps = 75 reward = 60.0\n",
      "DONE episode = 162 number of steps = 76 reward = 40.0\n",
      "DONE episode = 163 number of steps = 77 reward = 80.0\n",
      "DONE episode = 164 number of steps = 370 reward = 140.0\n",
      "DONE episode = 165 number of steps = 81 reward = 60.0\n",
      "DONE episode = 166 number of steps = 79 reward = 60.0\n",
      "DONE episode = 167 number of steps = 75 reward = 40.0\n",
      "DONE episode = 168 number of steps = 76 reward = 80.0\n",
      "DONE episode = 169 number of steps = 78 reward = 60.0\n",
      "DONE episode = 170 number of steps = 75 reward = 40.0\n",
      "DONE episode = 171 number of steps = 75 reward = 40.0\n",
      "DONE episode = 172 number of steps = 78 reward = 60.0\n",
      "DONE episode = 173 number of steps = 75 reward = 40.0\n",
      "DONE episode = 174 number of steps = 127 reward = 240.0\n",
      "DONE episode = 175 number of steps = 76 reward = 40.0\n",
      "DONE episode = 176 number of steps = 98 reward = 100.0\n",
      "DONE episode = 177 number of steps = 80 reward = 40.0\n",
      "DONE episode = 178 number of steps = 126 reward = 200.0\n",
      "DONE episode = 179 number of steps = 81 reward = 60.0\n",
      "DONE episode = 180 number of steps = 78 reward = 40.0\n",
      "DONE episode = 181 number of steps = 96 reward = 60.0\n",
      "DONE episode = 182 number of steps = 193 reward = 180.0\n",
      "DONE episode = 183 number of steps = 78 reward = 40.0\n",
      "DONE episode = 184 number of steps = 80 reward = 60.0\n",
      "DONE episode = 185 number of steps = 77 reward = 20.0\n",
      "DONE episode = 186 number of steps = 141 reward = 40.0\n",
      "DONE episode = 187 number of steps = 81 reward = 80.0\n",
      "DONE episode = 188 number of steps = 878 reward = 260.0\n",
      "DONE episode = 189 number of steps = 335 reward = 300.0\n",
      "DONE episode = 190 number of steps = 78 reward = 40.0\n",
      "DONE episode = 191 number of steps = 75 reward = 20.0\n",
      "DONE episode = 192 number of steps = 75 reward = 40.0\n",
      "DONE episode = 193 number of steps = 80 reward = 80.0\n",
      "DONE episode = 194 number of steps = 176 reward = 180.0\n",
      "DONE episode = 195 number of steps = 129 reward = 60.0\n",
      "DONE episode = 196 number of steps = 78 reward = 40.0\n",
      "DONE episode = 197 number of steps = 96 reward = 80.0\n",
      "DONE episode = 198 number of steps = 80 reward = 60.0\n",
      "DONE episode = 199 number of steps = 80 reward = 60.0\n",
      "DONE episode = 200 number of steps = 81 reward = 60.0\n",
      "DONE episode = 201 number of steps = 80 reward = 60.0\n",
      "DONE episode = 202 number of steps = 77 reward = 60.0\n",
      "DONE episode = 203 number of steps = 78 reward = 40.0\n",
      "DONE episode = 204 number of steps = 75 reward = 40.0\n",
      "DONE episode = 205 number of steps = 76 reward = 60.0\n",
      "DONE episode = 206 number of steps = 175 reward = 80.0\n",
      "DONE episode = 207 number of steps = 81 reward = 40.0\n",
      "DONE episode = 208 number of steps = 79 reward = 40.0\n",
      "DONE episode = 209 number of steps = 91 reward = 80.0\n",
      "DONE episode = 210 number of steps = 79 reward = 120.0\n",
      "DONE episode = 211 number of steps = 82 reward = 60.0\n",
      "DONE episode = 212 number of steps = 96 reward = 60.0\n",
      "DONE episode = 213 number of steps = 77 reward = 60.0\n",
      "DONE episode = 214 number of steps = 205 reward = 80.0\n",
      "DONE episode = 215 number of steps = 95 reward = 60.0\n",
      "DONE episode = 216 number of steps = 80 reward = 40.0\n",
      "DONE episode = 217 number of steps = 81 reward = 20.0\n",
      "DONE episode = 218 number of steps = 97 reward = 40.0\n",
      "DONE episode = 219 number of steps = 76 reward = 40.0\n",
      "DONE episode = 220 number of steps = 81 reward = 40.0\n",
      "DONE episode = 221 number of steps = 77 reward = 60.0\n",
      "DONE episode = 222 number of steps = 78 reward = 40.0\n",
      "DONE episode = 223 number of steps = 81 reward = 40.0\n",
      "DONE episode = 224 number of steps = 77 reward = 60.0\n",
      "DONE episode = 225 number of steps = 79 reward = 120.0\n",
      "DONE episode = 226 number of steps = 75 reward = 40.0\n",
      "DONE episode = 227 number of steps = 219 reward = 60.0\n",
      "DONE episode = 228 number of steps = 77 reward = 40.0\n",
      "DONE episode = 229 number of steps = 129 reward = 160.0\n",
      "DONE episode = 230 number of steps = 82 reward = 20.0\n",
      "DONE episode = 231 number of steps = 82 reward = 80.0\n",
      "DONE episode = 232 number of steps = 509 reward = 120.0\n",
      "DONE episode = 233 number of steps = 95 reward = 20.0\n",
      "DONE episode = 234 number of steps = 78 reward = 80.0\n",
      "DONE episode = 235 number of steps = 75 reward = 80.0\n",
      "DONE episode = 236 number of steps = 81 reward = 40.0\n",
      "DONE episode = 237 number of steps = 96 reward = 140.0\n",
      "DONE episode = 238 number of steps = 78 reward = 80.0\n",
      "DONE episode = 239 number of steps = 77 reward = 40.0\n",
      "DONE episode = 240 number of steps = 77 reward = 40.0\n",
      "DONE episode = 241 number of steps = 77 reward = 20.0\n",
      "DONE episode = 242 number of steps = 78 reward = 40.0\n",
      "DONE episode = 243 number of steps = 80 reward = 40.0\n",
      "DONE episode = 244 number of steps = 77 reward = 60.0\n",
      "DONE episode = 245 number of steps = 81 reward = 40.0\n",
      "DONE episode = 246 number of steps = 81 reward = 80.0\n",
      "DONE episode = 247 number of steps = 77 reward = 20.0\n",
      "DONE episode = 248 number of steps = 220 reward = 120.0\n",
      "DONE episode = 249 number of steps = 81 reward = 60.0\n",
      "DONE episode = 250 number of steps = 79 reward = 80.0\n",
      "DONE episode = 251 number of steps = 75 reward = 40.0\n",
      "DONE episode = 252 number of steps = 80 reward = 60.0\n",
      "DONE episode = 253 number of steps = 80 reward = 40.0\n",
      "DONE episode = 254 number of steps = 237 reward = 160.0\n",
      "DONE episode = 255 number of steps = 224 reward = 160.0\n",
      "DONE episode = 256 number of steps = 81 reward = 100.0\n",
      "DONE episode = 257 number of steps = 79 reward = 40.0\n",
      "DONE episode = 258 number of steps = 96 reward = 60.0\n",
      "DONE episode = 259 number of steps = 75 reward = 60.0\n",
      "DONE episode = 260 number of steps = 77 reward = 60.0\n",
      "DONE episode = 261 number of steps = 95 reward = 80.0\n",
      "DONE episode = 262 number of steps = 79 reward = 40.0\n",
      "DONE episode = 263 number of steps = 174 reward = 200.0\n",
      "DONE episode = 264 number of steps = 125 reward = 60.0\n",
      "DONE episode = 265 number of steps = 145 reward = 240.0\n",
      "DONE episode = 266 number of steps = 76 reward = 20.0\n",
      "DONE episode = 267 number of steps = 77 reward = 60.0\n",
      "DONE episode = 268 number of steps = 79 reward = 20.0\n",
      "DONE episode = 269 number of steps = 95 reward = 120.0\n",
      "DONE episode = 270 number of steps = 284 reward = 280.0\n",
      "DONE episode = 271 number of steps = 335 reward = 200.0\n",
      "DONE episode = 272 number of steps = 112 reward = 160.0\n",
      "DONE episode = 273 number of steps = 127 reward = 100.0\n",
      "DONE episode = 274 number of steps = 76 reward = 60.0\n",
      "DONE episode = 275 number of steps = 76 reward = 20.0\n",
      "DONE episode = 276 number of steps = 78 reward = 20.0\n",
      "DONE episode = 277 number of steps = 79 reward = 80.0\n",
      "DONE episode = 278 number of steps = 77 reward = 40.0\n",
      "DONE episode = 279 number of steps = 129 reward = 80.0\n",
      "DONE episode = 280 number of steps = 127 reward = 220.0\n",
      "DONE episode = 281 number of steps = 81 reward = 20.0\n",
      "DONE episode = 282 number of steps = 76 reward = 40.0\n",
      "DONE episode = 283 number of steps = 75 reward = 40.0\n",
      "DONE episode = 284 number of steps = 80 reward = 40.0\n",
      "DONE episode = 285 number of steps = 123 reward = 60.0\n",
      "DONE episode = 286 number of steps = 77 reward = 40.0\n",
      "DONE episode = 287 number of steps = 2141 reward = 300.0\n",
      "DONE episode = 288 number of steps = 77 reward = 40.0\n",
      "DONE episode = 289 number of steps = 416 reward = 100.0\n",
      "DONE episode = 290 number of steps = 80 reward = 60.0\n",
      "DONE episode = 291 number of steps = 173 reward = 160.0\n",
      "DONE episode = 292 number of steps = 76 reward = 0.0\n",
      "DONE episode = 293 number of steps = 77 reward = 40.0\n",
      "DONE episode = 294 number of steps = 77 reward = 40.0\n",
      "DONE episode = 295 number of steps = 667 reward = 240.0\n",
      "DONE episode = 296 number of steps = 78 reward = 40.0\n",
      "DONE episode = 297 number of steps = 319 reward = 140.0\n",
      "DONE episode = 298 number of steps = 177 reward = 60.0\n",
      "DONE episode = 299 number of steps = 80 reward = 60.0\n",
      "DONE episode = 300 number of steps = 81 reward = 20.0\n",
      "DONE episode = 301 number of steps = 80 reward = 60.0\n",
      "DONE episode = 302 number of steps = 78 reward = 80.0\n",
      "DONE episode = 303 number of steps = 202 reward = 60.0\n",
      "DONE episode = 304 number of steps = 110 reward = 180.0\n",
      "DONE episode = 305 number of steps = 110 reward = 80.0\n",
      "DONE episode = 306 number of steps = 79 reward = 40.0\n",
      "DONE episode = 307 number of steps = 911 reward = 140.0\n",
      "DONE episode = 308 number of steps = 124 reward = 60.0\n",
      "DONE episode = 309 number of steps = 77 reward = 40.0\n",
      "DONE episode = 310 number of steps = 1101 reward = 420.0\n",
      "DONE episode = 311 number of steps = 82 reward = 60.0\n",
      "DONE episode = 312 number of steps = 75 reward = 40.0\n",
      "DONE episode = 313 number of steps = 187 reward = 120.0\n",
      "DONE episode = 314 number of steps = 95 reward = 60.0\n",
      "DONE episode = 315 number of steps = 80 reward = 60.0\n",
      "DONE episode = 316 number of steps = 75 reward = 60.0\n",
      "DONE episode = 317 number of steps = 75 reward = 40.0\n",
      "DONE episode = 318 number of steps = 78 reward = 40.0\n",
      "DONE episode = 319 number of steps = 124 reward = 60.0\n",
      "DONE episode = 320 number of steps = 125 reward = 180.0\n",
      "DONE episode = 321 number of steps = 577 reward = 140.0\n",
      "DONE episode = 322 number of steps = 79 reward = 40.0\n",
      "DONE episode = 323 number of steps = 111 reward = 60.0\n",
      "DONE episode = 324 number of steps = 175 reward = 180.0\n",
      "DONE episode = 325 number of steps = 78 reward = 60.0\n",
      "DONE episode = 326 number of steps = 205 reward = 220.0\n",
      "DONE episode = 327 number of steps = 75 reward = 0.0\n",
      "DONE episode = 328 number of steps = 79 reward = 80.0\n",
      "DONE episode = 329 number of steps = 79 reward = 40.0\n",
      "DONE episode = 330 number of steps = 80 reward = 20.0\n",
      "DONE episode = 331 number of steps = 431 reward = 140.0\n",
      "DONE episode = 332 number of steps = 112 reward = 40.0\n",
      "DONE episode = 333 number of steps = 545 reward = 1350.0\n",
      "DONE episode = 334 number of steps = 77 reward = 60.0\n",
      "DONE episode = 335 number of steps = 75 reward = 60.0\n",
      "DONE episode = 336 number of steps = 81 reward = 80.0\n",
      "DONE episode = 337 number of steps = 77 reward = 80.0\n",
      "DONE episode = 338 number of steps = 123 reward = 60.0\n",
      "DONE episode = 339 number of steps = 92 reward = 60.0\n",
      "DONE episode = 340 number of steps = 174 reward = 80.0\n",
      "DONE episode = 341 number of steps = 170 reward = 80.0\n",
      "DONE episode = 342 number of steps = 128 reward = 180.0\n",
      "DONE episode = 343 number of steps = 97 reward = 40.0\n",
      "DONE episode = 344 number of steps = 77 reward = 40.0\n",
      "DONE episode = 345 number of steps = 114 reward = 80.0\n",
      "DONE episode = 346 number of steps = 956 reward = 120.0\n",
      "DONE episode = 347 number of steps = 765 reward = 180.0\n",
      "DONE episode = 348 number of steps = 330 reward = 220.0\n",
      "DONE episode = 349 number of steps = 1245 reward = 220.0\n",
      "DONE episode = 350 number of steps = 79 reward = 60.0\n",
      "DONE episode = 351 number of steps = 76 reward = 40.0\n",
      "DONE episode = 352 number of steps = 80 reward = 40.0\n",
      "DONE episode = 353 number of steps = 76 reward = 40.0\n",
      "DONE episode = 354 number of steps = 1052 reward = 220.0\n",
      "DONE episode = 355 number of steps = 77 reward = 140.0\n",
      "DONE episode = 356 number of steps = 81 reward = 80.0\n",
      "DONE episode = 357 number of steps = 125 reward = 60.0\n",
      "DONE episode = 358 number of steps = 81 reward = 40.0\n",
      "DONE episode = 359 number of steps = 190 reward = 120.0\n",
      "DONE episode = 360 number of steps = 224 reward = 60.0\n",
      "DONE episode = 361 number of steps = 225 reward = 140.0\n",
      "DONE episode = 362 number of steps = 176 reward = 140.0\n",
      "DONE episode = 363 number of steps = 193 reward = 160.0\n",
      "DONE episode = 364 number of steps = 191 reward = 240.0\n",
      "DONE episode = 365 number of steps = 80 reward = 60.0\n",
      "DONE episode = 366 number of steps = 210 reward = 160.0\n",
      "DONE episode = 367 number of steps = 204 reward = 40.0\n",
      "DONE episode = 368 number of steps = 125 reward = 160.0\n",
      "DONE episode = 369 number of steps = 75 reward = 40.0\n",
      "DONE episode = 370 number of steps = 267 reward = 200.0\n",
      "DONE episode = 371 number of steps = 574 reward = 200.0\n",
      "DONE episode = 372 number of steps = 82 reward = 40.0\n",
      "DONE episode = 373 number of steps = 205 reward = 80.0\n",
      "DONE episode = 374 number of steps = 78 reward = 40.0\n",
      "DONE episode = 375 number of steps = 91 reward = 60.0\n",
      "DONE episode = 376 number of steps = 78 reward = 80.0\n",
      "DONE episode = 377 number of steps = 76 reward = 40.0\n",
      "DONE episode = 378 number of steps = 76 reward = 40.0\n",
      "DONE episode = 379 number of steps = 465 reward = 80.0\n",
      "DONE episode = 380 number of steps = 113 reward = 100.0\n",
      "DONE episode = 381 number of steps = 81 reward = 60.0\n",
      "DONE episode = 382 number of steps = 223 reward = 420.0\n",
      "DONE episode = 383 number of steps = 399 reward = 80.0\n",
      "DONE episode = 384 number of steps = 1659 reward = 120.0\n",
      "DONE episode = 385 number of steps = 76 reward = 80.0\n",
      "DONE episode = 386 number of steps = 75 reward = 20.0\n",
      "DONE episode = 387 number of steps = 79 reward = 40.0\n",
      "DONE episode = 388 number of steps = 127 reward = 100.0\n",
      "DONE episode = 389 number of steps = 126 reward = 80.0\n",
      "DONE episode = 390 number of steps = 76 reward = 60.0\n",
      "DONE episode = 391 number of steps = 284 reward = 200.0\n",
      "DONE episode = 392 number of steps = 878 reward = 120.0\n",
      "DONE episode = 393 number of steps = 108 reward = 160.0\n",
      "DONE episode = 394 number of steps = 78 reward = 120.0\n",
      "DONE episode = 395 number of steps = 98 reward = 80.0\n",
      "DONE episode = 396 number of steps = 77 reward = 40.0\n",
      "DONE episode = 397 number of steps = 75 reward = 40.0\n",
      "DONE episode = 398 number of steps = 1278 reward = 200.0\n",
      "DONE episode = 399 number of steps = 1146 reward = 950.0\n",
      "DONE episode = 400 number of steps = 128 reward = 140.0\n",
      "DONE episode = 401 number of steps = 129 reward = 80.0\n",
      "DONE episode = 402 number of steps = 76 reward = 40.0\n",
      "DONE episode = 403 number of steps = 98 reward = 180.0\n",
      "DONE episode = 404 number of steps = 125 reward = 100.0\n",
      "DONE episode = 405 number of steps = 128 reward = 80.0\n",
      "DONE episode = 406 number of steps = 81 reward = 120.0\n",
      "DONE episode = 407 number of steps = 127 reward = 100.0\n",
      "DONE episode = 408 number of steps = 209 reward = 140.0\n",
      "DONE episode = 409 number of steps = 109 reward = 40.0\n",
      "DONE episode = 410 number of steps = 95 reward = 160.0\n",
      "DONE episode = 411 number of steps = 80 reward = 40.0\n",
      "DONE episode = 412 number of steps = 194 reward = 240.0\n",
      "DONE episode = 413 number of steps = 2128 reward = 540.0\n",
      "DONE episode = 414 number of steps = 701 reward = 60.0\n",
      "DONE episode = 415 number of steps = 1771 reward = 100.0\n",
      "DONE episode = 416 number of steps = 79 reward = 40.0\n",
      "DONE episode = 417 number of steps = 76 reward = 60.0\n",
      "DONE episode = 418 number of steps = 128 reward = 180.0\n",
      "DONE episode = 419 number of steps = 80 reward = 60.0\n",
      "DONE episode = 420 number of steps = 77 reward = 60.0\n",
      "DONE episode = 421 number of steps = 75 reward = 20.0\n",
      "DONE episode = 422 number of steps = 127 reward = 80.0\n",
      "DONE episode = 423 number of steps = 79 reward = 40.0\n",
      "DONE episode = 424 number of steps = 77 reward = 40.0\n",
      "DONE episode = 425 number of steps = 78 reward = 60.0\n",
      "DONE episode = 426 number of steps = 367 reward = 80.0\n",
      "DONE episode = 427 number of steps = 78 reward = 60.0\n",
      "DONE episode = 428 number of steps = 77 reward = 60.0\n",
      "DONE episode = 429 number of steps = 80 reward = 80.0\n",
      "DONE episode = 430 number of steps = 78 reward = 80.0\n",
      "DONE episode = 431 number of steps = 175 reward = 140.0\n",
      "DONE episode = 432 number of steps = 124 reward = 120.0\n",
      "DONE episode = 433 number of steps = 78 reward = 40.0\n",
      "DONE episode = 434 number of steps = 78 reward = 40.0\n",
      "DONE episode = 435 number of steps = 81 reward = 40.0\n",
      "DONE episode = 436 number of steps = 76 reward = 40.0\n",
      "DONE episode = 437 number of steps = 161 reward = 80.0\n",
      "DONE episode = 438 number of steps = 77 reward = 40.0\n",
      "DONE episode = 439 number of steps = 76 reward = 40.0\n",
      "DONE episode = 440 number of steps = 94 reward = 80.0\n",
      "DONE episode = 441 number of steps = 95 reward = 60.0\n",
      "DONE episode = 442 number of steps = 80 reward = 40.0\n",
      "DONE episode = 443 number of steps = 79 reward = 20.0\n",
      "DONE episode = 444 number of steps = 79 reward = 40.0\n",
      "DONE episode = 445 number of steps = 80 reward = 40.0\n",
      "DONE episode = 446 number of steps = 81 reward = 20.0\n",
      "DONE episode = 447 number of steps = 81 reward = 20.0\n",
      "DONE episode = 448 number of steps = 76 reward = 40.0\n",
      "DONE episode = 449 number of steps = 76 reward = 80.0\n",
      "DONE episode = 450 number of steps = 82 reward = 80.0\n",
      "DONE episode = 451 number of steps = 271 reward = 140.0\n",
      "DONE episode = 452 number of steps = 80 reward = 140.0\n",
      "DONE episode = 453 number of steps = 74 reward = 80.0\n",
      "DONE episode = 454 number of steps = 94 reward = 80.0\n",
      "DONE episode = 455 number of steps = 78 reward = 80.0\n",
      "DONE episode = 456 number of steps = 187 reward = 200.0\n",
      "DONE episode = 457 number of steps = 78 reward = 0.0\n",
      "DONE episode = 458 number of steps = 97 reward = 40.0\n",
      "DONE episode = 459 number of steps = 79 reward = 60.0\n",
      "DONE episode = 460 number of steps = 241 reward = 360.0\n",
      "DONE episode = 461 number of steps = 80 reward = 40.0\n",
      "DONE episode = 462 number of steps = 143 reward = 80.0\n",
      "DONE episode = 463 number of steps = 82 reward = 40.0\n",
      "DONE episode = 464 number of steps = 1790 reward = 360.0\n",
      "DONE episode = 465 number of steps = 80 reward = 60.0\n",
      "DONE episode = 466 number of steps = 77 reward = 60.0\n",
      "DONE episode = 467 number of steps = 94 reward = 60.0\n",
      "DONE episode = 468 number of steps = 76 reward = 40.0\n",
      "DONE episode = 469 number of steps = 122 reward = 320.0\n",
      "DONE episode = 470 number of steps = 76 reward = 40.0\n",
      "DONE episode = 471 number of steps = 75 reward = 20.0\n",
      "DONE episode = 472 number of steps = 76 reward = 40.0\n",
      "DONE episode = 473 number of steps = 76 reward = 40.0\n",
      "DONE episode = 474 number of steps = 96 reward = 40.0\n",
      "DONE episode = 475 number of steps = 75 reward = 40.0\n",
      "DONE episode = 476 number of steps = 513 reward = 180.0\n",
      "DONE episode = 477 number of steps = 82 reward = 40.0\n",
      "DONE episode = 478 number of steps = 92 reward = 20.0\n",
      "DONE episode = 479 number of steps = 82 reward = 80.0\n",
      "DONE episode = 480 number of steps = 124 reward = 60.0\n",
      "DONE episode = 481 number of steps = 80 reward = 40.0\n",
      "DONE episode = 482 number of steps = 75 reward = 60.0\n",
      "DONE episode = 483 number of steps = 206 reward = 180.0\n",
      "DONE episode = 484 number of steps = 94 reward = 120.0\n",
      "DONE episode = 485 number of steps = 76 reward = 40.0\n",
      "DONE episode = 486 number of steps = 78 reward = 60.0\n",
      "DONE episode = 487 number of steps = 80 reward = 0.0\n",
      "DONE episode = 488 number of steps = 80 reward = 40.0\n",
      "DONE episode = 489 number of steps = 347 reward = 40.0\n",
      "DONE episode = 490 number of steps = 1627 reward = 120.0\n",
      "DONE episode = 491 number of steps = 78 reward = 60.0\n",
      "DONE episode = 492 number of steps = 79 reward = 80.0\n",
      "DONE episode = 493 number of steps = 81 reward = 60.0\n",
      "DONE episode = 494 number of steps = 81 reward = 40.0\n",
      "DONE episode = 495 number of steps = 78 reward = 20.0\n",
      "DONE episode = 496 number of steps = 283 reward = 180.0\n",
      "DONE episode = 497 number of steps = 127 reward = 100.0\n",
      "DONE episode = 498 number of steps = 80 reward = 20.0\n",
      "DONE episode = 499 number of steps = 77 reward = 60.0\n",
      "DONE episode = 500 number of steps = 77 reward = 60.0\n",
      "DONE episode = 501 number of steps = 82 reward = 60.0\n",
      "DONE episode = 502 number of steps = 77 reward = 60.0\n",
      "DONE episode = 503 number of steps = 93 reward = 40.0\n",
      "DONE episode = 504 number of steps = 267 reward = 120.0\n",
      "DONE episode = 505 number of steps = 192 reward = 180.0\n",
      "DONE episode = 506 number of steps = 622 reward = 200.0\n",
      "DONE episode = 507 number of steps = 76 reward = 20.0\n",
      "DONE episode = 508 number of steps = 93 reward = 60.0\n",
      "DONE episode = 509 number of steps = 123 reward = 100.0\n",
      "DONE episode = 510 number of steps = 76 reward = 20.0\n",
      "DONE episode = 511 number of steps = 126 reward = 100.0\n",
      "DONE episode = 512 number of steps = 589 reward = 100.0\n",
      "DONE episode = 513 number of steps = 97 reward = 60.0\n",
      "DONE episode = 514 number of steps = 273 reward = 120.0\n",
      "DONE episode = 515 number of steps = 78 reward = 80.0\n",
      "DONE episode = 516 number of steps = 959 reward = 260.0\n",
      "DONE episode = 517 number of steps = 79 reward = 40.0\n",
      "DONE episode = 518 number of steps = 77 reward = 40.0\n",
      "DONE episode = 519 number of steps = 156 reward = 60.0\n",
      "DONE episode = 520 number of steps = 80 reward = 20.0\n",
      "DONE episode = 521 number of steps = 191 reward = 60.0\n",
      "DONE episode = 522 number of steps = 75 reward = 40.0\n",
      "DONE episode = 523 number of steps = 75 reward = 40.0\n",
      "DONE episode = 524 number of steps = 140 reward = 180.0\n",
      "DONE episode = 525 number of steps = 96 reward = 60.0\n",
      "DONE episode = 526 number of steps = 703 reward = 440.0\n",
      "DONE episode = 527 number of steps = 127 reward = 140.0\n",
      "DONE episode = 528 number of steps = 76 reward = 20.0\n",
      "DONE episode = 529 number of steps = 81 reward = 20.0\n",
      "DONE episode = 530 number of steps = 157 reward = 80.0\n",
      "DONE episode = 531 number of steps = 75 reward = 40.0\n",
      "DONE episode = 532 number of steps = 81 reward = 40.0\n",
      "DONE episode = 533 number of steps = 78 reward = 80.0\n",
      "DONE episode = 534 number of steps = 81 reward = 60.0\n",
      "DONE episode = 535 number of steps = 76 reward = 80.0\n",
      "DONE episode = 536 number of steps = 127 reward = 80.0\n",
      "DONE episode = 537 number of steps = 80 reward = 40.0\n",
      "DONE episode = 538 number of steps = 75 reward = 40.0\n",
      "DONE episode = 539 number of steps = 81 reward = 40.0\n",
      "DONE episode = 540 number of steps = 82 reward = 80.0\n",
      "DONE episode = 541 number of steps = 138 reward = 40.0\n",
      "DONE episode = 542 number of steps = 191 reward = 40.0\n",
      "DONE episode = 543 number of steps = 81 reward = 40.0\n",
      "DONE episode = 544 number of steps = 367 reward = 300.0\n",
      "DONE episode = 545 number of steps = 75 reward = 40.0\n",
      "DONE episode = 546 number of steps = 78 reward = 80.0\n",
      "DONE episode = 547 number of steps = 79 reward = 140.0\n",
      "DONE episode = 548 number of steps = 81 reward = 20.0\n",
      "DONE episode = 549 number of steps = 192 reward = 60.0\n",
      "DONE episode = 550 number of steps = 539 reward = 60.0\n",
      "DONE episode = 551 number of steps = 81 reward = 20.0\n",
      "DONE episode = 552 number of steps = 78 reward = 60.0\n",
      "DONE episode = 553 number of steps = 209 reward = 260.0\n",
      "DONE episode = 554 number of steps = 74 reward = 40.0\n",
      "DONE episode = 555 number of steps = 368 reward = 20.0\n",
      "DONE episode = 556 number of steps = 625 reward = 480.0\n",
      "DONE episode = 557 number of steps = 76 reward = 40.0\n",
      "DONE episode = 558 number of steps = 302 reward = 220.0\n",
      "DONE episode = 559 number of steps = 76 reward = 40.0\n",
      "DONE episode = 560 number of steps = 80 reward = 60.0\n",
      "DONE episode = 561 number of steps = 78 reward = 60.0\n",
      "DONE episode = 562 number of steps = 126 reward = 100.0\n",
      "DONE episode = 563 number of steps = 74 reward = 20.0\n",
      "DONE episode = 564 number of steps = 77 reward = 40.0\n",
      "DONE episode = 565 number of steps = 80 reward = 0.0\n",
      "DONE episode = 566 number of steps = 77 reward = 20.0\n",
      "DONE episode = 567 number of steps = 113 reward = 40.0\n",
      "DONE episode = 568 number of steps = 110 reward = 160.0\n",
      "DONE episode = 569 number of steps = 126 reward = 200.0\n",
      "DONE episode = 570 number of steps = 112 reward = 80.0\n",
      "DONE episode = 571 number of steps = 75 reward = 40.0\n",
      "DONE episode = 572 number of steps = 127 reward = 80.0\n",
      "DONE episode = 573 number of steps = 78 reward = 40.0\n",
      "DONE episode = 574 number of steps = 221 reward = 120.0\n",
      "DONE episode = 575 number of steps = 127 reward = 120.0\n",
      "DONE episode = 576 number of steps = 624 reward = 520.0\n",
      "DONE episode = 577 number of steps = 78 reward = 40.0\n",
      "DONE episode = 578 number of steps = 78 reward = 100.0\n",
      "DONE episode = 579 number of steps = 74 reward = 40.0\n",
      "DONE episode = 580 number of steps = 80 reward = 120.0\n",
      "DONE episode = 581 number of steps = 316 reward = 140.0\n",
      "DONE episode = 582 number of steps = 175 reward = 60.0\n",
      "DONE episode = 583 number of steps = 107 reward = 60.0\n",
      "DONE episode = 584 number of steps = 77 reward = 100.0\n",
      "DONE episode = 585 number of steps = 189 reward = 40.0\n",
      "DONE episode = 586 number of steps = 92 reward = 20.0\n",
      "DONE episode = 587 number of steps = 2830 reward = 750.0\n",
      "DONE episode = 588 number of steps = 322 reward = 340.0\n",
      "DONE episode = 589 number of steps = 75 reward = 40.0\n",
      "DONE episode = 590 number of steps = 126 reward = 120.0\n",
      "DONE episode = 591 number of steps = 140 reward = 160.0\n",
      "DONE episode = 592 number of steps = 80 reward = 40.0\n",
      "DONE episode = 593 number of steps = 109 reward = 140.0\n",
      "DONE episode = 594 number of steps = 113 reward = 80.0\n",
      "DONE episode = 595 number of steps = 96 reward = 40.0\n",
      "DONE episode = 596 number of steps = 92 reward = 80.0\n",
      "DONE episode = 597 number of steps = 513 reward = 120.0\n",
      "DONE episode = 598 number of steps = 76 reward = 80.0\n",
      "DONE episode = 599 number of steps = 81 reward = 0.0\n",
      "DONE episode = 600 number of steps = 80 reward = 60.0\n",
      "DONE episode = 601 number of steps = 81 reward = 40.0\n",
      "DONE episode = 602 number of steps = 80 reward = 20.0\n",
      "DONE episode = 603 number of steps = 142 reward = 60.0\n",
      "DONE episode = 604 number of steps = 82 reward = 0.0\n",
      "DONE episode = 605 number of steps = 91 reward = 60.0\n",
      "DONE episode = 606 number of steps = 77 reward = 40.0\n",
      "DONE episode = 607 number of steps = 92 reward = 40.0\n",
      "DONE episode = 608 number of steps = 139 reward = 60.0\n",
      "DONE episode = 609 number of steps = 80 reward = 0.0\n",
      "DONE episode = 610 number of steps = 126 reward = 80.0\n",
      "DONE episode = 611 number of steps = 370 reward = 180.0\n",
      "DONE episode = 612 number of steps = 78 reward = 120.0\n",
      "DONE episode = 613 number of steps = 76 reward = 40.0\n",
      "DONE episode = 614 number of steps = 223 reward = 120.0\n",
      "DONE episode = 615 number of steps = 223 reward = 160.0\n",
      "DONE episode = 616 number of steps = 79 reward = 60.0\n",
      "DONE episode = 617 number of steps = 96 reward = 40.0\n",
      "DONE episode = 618 number of steps = 91 reward = 80.0\n",
      "DONE episode = 619 number of steps = 78 reward = 20.0\n",
      "DONE episode = 620 number of steps = 271 reward = 140.0\n",
      "DONE episode = 621 number of steps = 283 reward = 100.0\n",
      "DONE episode = 622 number of steps = 76 reward = 40.0\n",
      "DONE episode = 623 number of steps = 81 reward = 40.0\n",
      "DONE episode = 624 number of steps = 92 reward = 40.0\n",
      "DONE episode = 625 number of steps = 77 reward = 40.0\n",
      "DONE episode = 626 number of steps = 108 reward = 160.0\n",
      "DONE episode = 627 number of steps = 127 reward = 160.0\n",
      "DONE episode = 628 number of steps = 79 reward = 40.0\n",
      "DONE episode = 629 number of steps = 74 reward = 20.0\n",
      "DONE episode = 630 number of steps = 81 reward = 40.0\n",
      "DONE episode = 631 number of steps = 91 reward = 80.0\n",
      "DONE episode = 632 number of steps = 143 reward = 200.0\n",
      "DONE episode = 633 number of steps = 127 reward = 140.0\n",
      "DONE episode = 634 number of steps = 123 reward = 100.0\n",
      "DONE episode = 635 number of steps = 107 reward = 200.0\n",
      "DONE episode = 636 number of steps = 171 reward = 60.0\n",
      "DONE episode = 637 number of steps = 79 reward = 40.0\n",
      "DONE episode = 638 number of steps = 78 reward = 80.0\n",
      "DONE episode = 639 number of steps = 174 reward = 80.0\n",
      "DONE episode = 640 number of steps = 79 reward = 40.0\n",
      "DONE episode = 641 number of steps = 78 reward = 40.0\n",
      "DONE episode = 642 number of steps = 76 reward = 40.0\n",
      "DONE episode = 643 number of steps = 76 reward = 20.0\n",
      "DONE episode = 644 number of steps = 78 reward = 40.0\n",
      "DONE episode = 645 number of steps = 187 reward = 40.0\n",
      "DONE episode = 646 number of steps = 77 reward = 20.0\n",
      "DONE episode = 647 number of steps = 94 reward = 40.0\n",
      "DONE episode = 648 number of steps = 75 reward = 20.0\n",
      "DONE episode = 649 number of steps = 113 reward = 60.0\n",
      "DONE episode = 650 number of steps = 108 reward = 120.0\n",
      "DONE episode = 651 number of steps = 79 reward = 40.0\n",
      "DONE episode = 652 number of steps = 81 reward = 60.0\n",
      "DONE episode = 653 number of steps = 111 reward = 80.0\n",
      "DONE episode = 654 number of steps = 112 reward = 140.0\n",
      "DONE episode = 655 number of steps = 77 reward = 40.0\n",
      "DONE episode = 656 number of steps = 124 reward = 120.0\n",
      "DONE episode = 657 number of steps = 97 reward = 100.0\n",
      "DONE episode = 658 number of steps = 112 reward = 100.0\n",
      "DONE episode = 659 number of steps = 77 reward = 40.0\n",
      "DONE episode = 660 number of steps = 78 reward = 20.0\n",
      "DONE episode = 661 number of steps = 79 reward = 40.0\n",
      "DONE episode = 662 number of steps = 81 reward = 60.0\n",
      "DONE episode = 663 number of steps = 112 reward = 180.0\n",
      "DONE episode = 664 number of steps = 896 reward = 200.0\n",
      "DONE episode = 665 number of steps = 79 reward = 40.0\n",
      "DONE episode = 666 number of steps = 80 reward = 20.0\n",
      "DONE episode = 667 number of steps = 78 reward = 40.0\n",
      "DONE episode = 668 number of steps = 125 reward = 100.0\n",
      "DONE episode = 669 number of steps = 109 reward = 40.0\n",
      "DONE episode = 670 number of steps = 366 reward = 180.0\n",
      "DONE episode = 671 number of steps = 177 reward = 100.0\n",
      "DONE episode = 672 number of steps = 209 reward = 180.0\n",
      "DONE episode = 673 number of steps = 80 reward = 40.0\n",
      "DONE episode = 674 number of steps = 80 reward = 40.0\n",
      "DONE episode = 675 number of steps = 82 reward = 20.0\n",
      "DONE episode = 676 number of steps = 127 reward = 80.0\n",
      "DONE episode = 677 number of steps = 172 reward = 80.0\n",
      "DONE episode = 678 number of steps = 78 reward = 60.0\n",
      "DONE episode = 679 number of steps = 76 reward = 40.0\n",
      "DONE episode = 680 number of steps = 79 reward = 40.0\n",
      "DONE episode = 681 number of steps = 79 reward = 40.0\n",
      "DONE episode = 682 number of steps = 75 reward = 40.0\n",
      "DONE episode = 683 number of steps = 77 reward = 60.0\n",
      "DONE episode = 684 number of steps = 75 reward = 40.0\n",
      "DONE episode = 685 number of steps = 95 reward = 80.0\n",
      "DONE episode = 686 number of steps = 78 reward = 20.0\n",
      "DONE episode = 687 number of steps = 80 reward = 40.0\n",
      "DONE episode = 688 number of steps = 271 reward = 160.0\n",
      "DONE episode = 689 number of steps = 78 reward = 60.0\n",
      "DONE episode = 690 number of steps = 78 reward = 60.0\n",
      "DONE episode = 691 number of steps = 113 reward = 60.0\n",
      "DONE episode = 692 number of steps = 76 reward = 20.0\n",
      "DONE episode = 693 number of steps = 78 reward = 60.0\n",
      "DONE episode = 694 number of steps = 76 reward = 60.0\n",
      "DONE episode = 695 number of steps = 81 reward = 20.0\n",
      "DONE episode = 696 number of steps = 496 reward = 500.0\n",
      "DONE episode = 697 number of steps = 142 reward = 60.0\n",
      "DONE episode = 698 number of steps = 79 reward = 20.0\n",
      "DONE episode = 699 number of steps = 75 reward = 40.0\n",
      "DONE episode = 700 number of steps = 80 reward = 40.0\n",
      "DONE episode = 701 number of steps = 79 reward = 60.0\n",
      "DONE episode = 702 number of steps = 79 reward = 40.0\n",
      "DONE episode = 703 number of steps = 76 reward = 60.0\n",
      "DONE episode = 704 number of steps = 76 reward = 20.0\n",
      "DONE episode = 705 number of steps = 609 reward = 100.0\n",
      "DONE episode = 706 number of steps = 113 reward = 100.0\n",
      "DONE episode = 707 number of steps = 192 reward = 20.0\n",
      "DONE episode = 708 number of steps = 269 reward = 260.0\n",
      "DONE episode = 709 number of steps = 109 reward = 160.0\n",
      "DONE episode = 710 number of steps = 402 reward = 80.0\n",
      "DONE episode = 711 number of steps = 75 reward = 20.0\n",
      "DONE episode = 712 number of steps = 2558 reward = 180.0\n",
      "DONE episode = 713 number of steps = 78 reward = 40.0\n",
      "DONE episode = 714 number of steps = 75 reward = 60.0\n",
      "DONE episode = 715 number of steps = 141 reward = 60.0\n",
      "DONE episode = 716 number of steps = 81 reward = 20.0\n",
      "DONE episode = 717 number of steps = 78 reward = 60.0\n",
      "DONE episode = 718 number of steps = 82 reward = 40.0\n",
      "DONE episode = 719 number of steps = 79 reward = 40.0\n",
      "DONE episode = 720 number of steps = 79 reward = 40.0\n",
      "DONE episode = 721 number of steps = 238 reward = 80.0\n",
      "DONE episode = 722 number of steps = 75 reward = 20.0\n",
      "DONE episode = 723 number of steps = 109 reward = 80.0\n",
      "DONE episode = 724 number of steps = 128 reward = 120.0\n",
      "DONE episode = 725 number of steps = 110 reward = 40.0\n",
      "DONE episode = 726 number of steps = 109 reward = 80.0\n",
      "DONE episode = 727 number of steps = 95 reward = 20.0\n",
      "DONE episode = 728 number of steps = 127 reward = 60.0\n",
      "DONE episode = 729 number of steps = 1389 reward = 80.0\n",
      "DONE episode = 730 number of steps = 81 reward = 0.0\n",
      "DONE episode = 731 number of steps = 90 reward = 40.0\n",
      "DONE episode = 732 number of steps = 272 reward = 60.0\n",
      "DONE episode = 733 number of steps = 76 reward = 40.0\n",
      "DONE episode = 734 number of steps = 3023 reward = 180.0\n",
      "DONE episode = 735 number of steps = 123 reward = 40.0\n",
      "DONE episode = 736 number of steps = 288 reward = 140.0\n",
      "DONE episode = 737 number of steps = 113 reward = 60.0\n",
      "DONE episode = 738 number of steps = 124 reward = 80.0\n",
      "DONE episode = 739 number of steps = 130 reward = 60.0\n",
      "DONE episode = 740 number of steps = 78 reward = 40.0\n",
      "DONE episode = 741 number of steps = 75 reward = 20.0\n",
      "DONE episode = 742 number of steps = 78 reward = 20.0\n",
      "DONE episode = 743 number of steps = 75 reward = 60.0\n",
      "DONE episode = 744 number of steps = 80 reward = 20.0\n",
      "DONE episode = 745 number of steps = 95 reward = 80.0\n",
      "DONE episode = 746 number of steps = 114 reward = 60.0\n",
      "DONE episode = 747 number of steps = 124 reward = 140.0\n",
      "DONE episode = 748 number of steps = 127 reward = 140.0\n",
      "DONE episode = 749 number of steps = 172 reward = 140.0\n",
      "DONE episode = 750 number of steps = 80 reward = 20.0\n",
      "DONE episode = 751 number of steps = 79 reward = 40.0\n",
      "DONE episode = 752 number of steps = 75 reward = 40.0\n",
      "DONE episode = 753 number of steps = 77 reward = 40.0\n",
      "DONE episode = 754 number of steps = 80 reward = 20.0\n",
      "DONE episode = 755 number of steps = 110 reward = 80.0\n",
      "DONE episode = 756 number of steps = 434 reward = 260.0\n",
      "DONE episode = 757 number of steps = 144 reward = 140.0\n",
      "DONE episode = 758 number of steps = 129 reward = 100.0\n",
      "DONE episode = 759 number of steps = 75 reward = 20.0\n",
      "DONE episode = 760 number of steps = 79 reward = 20.0\n",
      "DONE episode = 761 number of steps = 79 reward = 20.0\n",
      "DONE episode = 762 number of steps = 190 reward = 120.0\n",
      "DONE episode = 763 number of steps = 1162 reward = 200.0\n",
      "DONE episode = 764 number of steps = 112 reward = 60.0\n",
      "DONE episode = 765 number of steps = 114 reward = 60.0\n",
      "DONE episode = 766 number of steps = 125 reward = 80.0\n",
      "DONE episode = 767 number of steps = 75 reward = 0.0\n",
      "DONE episode = 768 number of steps = 78 reward = 40.0\n",
      "DONE episode = 769 number of steps = 107 reward = 20.0\n",
      "DONE episode = 770 number of steps = 96 reward = 40.0\n",
      "DONE episode = 771 number of steps = 113 reward = 20.0\n",
      "DONE episode = 772 number of steps = 74 reward = 100.0\n",
      "DONE episode = 773 number of steps = 98 reward = 0.0\n",
      "DONE episode = 774 number of steps = 175 reward = 40.0\n",
      "DONE episode = 775 number of steps = 78 reward = 60.0\n",
      "DONE episode = 776 number of steps = 80 reward = 60.0\n",
      "DONE episode = 777 number of steps = 206 reward = 60.0\n",
      "DONE episode = 778 number of steps = 109 reward = 40.0\n",
      "DONE episode = 779 number of steps = 107 reward = 40.0\n",
      "DONE episode = 780 number of steps = 111 reward = 80.0\n",
      "DONE episode = 781 number of steps = 77 reward = 20.0\n",
      "DONE episode = 782 number of steps = 81 reward = 20.0\n",
      "DONE episode = 783 number of steps = 93 reward = 60.0\n",
      "DONE episode = 784 number of steps = 77 reward = 60.0\n",
      "DONE episode = 785 number of steps = 93 reward = 40.0\n",
      "DONE episode = 786 number of steps = 81 reward = 60.0\n",
      "DONE episode = 787 number of steps = 154 reward = 120.0\n",
      "DONE episode = 788 number of steps = 139 reward = 100.0\n",
      "DONE episode = 789 number of steps = 77 reward = 40.0\n",
      "DONE episode = 790 number of steps = 78 reward = 40.0\n",
      "DONE episode = 791 number of steps = 78 reward = 40.0\n",
      "DONE episode = 792 number of steps = 80 reward = 40.0\n",
      "DONE episode = 793 number of steps = 79 reward = 20.0\n",
      "DONE episode = 794 number of steps = 123 reward = 160.0\n",
      "DONE episode = 795 number of steps = 143 reward = 40.0\n",
      "DONE episode = 796 number of steps = 1856 reward = 160.0\n",
      "DONE episode = 797 number of steps = 95 reward = 0.0\n",
      "DONE episode = 798 number of steps = 95 reward = 60.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if let_training:\n",
    "    training()\n",
    "\n",
    "if let_play:\n",
    "    env.terminal_on_life_loss = False\n",
    "    play()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NtaGEQz9kh2h",
   "metadata": {
    "id": "NtaGEQz9kh2h"
   },
   "outputs": [],
   "source": [
    "#!rm -r ./sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9-PYk6Yj2-c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 820,
     "status": "ok",
     "timestamp": 1665074732809,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "b9-PYk6Yj2-c",
    "outputId": "e1565445-ca51-4d71-fee1-c565a81dda82"
   },
   "outputs": [],
   "source": [
    "#!zip -r /content/Phoenix_dueling.zip /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77KRHlFFhhNG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1665074736231,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "77KRHlFFhhNG",
    "outputId": "dba6b184-6bec-447d-abef-369969bbf0ad"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#files.download('Phoenix_dueling.zip')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
