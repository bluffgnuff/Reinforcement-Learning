{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "TDL4T4160P-8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21905,
     "status": "ok",
     "timestamp": 1665042566743,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "TDL4T4160P-8",
    "outputId": "f5f0c89a-3413-4a14-b0ac-91798038dadf"
   },
   "outputs": [],
   "source": [
    " #!pip install gym[atari,accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "Lab36Cm53hT3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1666077971727,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "Lab36Cm53hT3",
    "outputId": "685e3000-67fa-4bb3-e1ec-7a50773a17f0"
   },
   "outputs": [],
   "source": [
    "# !apt install xvfb\n",
    "# !pip install gym-notebook-wrapper\n",
    "# !pip show tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84dfbf-263e-4d9a-ad88-2eaf271e4421",
   "metadata": {
    "id": "ae84dfbf-263e-4d9a-ad88-2eaf271e4421"
   },
   "source": [
    "# Dueling Network Architecture Implementation\n",
    "The Duelling newtorwk is an artificial neural network architecture that has improved the state of the art in the DQN area used in combination with Dual DQN and Prioritized Expirience Replay. This approach splits the action value calculation using a combination of state value function and advantage function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc578666-0c50-4681-8b90-bc48ece391d6",
   "metadata": {
    "id": "fc578666-0c50-4681-8b90-bc48ece391d6"
   },
   "source": [
    "# Searching for available environments\n",
    "We want to test the performance of our architecture with the Atari game 'Phoenix'.\n",
    "Here we check wich kind of versions of this game are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "82d78d38-4889-44d7-a8cd-49d3c9059bad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1539,
     "status": "ok",
     "timestamp": 1665042568266,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "82d78d38-4889-44d7-a8cd-49d3c9059bad",
    "outputId": "9d44fd58-f58f-4b95-cfa9-f00f7d2a04e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALE/Phoenix-ram-v5\n",
      "ALE/Phoenix-v5\n",
      "Phoenix-ram-v0\n",
      "Phoenix-ram-v4\n",
      "Phoenix-ramDeterministic-v0\n",
      "Phoenix-ramDeterministic-v4\n",
      "Phoenix-ramNoFrameskip-v0\n",
      "Phoenix-ramNoFrameskip-v4\n",
      "Phoenix-v0\n",
      "Phoenix-v4\n",
      "PhoenixDeterministic-v0\n",
      "PhoenixDeterministic-v4\n",
      "PhoenixNoFrameskip-v0\n",
      "PhoenixNoFrameskip-v4\n"
     ]
    }
   ],
   "source": [
    "from gym import envs\n",
    "\n",
    "# Searching for available environments\n",
    "game_name = \"Phoenix\"\n",
    "all_envs = envs.registry.values()\n",
    "env_ids = [env_spec.id for env_spec in all_envs]\n",
    "\n",
    "for id in sorted(env_ids):\n",
    "    if game_name in id:\n",
    "        print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5b1aa-6e92-4ac3-9079-c49121fafaef",
   "metadata": {
    "id": "12c5b1aa-6e92-4ac3-9079-c49121fafaef"
   },
   "source": [
    "# Environment Configuration\n",
    "We select the version 4 of the enviroment with no frameskipping and select as render mode human. The no frameskipping is used to make this enviroment compatible with the optimization made by *AtariPreprocessing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3cecf335-3193-4ce8-90f5-dfe1c3a82e5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1665042568266,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "3cecf335-3193-4ce8-90f5-dfe1c3a82e5f",
    "outputId": "28ea8b12-3721-41e2-a222-156e9e23bd33"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import AtariPreprocessing\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# Make Parameters:\n",
    "game_name = \"Phoenix\"\n",
    "game_mode = \"NoFrameskip\"  # [Deterministic | NoFrameskip | ram | ramDeterministic | ramNoFrameskip ]\n",
    "game_version = \"v4\"  # [v0 | v4 | v5]\n",
    "env_name = '{}{}-{}'.format(game_name, game_mode, game_version)\n",
    "env_render_mode = 'human'  # [human | rgb_array]\n",
    "env_frame_skip = 4\n",
    "\n",
    "env = gym.make(env_name, render_mode=env_render_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "23b68bf0-d09a-432b-9fe9-9991f0d8a84a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1665042568267,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "23b68bf0-d09a-432b-9fe9-9991f0d8a84a",
    "outputId": "1921b05a-a9fa-47f2-9246-26adafe63578"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4a30296280>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env.reset() \n",
    "state = env.render(mode=\"rgb_array\")\n",
    "plt.imshow(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04326819-e929-40fe-9a26-ecfc317f7882",
   "metadata": {
    "id": "04326819-e929-40fe-9a26-ecfc317f7882"
   },
   "source": [
    "## Enviornment Observations\n",
    "Below we have a fist look to the environment characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "143cf671-a4e9-46a5-b06c-5f66f83fdc22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1665042568267,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "143cf671-a4e9-46a5-b06c-5f66f83fdc22",
    "outputId": "ffd6528b-5a7d-4fbf-d9e5-c4aad9150b7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "401a5ee2-bf0f-4721-be41-32c498caf4f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1665042568268,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "401a5ee2-bf0f-4721-be41-32c498caf4f3",
    "outputId": "ef5b5156-1509-47c2-98e8-2ffa96ea7495"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(8)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2c5513e8-9f39-47bd-b667-a7436383795e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1665042568268,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "2c5513e8-9f39-47bd-b667-a7436383795e",
    "outputId": "f6792f4c-9ef4-485e-ef92-4875a0e28914"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'DOWN', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad4449-5a13-4032-9645-53f78265617b",
   "metadata": {
    "id": "39ad4449-5a13-4032-9645-53f78265617b"
   },
   "source": [
    "## Environment Optimization\n",
    "We optimize the enviroment adding the frame skipping, changing its observation in greyscale and following the experiment did in the paper we set to at most 30 the no-op actions; to get this we use the AtariPreprocessing wrapper.\n",
    "we use Framestack to create observation of 4 frames to give the idea of moviment to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "63a89f89-355e-407a-a036-d1d14bfe3fba",
   "metadata": {
    "id": "63a89f89-355e-407a-a036-d1d14bfe3fba"
   },
   "outputs": [],
   "source": [
    "env = AtariPreprocessing(env, frame_skip=env_frame_skip, grayscale_obs=True, terminal_on_life_loss=False, noop_max=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "43876bf8-8f34-4118-990a-dba1bbbc67fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "executionInfo": {
     "elapsed": 894,
     "status": "ok",
     "timestamp": 1665042569152,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "43876bf8-8f34-4118-990a-dba1bbbc67fd",
    "outputId": "bead8004-41c7-4082-d536-f1d6bce913c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4a301f33a0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = FrameStack(env, 4)\n",
    "state = env.reset()\n",
    "plt.imshow(state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "51c6620c-52ee-48de-bc6b-4caba64b8f1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1665042569153,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "51c6620c-52ee-48de-bc6b-4caba64b8f1f",
    "outputId": "4c23526d-3fe5-4795-bba8-e84ad50f9222"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0735fbbc-11c3-418f-8026-f3eba9dc5cb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1665042569153,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "0735fbbc-11c3-418f-8026-f3eba9dc5cb9",
    "outputId": "ce6cfa59-1f25-4917-d5c2-b5aa4a49c4c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(8)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dccc025-b2ae-48c8-a70d-0e146d5c9d7c",
   "metadata": {
    "id": "9dccc025-b2ae-48c8-a70d-0e146d5c9d7c"
   },
   "source": [
    "# Network configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579e3d9-51f6-4612-bf77-25db75a98d4a",
   "metadata": {
    "id": "e579e3d9-51f6-4612-bf77-25db75a98d4a"
   },
   "source": [
    "## Policy\n",
    "The policy is the component that choose the action to perform; using an $\\epsilon$-gready policy the action chosen can be a random with probability $\\epsilon$ or an action suggested by the ANN with probability $1 - \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7c3213c2-3be1-4337-bf89-5a32f9dbf2c8",
   "metadata": {
    "id": "7c3213c2-3be1-4337-bf89-5a32f9dbf2c8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class EpsilonGreedyPolicy:\n",
    "\n",
    "    def __init__(self, model, action_space_size, episodes=1, min_epsilon=0):\n",
    "        self.model = model\n",
    "        self.action_space_size = action_space_size\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.episode = 1\n",
    "        self.episodes = episodes\n",
    "\n",
    "    def get_action(self, state):\n",
    "        epsilon = max(1 - self.episode / self.episodes, self.min_epsilon)\n",
    "        random = np.random.random()\n",
    "        # print(random, epsilon)\n",
    "        if random < epsilon:\n",
    "            action = np.random.randint(self.action_space_size)\n",
    "            return action\n",
    "        else:\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = self.model(state_tensor, training=False)\n",
    "            \n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "            return action\n",
    "\n",
    "    def next_episode(self):\n",
    "        self.episode += 1\n",
    "\n",
    "    def reset_episodes(self):\n",
    "        self.episode = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c4fef-7fd8-4e38-9eac-b7623174d588",
   "metadata": {
    "id": "255c4fef-7fd8-4e38-9eac-b7623174d588"
   },
   "source": [
    "## Replication Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f6dd9-3076-4c01-bf7f-01b9400c6fc2",
   "metadata": {
    "id": "6d9f6dd9-3076-4c01-bf7f-01b9400c6fc2"
   },
   "source": [
    "### Prioritized experience replay\n",
    "The Prioritized experience replay was introduced in the paper \"Prioritized experience replay\" (https://arxiv.org/abs/1511.05952), it consists in an evolution of the replay buffer ordering the experiences to replay by priority. In this experiment we adopt the **rank-based** variant where the experience sampling from the buffer it's done with probability $ P(i) = \\frac{p_{i}^{\\alpha}}{\\sum_k{p_{k}^{\\alpha}}} $ and $p(i)=\\frac{1}{rank(i)}$ where $rank(i)$ is the rank of the transition *i*, $\\alpha$ is called **priority exponent**. It is necessary to compute the importance sampling weights as $w_j = \\frac{(N * P(j))^{-\\beta}}{max_i{w_i}} $ and $w_i = (\\frac{1}{N} . \\frac{1}{P(i)})^\\beta$ to avoid overfitting fot the experiences with more priority.\n",
    "\n",
    "In both the paper the parameters are setted as follow: **priority exponent** $\\alpha= 0.7$,  the **importance sampling exponent** $\\beta = [0.5, 1]$.\n",
    "In the paper is proposed a **heap array** structure to implement the buffer. Due to the particular structure and the amount of property of the replay buffer in the Prioritized Experience Replay we choose to describe it as a class. The heap array structure is implemented as a list sorted every *T* step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9e8897eb-7fea-48e8-93d4-cfd169a6278a",
   "metadata": {
    "id": "9e8897eb-7fea-48e8-93d4-cfd169a6278a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# We set a time to haepify to sort the buffer every K time step.\n",
    "class PrioritizedExperienceReplayRankBased:\n",
    "    \"\"\"\n",
    "    contains the tuples (TD_error, experience)\n",
    "    replay_buffer --- it's the max size of the buffer, over which before add an experience one is remove\n",
    "    max_buffer_size --- time step before sort the structure\n",
    "    time_to_haepify --- the last time step\n",
    "    mod_curr_step = 0  --- the alpha parameter used to calculate the probability of the i-th element P(i) to be sampled\n",
    "    alpha -- alpha parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_buffer_size, step_to_heapify, alpha):\n",
    "        self.max_buffer_size = max_buffer_size\n",
    "        # (TD, experience)\n",
    "        # Probably list is not the most efficient structure to use np array ?\n",
    "        self.replay_buffer = []\n",
    "        self.alpha = alpha\n",
    "        self.heapify_threshold = step_to_heapify  # here we stock the threshold to sort the buffer\n",
    "        self.step_to_heapify = step_to_heapify  # number of next steps before heapify\n",
    "        self.max_td_error = 0\n",
    "\n",
    "    def set_alpha(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # Add experience in the buffer mapping it with its last TD_error\n",
    "    def add_experience(self, experience):\n",
    "        if len(self.replay_buffer) == self.max_buffer_size:\n",
    "            self.remove_experience()\n",
    "\n",
    "        # New experience where td_error is unknown are set with the max td error\n",
    "        # NB we are considering the max td error as the error of the experience in first position, but the buffer may\n",
    "        # not have been sorted yet\n",
    "        if len(self.replay_buffer) > 0:\n",
    "            self.max_td_error = self.replay_buffer[0][0]\n",
    "\n",
    "        self.replay_buffer.append((self.max_td_error, experience))\n",
    "        self.step_to_heapify -= 1\n",
    "        if self.step_to_heapify == 0:\n",
    "            self.replay_buffer.sort(key=lambda el: el[0], reverse=True)\n",
    "            self.step_to_heapify = self.heapify_threshold\n",
    "\n",
    "    # Remove experience from the buffer\n",
    "    def remove_experience(self):\n",
    "        self.replay_buffer.pop(-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def zip_f_sampling(alpha, n):\n",
    "        x = np.arange(1, n + 1)\n",
    "        weights = x ** (-alpha)\n",
    "        weights /= weights.sum()\n",
    "        zipf = stats.rv_discrete(values=(x, weights))\n",
    "        return zipf.rvs() - 1\n",
    "\n",
    "    # Get batch_size samples from the buffer; using the beta parameter to compute the importance sampling weight\n",
    "    # Beta value can change while training we can delegate its control outside\n",
    "    def sample_experience(self, batch_size, beta):\n",
    "        experiences = []\n",
    "        importance_sampling_weights = []\n",
    "        n = len(self.replay_buffer) - 1\n",
    "        indexes = []\n",
    "\n",
    "        for i in range(0, batch_size):\n",
    "            # Sample index and check the experience is not already present in the batch\n",
    "            index = self.zip_f_sampling(self.alpha, n)\n",
    "            while index in indexes:\n",
    "                index = self.zip_f_sampling(self.alpha, n)\n",
    "            indexes.append(index)\n",
    "            # importance sampling weights computation\n",
    "            rank = index + 1\n",
    "            pj = 1 / rank\n",
    "            importance_sampling_weights.append(((n * pj) ** (-beta)))\n",
    "            experiences.append(self.replay_buffer[index][1])\n",
    "\n",
    "        # Normalization step\n",
    "        max_weight = max(importance_sampling_weights)\n",
    "        importance_sampling_weights_normalized = np.divide(importance_sampling_weights, max_weight)\n",
    "        return indexes, experiences, importance_sampling_weights_normalized\n",
    "\n",
    "    def update_td_error(self, index, td_error):\n",
    "        self.replay_buffer[index] = [td_error, self.replay_buffer[index][1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb99df-922b-4ef1-9865-8d653c135eb8",
   "metadata": {
    "id": "5ddb99df-922b-4ef1-9865-8d653c135eb8"
   },
   "source": [
    "## Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e58cc1cd-e7dc-4f40-ba18-b5127abc5fff",
   "metadata": {
    "id": "e58cc1cd-e7dc-4f40-ba18-b5127abc5fff"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras as krs\n",
    "\n",
    "input_shape = env.observation_space.shape\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d096c-c9d8-4012-9400-25075d3779cc",
   "metadata": {
    "id": "257d096c-c9d8-4012-9400-25075d3779cc"
   },
   "source": [
    "## Neural Network Creation\n",
    "The network architecture proposed follows the structure used in *\"Dueling Network Architectures for Deep Reinforcement Learning\"* https://arxiv.org/abs/1511.06581 composed by 3 convolutional layers and 2 fully connected layer for each stream (advantage, value).\n",
    "It's possible to create a dueling network using the `DQNAgent` of `rl.agents.dqn` setting `enable_dueling_network=True` in the constructor, but the perpouse of this experiment is to show how to develop it manually so that is not used.\n",
    "\n",
    "The output of the value stream and the output of the advantage stream are merged to obtain the action-value function in the last module of the network using the following formula:\n",
    "$$ Q(s, a; \\theta, \\alpha, \\beta) = V (s;\\theta, \\beta) + ( A(s, a; \\theta, \\alpha) − max_{a' ∈|A|} A(s, a'; \\theta, \\alpha)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0a878b21-2040-4ca8-aeff-3563499bf9f9",
   "metadata": {
    "id": "0a878b21-2040-4ca8-aeff-3563499bf9f9"
   },
   "outputs": [],
   "source": [
    "from tensorflow import math\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "\n",
    "\n",
    "def create_dueling_model(input_shape, number_actions):\n",
    "    backend.set_image_data_format('channels_first')\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    value_stream_1 = layers.Dense(512)(layer4)\n",
    "    value_stream_2 = layers.Dense(1)(value_stream_1)  # scalar output size\n",
    "\n",
    "    advantage_stream_1 = layers.Dense(512)(layer4)\n",
    "    advantage_stream_2 = layers.Dense(number_actions)(advantage_stream_1)  # output size equal to the actions available\n",
    "\n",
    "    # Combination of the streams: a Q value for each state\n",
    "    q_values = value_stream_2 + math.subtract(advantage_stream_2, math.reduce_mean(advantage_stream_2, axis=1,\n",
    "                                                                                   keepdims=True))\n",
    "    # Alternative q_value\n",
    "    # q_value = value_stream_2 + (advantage_stream_2 - backend.max(advantage_stream_2, axis=1, keepdims=True))\n",
    "    return Model(inputs=[inputs], outputs=[q_values])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b2cfe-111c-4afe-a3c5-391066124fe3",
   "metadata": {
    "id": "ff1b2cfe-111c-4afe-a3c5-391066124fe3"
   },
   "source": [
    "# Agent \n",
    "Here we define a custom agent to perfome action in the enviroment using a DoubleDQN training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391a27a-28ac-4d3d-9c8f-b42bb303bd76",
   "metadata": {
    "id": "e391a27a-28ac-4d3d-9c8f-b42bb303bd76"
   },
   "source": [
    "## Play one step\n",
    "With this function we want to ask to the policy what action must be choosen and perform it on the eniroment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec3a88-954a-49f5-96b9-a6d34c29a16d",
   "metadata": {
    "id": "bdec3a88-954a-49f5-96b9-a6d34c29a16d"
   },
   "source": [
    "## Gradient \n",
    "In our scenario the gradient that is backpropageted to the last convolutional layer must be rescaled by $\\frac{1}{\\sqrt{2}}$. Furthermore we have to realize by hand the gradient clipping that is not realized by the optimizer since we are using a custoom loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bd489-2d61-4334-b692-51a88f5ecb29",
   "metadata": {
    "id": "c45bd489-2d61-4334-b692-51a88f5ecb29"
   },
   "source": [
    "## Double DQN Training\n",
    "Double DQN algorithm uses a second network, beyond the network used for the prediction. So in the training process the main network is used to choose an action and an other to evaluate it, this permit to mitigate the overfitting present in the classic DQN. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gg9foUcDcp1r",
   "metadata": {
    "id": "gg9foUcDcp1r"
   },
   "source": [
    "## DQN Agent code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6bad3e87-4bed-4e08-a34e-8a2e2fc06a83",
   "metadata": {
    "id": "6bad3e87-4bed-4e08-a34e-8a2e2fc06a83"
   },
   "outputs": [],
   "source": [
    "import math as mt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DuelDQNAgent:\n",
    "\n",
    "    ## We keep the creation model outside the agent to ensure a fine-grained control on it\n",
    "    def __init__(self, env, model, policy, model_target=None, optimizer=None, replay_buffer=None):\n",
    "        self.env = env\n",
    "        self.model_primary = model\n",
    "        self.model_target = model_target\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "    def set_policy(self, policy):\n",
    "        self.policy = policy\n",
    "\n",
    "    # Execs one action receiving in input the environment, its state, the current episode.\n",
    "    # If training its true add the experience in the replay buffer\n",
    "    def play_one_step(self, state):\n",
    "        action = self.policy.get_action(state)\n",
    "        # print(\"action {}\".format(action))\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        return action, reward, next_state, done, info\n",
    "\n",
    "    # Play\n",
    "    def play(self):\n",
    "        state = self.env.reset()\n",
    "        steps = 0\n",
    "        cumulative_reward = 0\n",
    "        while True:\n",
    "            action, reward, next_state, done, info = self.play_one_step(state)\n",
    "            cumulative_reward += reward\n",
    "            if done:\n",
    "                print(\"DONE number of steps: {} reward:  {}\".format(steps, cumulative_reward))\n",
    "                break\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "        return steps, cumulative_reward\n",
    "\n",
    "    ## Double DQN Training\n",
    "    @staticmethod\n",
    "    def gradient_clipping(gradients, clipping_value):\n",
    "        clipped_gradients = [(tf.clip_by_norm(grad, clipping_value)) for grad in gradients]\n",
    "        return clipped_gradients\n",
    "\n",
    "    def weighted_gradient(self, best_on_target_q_values, importance_sampling_weights, states, loss_function, mask,\n",
    "                          step_size=1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(importance_sampling_weights)\n",
    "            all_q_values = self.model_primary(states)\n",
    "            q_values = tf.reduce_sum(all_q_values * mask, axis=1, keepdims=True)\n",
    "            loss_value = loss_function(best_on_target_q_values, q_values)\n",
    "            loss_corrected = tf.multiply(loss_value, importance_sampling_weights, step_size)\n",
    "        grads = tape.gradient(loss_corrected, self.model_primary.trainable_variables)\n",
    "        return grads, loss_value\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_grad(gradients, rescale_value, index):\n",
    "        tensor_to_scale = gradients[index]\n",
    "        rescaled_tensor = tf.multiply(tensor_to_scale, rescale_value)\n",
    "        gradients[index] = rescaled_tensor\n",
    "        return gradients\n",
    "\n",
    "    # Collects samples of the previous experiences from the replay buffer\n",
    "    # and use them to improve the weights update of the Neural Network.\n",
    "    def double_dqn_training_step(self, batch_size, loss_function, discount_factor, clipping_value, beta, step_size=1):\n",
    "        indexes, experiences, importance_sampling_weights = self.replay_buffer.sample_experience(batch_size, beta)\n",
    "        states, actions, rewards, next_states, dones = [np.array([experience[field_index] for experience in experiences]\n",
    "                                                                 ) for field_index in range(5)]\n",
    "\n",
    "        action_space = self.env.action_space.n\n",
    "        # Predict using the primary network\n",
    "        next_q_values = self.model_primary.predict(next_states)\n",
    "        next_q_values_target = self.model_target.predict(next_states)\n",
    "        \n",
    "        # Select the action that lead us to the higher next Q value\n",
    "        best_actions = np.argmax(next_q_values, axis=1)\n",
    "        best_action_mask = tf.one_hot(best_actions, action_space)\n",
    "\n",
    "        next_q_value_target = tf.reduce_sum(next_q_values_target * best_action_mask, axis=1)\n",
    "        best_on_target_q_values = (rewards + (1-dones)*discount_factor*next_q_value_target)\n",
    "\n",
    "        mask = tf.one_hot(actions, action_space)\n",
    "        importance_sampling_weights = tf.convert_to_tensor(importance_sampling_weights, tf.float32)\n",
    "        weighted_gradient, loss_value = self.weighted_gradient(best_on_target_q_values, importance_sampling_weights,\n",
    "                                                               states, loss_function, mask, step_size)\n",
    "\n",
    "        for index, td_error in zip(indexes, loss_value):\n",
    "            self.replay_buffer.update_td_error(index, abs(td_error))\n",
    "\n",
    "        # We rescale the last convolutional layer to 1/sqrt(2) to balance the double backpropagation\n",
    "        rescale_value = (1 / mt.sqrt(2))\n",
    "        # The index of the last sequential layer\n",
    "        index_gradient_to_rescale = 4\n",
    "        rescaled_grads = self.rescale_grad(weighted_gradient, rescale_value, index_gradient_to_rescale)\n",
    "\n",
    "        # Since we are in a custom loop we have to clip the gradient by hand, we can't delegate it to the optimizer\n",
    "        clipped_gradients = self.gradient_clipping(rescaled_grads, clipping_value)\n",
    "        # Application gradient descent trough optimizer\n",
    "        self.optimizer.apply_gradients(zip(clipped_gradients, self.model_primary.trainable_variables))\n",
    "\n",
    "    # We use the training step just when there is enough samples on the replay buffer\n",
    "    def double_dqn_training(self, batch_size, loss_function, discount_factor, freq_replacement, training_freq,\n",
    "                            clipping_value, beta_min, beta_max, max_episodes=600):\n",
    "        rewards = []\n",
    "        steps = []\n",
    "        \n",
    "\n",
    "        for episode in range(1, max_episodes+1):\n",
    "            state = self.env.reset()\n",
    "            cumulative_reward = 0\n",
    "            step = 0\n",
    "            beta = max(beta_min, (beta_max * episode / max_episodes))\n",
    "\n",
    "            while True:\n",
    "                action, reward, next_state, done, info = self.play_one_step(state)\n",
    "                experience = [state, action, reward, next_state, done]\n",
    "                cumulative_reward += reward\n",
    "                self.replay_buffer.add_experience(experience)\n",
    "                if done:\n",
    "                    print(\n",
    "                        \"DONE episode = {} number of steps = {} reward = {}\".format(episode, step, cumulative_reward))\n",
    "                    rewards.append(cumulative_reward)\n",
    "                    steps.append(step)\n",
    "                    break\n",
    "                if len(self.replay_buffer.replay_buffer) > batch_size and (step % training_freq) == 0:\n",
    "                    self.double_dqn_training_step(batch_size, loss_function, discount_factor, clipping_value, beta)\n",
    "                if (step % freq_replacement) == 0:\n",
    "                    self.model_target.set_weights(self.model_primary.get_weights())\n",
    "                state = next_state\n",
    "                step = step + 1\n",
    "\n",
    "            self.policy.next_episode()\n",
    "            return steps, rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bETJE4sDcnvu",
   "metadata": {
    "id": "bETJE4sDcnvu"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9538fe86-67f4-4292-b80c-0225a088b271",
   "metadata": {
    "id": "9538fe86-67f4-4292-b80c-0225a088b271"
   },
   "source": [
    "## Result plots\n",
    "We use this function to generate the plot representing the rewards or the steps for each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6ef6ec3d-002c-4d30-9052-4e05db6839a0",
   "metadata": {
    "id": "6ef6ec3d-002c-4d30-9052-4e05db6839a0"
   },
   "outputs": [],
   "source": [
    "def plot_result(x_label, y_label, x, y, name):\n",
    "    plt.ylabel(x_label)\n",
    "    plt.xlabel(y_label)\n",
    "    plt.plot(x, y)\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364dba26-35cc-4d53-878a-6bf0496b5a24",
   "metadata": {
    "id": "364dba26-35cc-4d53-878a-6bf0496b5a24"
   },
   "source": [
    "# Training\n",
    "The learnig step is executed with **Double Deep Q-networks** algorithm presented in the paper *\"Deep reinforcement learning with double Q-learning\"*.https://arxiv.org/pdf/1509.06461.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e16f49-345d-460c-922d-f529d9291a07",
   "metadata": {
    "id": "23e16f49-345d-460c-922d-f529d9291a07"
   },
   "source": [
    "## Training parameters\n",
    "We adopt as optimizer the **Adam** implementation setting the learning rate equal to $6.25x10^{-5}$ and **clipping the gradient** norm at most to 10; the parameters are specified in the paper \"*Deep reinforcement learning with double Q-learning*\" (https://arxiv.org/pdf/1509.06461.pdf)\n",
    "To evaluate the loss score we use the `mean_squared_error`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4742cf2f-c7c6-403e-9da9-32d47e7717da",
   "metadata": {
    "id": "4742cf2f-c7c6-403e-9da9-32d47e7717da"
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Environment info\n",
    "input_shape = env.observation_space.shape\n",
    "actions_number = env.action_space.n\n",
    "\n",
    "# Model persistent file\n",
    "primary_model_file_name = \"{}_dueling_model\".format(game_name)\n",
    "\n",
    "# Training Parameters\n",
    "loss_function = losses.mean_squared_error\n",
    "batch_size = 32 # @param {type:\"integer\"}\n",
    "discount_factor = 0.95 # @param {type:\"number\"}\n",
    "learning_rate = 6.25e-5 # @param {type:\"number\"}\n",
    "episodes = 500 # @param {type:\"integer\"}\n",
    "clipping_value = 10 # @param {type:\"number\"}\n",
    "training_freq = 4 # @param {type:\"integer\"}\n",
    "\n",
    "# Dual DQN Training\n",
    "freq_replacement = 500 # @param {type:\"integer\"}\n",
    "\n",
    "# Replay buffer parameters\n",
    "buffer_size = 10000 # @param {type:\"integer\"}\n",
    "step_to_heapify = 200 # @param {type:\"integer\"}\n",
    "alpha = 0.5 # @param {type:\"number\"}\n",
    "beta_max = 1 # @param {type:\"number\"}\n",
    "beta_min = 0.5 # @param {type:\"number\"}\n",
    "\n",
    "# Policy parameters\n",
    "min_epsilon = 0.01 # @param {type:\"number\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf7e90-0940-497c-8dc3-cafa5366bc3f",
   "metadata": {
    "id": "5fbf7e90-0940-497c-8dc3-cafa5366bc3f"
   },
   "source": [
    "## Model creation / loading \n",
    "In this step we check wheter there is an already saved model and load it in that case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "44e76e0d-1130-47d8-b055-dbaa73907926",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3653,
     "status": "ok",
     "timestamp": 1665042575453,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "44e76e0d-1130-47d8-b055-dbaa73907926",
    "outputId": "49ca5eb3-67e4-44d7-a61a-02e830afe86d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found an existing model\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4, 84, 84)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 20, 20)   8224        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 9, 9)     32832       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 7, 7)     36928       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3136)         0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          1606144     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 8)            4104        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          1606144     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean (TFOpLambda (None, 1)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            513         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda)   (None, 8)            0           dense_3[0][0]                    \n",
      "                                                                 tf.math.reduce_mean[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 8)            0           dense_1[0][0]                    \n",
      "                                                                 tf.math.subtract[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 3,294,889\n",
      "Trainable params: 3,294,889\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Model creation\n",
    "file_primary = Path(primary_model_file_name)\n",
    "if file_primary.exists():\n",
    "    print(\"Found an existing model\")\n",
    "    model = load_model(primary_model_file_name)\n",
    "else:\n",
    "    print(\"Model not found, a new one will be crate\")\n",
    "    model = create_dueling_model(input_shape, actions_number)\n",
    "\n",
    "# Print a summary about the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8325f-d204-47a9-84d0-e0532a453eb5",
   "metadata": {
    "id": "3cf8325f-d204-47a9-84d0-e0532a453eb5"
   },
   "source": [
    "## Training\n",
    "Here we ran the training operation. After a training session we save two plot episode - rewards, episode - steps. Also we save a csv with two column: steps and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ef554953-be5f-4c07-9537-7f61a548629a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 32149848,
     "status": "error",
     "timestamp": 1665074725297,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "ef554953-be5f-4c07-9537-7f61a548629a",
    "outputId": "d26f332c-d658-4041-b41b-06a1d68fdd00"
   },
   "outputs": [],
   "source": [
    "def training():\n",
    "    try:\n",
    "        model_target = create_dueling_model(input_shape, actions_number)\n",
    "        model_target.set_weights(model.get_weights())\n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "        policy_training = EpsilonGreedyPolicy(model, actions_number, episodes=episodes, min_epsilon=min_epsilon)\n",
    "        replay_buffer = PrioritizedExperienceReplayRankBased(buffer_size, step_to_heapify, alpha)\n",
    "        agent = DuelDQNAgent(env, model, policy_training, model_target, optimizer, replay_buffer)\n",
    "        steps, rewards = agent.double_dqn_training(batch_size, loss_function, discount_factor, freq_replacement,\n",
    "                                                   training_freq, clipping_value, beta_min, beta_max, episodes)\n",
    "\n",
    "        ext = \"png\"\n",
    "        name_plot_eps_steps = \"{} Training Episodes Steps.{}\".format(game_name, ext)\n",
    "        name_plot_eps_rewards = \"{} Training Episodes Rewards.{}\".format(game_name, ext)\n",
    "        file_plot_1 = Path(name_plot_eps_steps)\n",
    "        i = 1\n",
    "        while file_plot_1.exists():\n",
    "            i += 1\n",
    "            name_plot_eps_steps = \"{} Training Episodes Steps_{}.{}\".format(game_name, i, ext)\n",
    "            name_plot_eps_rewards = \"{} Training Episodes Rewards_{}.{}\".format(game_name, i, ext)\n",
    "            file_plot_1 = Path(name_plot_eps_steps)\n",
    "\n",
    "        plot_result(\"Episode\", \"Steps\", range(1, episodes + 1), steps, name_plot_eps_steps)\n",
    "        plot_result(\"Episode\", \"Rewards\", range(1, episodes + 1), rewards, name_plot_eps_rewards)\n",
    "\n",
    "    finally:\n",
    "        model.save(primary_model_file_name)\n",
    "\n",
    "        csv_name = \"{}.csv\".format(game_name)\n",
    "        if steps is not None and rewards is not None:\n",
    "            dict = {'steps': steps, 'rewards': rewards}\n",
    "            df = pd.DataFrame(dict)\n",
    "            df.to_csv(csv_name, mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a05b7-819e-4b6d-ad07-bce40bd6fe01",
   "metadata": {},
   "source": [
    "# Play\n",
    "Here we play a game (one episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "96580b5d-b9c7-418c-bdbe-a780b87ce2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play():\n",
    "    policy_play = EpsilonGreedyPolicy(model, actions_number, min_epsilon=min_epsilon)\n",
    "    agent = DuelDQNAgent(env, model, policy_play)\n",
    "    steps, reward = agent.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0525ad1-afdc-4a71-9bba-0041a3abad5d",
   "metadata": {
    "id": "b0525ad1-afdc-4a71-9bba-0041a3abad5d"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "vi51G7gJfU0N",
   "metadata": {
    "id": "vi51G7gJfU0N"
   },
   "outputs": [],
   "source": [
    "let_training = True # @param {type:\"boolean\"}\n",
    "let_play = False # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "28b03c22-dc33-4db8-b3e1-a4408d017e01",
   "metadata": {
    "id": "28b03c22-dc33-4db8-b3e1-a4408d017e01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 10:32:06.500867: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-10-19 10:32:06.520044: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200660000 Hz\n",
      "2022-10-19 10:32:34.184146: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Phoenix_dueling_model/assets\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'steps' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3086/1130937364.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDuelDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         steps, rewards = agent.double_dqn_training(batch_size, loss_function, discount_factor, freq_replacement,\n\u001b[0m\u001b[1;32m     10\u001b[0m                                                    training_freq, clipping_value, beta_min, beta_max, episodes)\n",
      "\u001b[0;32m/tmp/ipykernel_3086/4183698906.py\u001b[0m in \u001b[0;36mdouble_dqn_training\u001b[0;34m(self, batch_size, loss_function, discount_factor, freq_replacement, training_freq, clipping_value, beta_min, beta_max, max_episodes)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtraining_freq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble_dqn_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipping_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfreq_replacement\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3086/4183698906.py\u001b[0m in \u001b[0;36mdouble_dqn_training_step\u001b[0;34m(self, batch_size, loss_function, discount_factor, clipping_value, beta, step_size)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_error\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_td_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mabs\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m    396\u001b[0m   \"\"\"\n\u001b[0;32m--> 397\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Abs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname_scope\u001b[0;34m(name, default_name, values, skip_on_eager)\u001b[0m\n\u001b[1;32m   6426\u001b[0m   \"\"\"\n\u001b[0;32m-> 6427\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6428\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minternal_name_scope_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecuting_eagerly\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1932\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecuting_eagerly\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;34m\"\"\"Returns True if current thread has eager executing enabled.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3086/1098984536.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlet_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlet_play\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminal_on_life_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3086/1130937364.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mcsv_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}.csv\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rewards'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'steps' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "if let_training:\n",
    "    training()\n",
    "\n",
    "if let_play:\n",
    "    env.terminal_on_life_loss = True\n",
    "    play()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NtaGEQz9kh2h",
   "metadata": {
    "id": "NtaGEQz9kh2h"
   },
   "outputs": [],
   "source": [
    "#!rm -r ./sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9-PYk6Yj2-c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 820,
     "status": "ok",
     "timestamp": 1665074732809,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "b9-PYk6Yj2-c",
    "outputId": "e1565445-ca51-4d71-fee1-c565a81dda82"
   },
   "outputs": [],
   "source": [
    "#!zip -r /content/Phoenix_dueling.zip /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77KRHlFFhhNG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1665074736231,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "77KRHlFFhhNG",
    "outputId": "dba6b184-6bec-447d-abef-369969bbf0ad"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#files.download('Phoenix_dueling.zip')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
