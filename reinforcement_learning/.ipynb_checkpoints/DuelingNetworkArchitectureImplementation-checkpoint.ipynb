{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae84dfbf-263e-4d9a-ad88-2eaf271e4421",
   "metadata": {},
   "source": [
    "# Dueling Network Architecture Implementation\n",
    "The Duelling newtork is an artificial neural network architecture that has improved the state of the art in the DQN area used in combination with Dual DQN and Prioritized Expirience Replay. This approach splits the action value calculation using a combination of state value function and advantage function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc578666-0c50-4681-8b90-bc48ece391d6",
   "metadata": {},
   "source": [
    "# Searching for available environments\n",
    "We want to test the performance of our architecture with the Atari game 'BeamRider', we can check wich kind of version of this game are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d78d38-4889-44d7-a8cd-49d3c9059bad",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m envs\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Searching for available environments\u001b[39;00m\n\u001b[1;32m      4\u001b[0m game_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhoenix\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "from gym import envs\n",
    "\n",
    "# Searching for available environments\n",
    "game_name = \"Phoenix\"\n",
    "all_envs = envs.registry.values()\n",
    "env_ids = [env_spec.id for env_spec in all_envs]\n",
    "\n",
    "for id in sorted(env_ids):\n",
    "    if game_name in id:\n",
    "        print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5b1aa-6e92-4ac3-9079-c49121fafaef",
   "metadata": {},
   "source": [
    "# Environment Configuration\n",
    "We select the version 4 of the enviroment with no frameskipping and select as render mode rgb array. The no frameskipping is used to make this enviroment compatible with the optimization made by *AtariPreprocessing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cecf335-3193-4ce8-90f5-dfe1c3a82e5f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ste/anaconda3/lib/python3.9/site-packages/requests/packages/urllib3/_collections.py:2: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Mapping, MutableMapping\n",
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "# from gym import envs\n",
    "from gym.wrappers import AtariPreprocessing\n",
    "\n",
    "# Make Parameters:\n",
    "game_name = \"Phoenix\"\n",
    "game_mode = \"NoFrameskip\"  # [Deterministic | NoFrameskip | ram | ramDeterministic | ramNoFrameskip ]\n",
    "game_version = \"v4\"  # [v0 | v4 | v5]\n",
    "env_name = '{}{}-{}'.format(game_name, game_mode, game_version)\n",
    "env_render_mode = 'rgb_array'  # [human | rgb_array]\n",
    "env_frame_skip = 4\n",
    "\n",
    "env = envs.make(env_name, render_mode=env_render_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b68bf0-d09a-432b-9fe9-9991f0d8a84a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/core.py:51: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:297: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset() \n",
    "env.render(mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04326819-e929-40fe-9a26-ecfc317f7882",
   "metadata": {},
   "source": [
    "## Envorment Observations\n",
    "Below we have a fist look to the enviroment characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "143cf671-a4e9-46a5-b06c-5f66f83fdc22",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "401a5ee2-bf0f-4721-be41-32c498caf4f3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c5513e8-9f39-47bd-b667-a7436383795e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP',\n",
       " 'FIRE',\n",
       " 'UP',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'UPRIGHT',\n",
       " 'UPLEFT',\n",
       " 'RIGHTFIRE',\n",
       " 'LEFTFIRE']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78da9482-4415-4d63-83f0-e27cc579673c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc245f59160>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.render(mode=env_render_mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad4449-5a13-4032-9645-53f78265617b",
   "metadata": {},
   "source": [
    "## Enviroment Optimization\n",
    "We optimize the enviroment adding the frame skipping, changing its observation in greyscale and following the experiment did in the paper we set to at most 30 the no-op actions; to get this we use the AtariPreprocessing wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63a89f89-355e-407a-a036-d1d14bfe3fba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "env_prep = AtariPreprocessing(env, frame_skip=env_frame_skip, grayscale_obs=True, noop_max=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43876bf8-8f34-4118-990a-dba1bbbc67fd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_prep.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c6620c-52ee-48de-bc6b-4caba64b8f1f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_prep.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0735fbbc-11c3-418f-8026-f3eba9dc5cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_prep.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dccc025-b2ae-48c8-a70d-0e146d5c9d7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Network configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb99df-922b-4ef1-9865-8d653c135eb8",
   "metadata": {},
   "source": [
    "## Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e58cc1cd-e7dc-4f40-ba18-b5127abc5fff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 17:44:33.257279: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-14 17:44:33.865599: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-14 17:44:34.773513: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ste/anaconda3/lib/python3.9/site-packages/cv2/../../lib64::/home/ste/.mujoco/mujoco210/bin:/usr/lib/nvidia:/home/ste/anaconda3/lib/\n",
      "2022-09-14 17:44:34.773673: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ste/anaconda3/lib/python3.9/site-packages/cv2/../../lib64::/home/ste/.mujoco/mujoco210/bin:/usr/lib/nvidia:/home/ste/anaconda3/lib/\n",
      "2022-09-14 17:44:34.773681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/h5py/__init__.py:46: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import losses \n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "input_shape = env_prep.observation_space.shape\n",
    "num_actions = env_prep.action_space.n\n",
    "min_epsilon = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579e3d9-51f6-4612-bf77-25db75a98d4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Policy\n",
    "The policy is the component that choose the action to perform; using an $\\epsilon$-gready policy the action choosen can be a radom one with probability $\\epsilon$ and an action suggested by the ANN with probability $1 - \\epsilon$.\n",
    "From paper: \" When acting, it suffices to evaluate the advantage stream to make decisions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c3213c2-3be1-4337-bf89-5a32f9dbf2c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        pass\n",
    "\n",
    "    def get_action(self, state):\n",
    "        pass\n",
    "\n",
    "\n",
    "class EpsilonGreedyPolicy(Policy):\n",
    "\n",
    "    def __init__(self, model, action_space_size, episodes=1, min_epsilon=0):\n",
    "        super().__init__(model)\n",
    "        self.action_space_size = action_space_size\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.episode = 1\n",
    "        self.episodes = episodes\n",
    "\n",
    "    def get_action(self, state):\n",
    "        epsilon = max(1 - self.episode / self.episodes, self.min_epsilon)\n",
    "        random = np.random.random()\n",
    "        # print(random, epsilon)\n",
    "        if random < epsilon:\n",
    "            action = np.random.randint(self.action_space_size)\n",
    "            return action\n",
    "        else:\n",
    "            state = state[np.newaxis]\n",
    "            q_values = self.model(state)\n",
    "            # q_values = self.model(state[np.newaxis])\n",
    "            action = np.argmax(q_values[0])\n",
    "            return action\n",
    "\n",
    "    def next_episode(self):\n",
    "        self.episode += 1\n",
    "\n",
    "    def reset_episodes(self):\n",
    "        self.episode = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d096c-c9d8-4012-9400-25075d3779cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural Network Creation\n",
    "The creation of the DQN Model is based on the example presented in https://keras.io/examples/rl/deep_q_network_breakout/. The network architecture proposed follows the structure proposed in *\"Dueling Network Architectures for Deep Reinforcement Learning\"* https://arxiv.org/abs/1511.06581 composed by 3 convolutional layers and 2 fully connected layer for each stream (advantage, value).\n",
    "It's possible to create a dueling network using the `DQNAgent` of `rl.agents.dqn` setting `enable_dueling_network=True` in the constructor, but the perpouse of this experiment is to show how to develop it manually so that is not used.\n",
    "\n",
    "The output of the value stream and the output of the advantage stream are merged to obtain the action-value function in the last module of the network using the following formula:\n",
    "$$ Q(s, a; \\theta, \\alpha, \\beta) = V (s;\\theta, \\beta) + ( A(s, a; \\theta, \\alpha) − max_{a' ∈|A|} A(s, a'; \\theta, \\alpha)) $$\n",
    "\n",
    "Following the paper: \n",
    "\"Since both the advantage and the value stream propagate gradients to the last convolutional layer in the backward pass, we rescale the combined gradient entering the last convolutional layer by $\\frac{1} {\\sqrt2}$.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a878b21-2040-4ca8-aeff-3563499bf9f9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import math\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "\n",
    "def create_dueling_model(input_shape, number_actions):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv1D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv1D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv1D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    value_stream_1 = layers.Dense(512)(layer4)\n",
    "    value_stream_2 = layers.Dense(1)(value_stream_1)  # scalar output size\n",
    "\n",
    "    advantage_stream_1 = layers.Dense(512)(layer4)\n",
    "    advantage_stream_2 = layers.Dense(number_actions)(advantage_stream_1)  # output size equal to the actions available\n",
    "\n",
    "    # Combination of the streams: a Q value for each state\n",
    "    q_values = value_stream_2 + math.subtract(advantage_stream_2, math.reduce_mean(advantage_stream_2, axis=1,\n",
    "                                                                                   keepdims=True))\n",
    "    # Alternative q_value\n",
    "    # q_value = value_stream_2 + (advantage_stream_2 - backend.max(advantage_stream_2, axis=1, keepdims=True))\n",
    "    return Model(inputs=[inputs], outputs=[q_values])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c4fef-7fd8-4e38-9eac-b7623174d588",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Replication Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f6dd9-3076-4c01-bf7f-01b9400c6fc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prioritized experience replay\n",
    "The Prioritized experience replay was introduced in the paper \"Prioritized experience replay\" (https://arxiv.org/abs/1511.05952), it consists in an evolution in the replay buffer usage, ordering by priority the experience to replay. In this experiment we adopt the **rank-based** variant where the experience sampling from the buffer it's done with probability $ P(i) = \\frac{p_{i}^{\\alpha}}{\\sum_k{p_{k}^{\\alpha}}} $ and $p(i)=\\frac{1}{rank(i)}$ where $rank(i)$ is the rank of the -ith transition when the buffer is sorted by the TD error $\\delta$ of each experience; $\\alpha$ is called **priority exponent**. Is also compute the importance sampling weights as $w_j = \\frac{(N . P(i))^{-\\beta}}{max_i{w_i}} $ and $w_i = (\\frac{1}{N} . \\frac{1}{P(i)})^\\beta$\n",
    "\n",
    "In both the paper the parameters are setted as follow: **priority exponent** $\\alpha= 0.7$,  the **importance sampling exponent** $\\beta = [0.5, 1]$.\n",
    "In the paper is proposed a **heap array** structure to implement the buffer. Due to the particular structure and the amount of property of the replay buffer in the Prioritized Experience Replay, we choose to describe it as a class. The heap array structure is implemented as a deque sorted every *T* step.\n",
    "\n",
    "In the Prioritized experience replay paper another kind of implementation in propoused that consistes in the divition of the buffer in a *batchsize* parts and sampling from them; this garantees to have a sample for each rank category. We follow the first implementation using the numpy *zipf* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d888860-122e-464c-aa91-525929d88e3b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2186788287.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_9215/2186788287.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    def remove_experience():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_buffer_size):\n",
    "        self.max_buffer_size = max_buffer_size\n",
    "        pass\n",
    "\n",
    "    def sample_experience(self, batch_size):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e8897eb-7fea-48e8-93d4-cfd169a6278a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3768136602.py, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_9215/3768136602.py\"\u001b[0;36m, line \u001b[0;32m52\u001b[0m\n\u001b[0;31m    importance_sampling_weight =/ max_weight\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# import heapq as heap\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# We set a time to haepify to sort the buffer every K time step.\n",
    "class PrioritizedExperienceReplayRankBased(ReplayBuffer):\n",
    "    \"\"\"\n",
    "    contains the tuples (TD_error, experience)\n",
    "    replay_buffer --- it's the max size of the buffer, over which before add an experience one is remove\n",
    "    max_buffer_size --- time step before sort the structure\n",
    "    time_to_haepify --- the last time step\n",
    "    mod_curr_step = 0  --- the alpha parameter used to calculate the probability of the i-th element P(i) to be sampled\n",
    "    alpha -- alpha parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_buffer_size, step_to_heapify, alpha):\n",
    "        super().__init__(max_buffer_size=max_buffer_size)\n",
    "        # (TD, experience)\n",
    "        # Probably list is not the most efficient structure to use np array ?\n",
    "        self.replay_buffer = []\n",
    "        self.alpha = alpha\n",
    "        self.heapify_threshold = step_to_heapify  # here we stock the threshold to sort the buffer\n",
    "        self.step_to_heapify = step_to_heapify  # number of next steps before heapify\n",
    "        self.max_td_error = 0\n",
    "\n",
    "    def set_alpha(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # Add experience in the buffer mapping it with its last TD_error\n",
    "    def add_experience(self, experience):\n",
    "        if len(self.replay_buffer) == self.max_buffer_size:\n",
    "            self.remove_experience()\n",
    "\n",
    "        # New experience where td_error is unknown are set with the max td error\n",
    "        # NB we are considering the max td error as the error of the experience in first position, but the buffer may\n",
    "        # not have been sorted yet\n",
    "        if len(self.replay_buffer) > 0:\n",
    "            self.max_td_error = self.replay_buffer[0][0]\n",
    "\n",
    "        self.replay_buffer.append((self.max_td_error, experience))\n",
    "        self.step_to_heapify -= 1\n",
    "        if self.step_to_heapify == 0:\n",
    "            self.replay_buffer.sort(key=lambda el: el[0], reverse=True)\n",
    "            self.step_to_heapify = self.heapify_threshold\n",
    "\n",
    "    # Remove experience from the buffer\n",
    "    def remove_experience(self):\n",
    "        self.replay_buffer.pop(-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def zip_f_sampling(alpha, n):\n",
    "        x = np.arange(1, n + 1)\n",
    "        weights = x ** (-alpha)\n",
    "        weights /= weights.sum()\n",
    "        zipf = stats.rv_discrete(values=(x, weights))\n",
    "        return zipf.rvs() - 1\n",
    "\n",
    "    # Get batch_size samples from the buffer; using the beta parameter to compute the importance sampling weight\n",
    "    # Beta value can change while training we can delegate its control outside\n",
    "    def sample_experience(self, batch_size, beta):\n",
    "        experiences = []\n",
    "        importance_sampling_weights = []\n",
    "        n = len(self.replay_buffer) - 1\n",
    "        indexes = []\n",
    "\n",
    "        for i in range(0, batch_size):\n",
    "            # Sample index and check the experience is not already present in the batch\n",
    "            index = self.zip_f_sampling(self.alpha, n)\n",
    "            while index in indexes:\n",
    "                index = self.zip_f_sampling(self.alpha, n)\n",
    "            indexes.append(index)\n",
    "            # importance sampling weights computation\n",
    "            rank = index + 1\n",
    "            pj = 1 / rank\n",
    "            importance_sampling_weights.append(((n * pj) ** (-beta)))\n",
    "            experiences.append(self.replay_buffer[index][1])\n",
    "\n",
    "        # Normalization step\n",
    "        max_weight = max(importance_sampling_weights)\n",
    "        importance_sampling_weights_normalized = np.divide(importance_sampling_weights, max_weight)\n",
    "        return indexes, experiences, importance_sampling_weights_normalized\n",
    "\n",
    "    def update_td_error(self, index, td_error):\n",
    "        self.replay_buffer[index] = [td_error, self.replay_buffer[index][1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c5600c-4ed7-45d3-a6dc-49c46984d245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff1b2cfe-111c-4afe-a3c5-391066124fe3",
   "metadata": {},
   "source": [
    "# Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad3e87-4bed-4e08-a34e-8a2e2fc06a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DuelDQNAgent:\n",
    "\n",
    "    ## We keep the creation model outside the agent to ensure a fine-grained control on it\n",
    "    def __init__(self, env, model, model_target, policy, optimizer, replay_buffer):\n",
    "        self.env = env\n",
    "        self.model_primary = model\n",
    "        self.model_target = model_target\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "    def set_policy(self, policy):\n",
    "        self.policy = policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391a27a-28ac-4d3d-9c8f-b42bb303bd76",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Play one step\n",
    "With this function we want to ask to the policy what action must be choosen and perform it on the evniroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1853e99-1541-43b8-bb23-0a3d5c8f6521",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Execs one action receiving in input the environment, its state, the current episode.\n",
    "    # If training its true add the experience in the replay buffer\n",
    "    def play_one_step(self, state):\n",
    "        action = self.policy.get_action(state)\n",
    "        # print(\"action {}\".format(action))\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        return action, reward, next_state, done, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec3a88-954a-49f5-96b9-a6d34c29a16d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gradient \n",
    "The gradient is the mathematical function that calcualte the weights update in the ANN following the gradient minimization tecnique. In our scenario the gradient that is backpropageted to the last convolutional layer must be rescaled by $\\frac{1}{\\sqrt{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f0b41cc-a05c-4aed-bdde-76aed7d0621c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "    def weighted_gradient(self, best_on_target_q_values, importance_sampling_weights, states, loss_function, mask,\n",
    "                          step_size=1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(importance_sampling_weights)\n",
    "            all_q_values = self.model_primary(states)\n",
    "            q_values = tf.reduce_sum(all_q_values * mask, axis=1, keepdims=True)\n",
    "            loss_value = loss_function(best_on_target_q_values, q_values)\n",
    "            loss_corrected = tf.multiply(loss_value, importance_sampling_weights, step_size)\n",
    "        grads = tape.gradient(loss_corrected, self.model_primary.trainable_variables)\n",
    "        return grads, loss_value\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_grad(gradients, rescale_value, index):\n",
    "        tensor_to_scale = gradients[index]\n",
    "        rescaled_tensor = tf.multiply(tensor_to_scale, rescale_value)\n",
    "        gradients[index] = rescaled_tensor\n",
    "        return gradients\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_clipping(gradients, clipping_value):\n",
    "        clipped_gradients = [(tf.clip_by_norm(grad, clipping_value)) for grad in gradients]\n",
    "        return clipped_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bd489-2d61-4334-b692-51a88f5ecb29",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Double DQN Training\n",
    "This training algorithm uses a second network, beyond the network used for the prediction. So in the training process one network is used to evalute an action and an other to evaluate it, this permit to mitigate the overfitting present in the classic DQN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f95beea4-14d8-40d5-932b-7db584c768e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Collects samples of the previous experiences from the replay buffer\n",
    "    # and use them to improve the weights update of the Neural Network.\n",
    "    def double_dqn_training_step(self, batch_size, loss_function, discount_factor, clipping_value, beta, step_size=1):\n",
    "        indexes, experiences, importance_sampling_weights = self.replay_buffer.sample_experience(batch_size, beta)\n",
    "        states, actions, rewards, next_states, dones = [np.array([experience[field_index] for experience in experiences]\n",
    "                                                                 ) for field_index in range(5)]\n",
    "\n",
    "        action_space = self.env.action_space.n\n",
    "        # Predict using the primary network\n",
    "        next_q_values = self.model_primary.predict(next_states)\n",
    "        next_q_values_target = self.model_target.predict(next_states)\n",
    "\n",
    "        # Select the action that lead us to the higher next Q value\n",
    "        best_actions = np.argmax(next_q_values, axis=1)\n",
    "        best_action_mask = tf.one_hot(best_actions, action_space)\n",
    "\n",
    "        next_q_value_target = tf.reduce_sum(next_q_values_target * best_action_mask, axis=1)\n",
    "        best_on_target_q_values = (rewards + (1-dones)*discount_factor*next_q_value_target)\n",
    "\n",
    "        mask = tf.one_hot(actions, action_space)\n",
    "        importance_sampling_weights = tf.convert_to_tensor(importance_sampling_weights, tf.float32)\n",
    "        weighted_gradient, loss_value = self.weighted_gradient(best_on_target_q_values, importance_sampling_weights,\n",
    "                                                               states, loss_function, mask, step_size)\n",
    "\n",
    "        for index, td_error in zip(indexes, loss_value):\n",
    "            self.replay_buffer.update_td_error(index, td_error)\n",
    "\n",
    "        # We rescale the last convolutional layer to 1/sqrt(2) to balance the double backpropagation\n",
    "        rescale_value = (1 / math.sqrt(2))\n",
    "        # The index of the last sequential layer\n",
    "        index_gradient_to_rescale = 4\n",
    "        rescaled_grads = self.rescale_grad(weighted_gradient, rescale_value, index_gradient_to_rescale)\n",
    "\n",
    "        # Since we are in a custom loop we have to clip the gradient by hand, we can't delegate it to the optimizer\n",
    "        clipped_gradients = self.gradient_clipping(rescaled_grads, clipping_value)\n",
    "        # Application gradient descent trough optimizer\n",
    "        self.optimizer.apply_gradients(zip(clipped_gradients, self.model_primary.trainable_variables))\n",
    "\n",
    "    # We use the training step just when there is enough samples on the replay buffer\n",
    "    def double_dqn_training(self, batch_size, loss_function, discount_factor, freq_replacement, clipping_value,\n",
    "                            beta_min, beta_max, max_episodes=600):\n",
    "        rewards = []\n",
    "        steps = []\n",
    "\n",
    "        for episode in range(1, max_episodes+1):\n",
    "            state = self.env.reset()\n",
    "            cumulative_reward = 0\n",
    "            step = 0\n",
    "            beta = max(beta_min, (beta_max * episode / max_episodes))\n",
    "\n",
    "            while True:\n",
    "                action, reward, next_state, done, info = self.play_one_step(state)\n",
    "                experience = [state, action, reward, next_state, done]\n",
    "                cumulative_reward += reward\n",
    "                self.replay_buffer.add_experience(experience)\n",
    "                if done:\n",
    "                    print(\n",
    "                        \"DONE episode = {} number of steps = {} reward = {}\".format(episode, step, cumulative_reward))\n",
    "                    rewards.append(cumulative_reward)\n",
    "                    steps.append(step)\n",
    "                    break\n",
    "                if len(self.replay_buffer.replay_buffer) > batch_size:\n",
    "                    self.double_dqn_training_step(batch_size, loss_function, discount_factor, clipping_value, beta)\n",
    "                if step == freq_replacement:\n",
    "                    self.model_target.set_weights(self.model_primary.get_weights())\n",
    "                state = next_state\n",
    "                step = step + 1\n",
    "\n",
    "            self.policy.next_episode()\n",
    "\n",
    "        return steps, rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b012bf50-d20e-4791-beda-7e05d9f9f576",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aadf752-7a90-40ef-a46d-8ab502a5b135",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "    def play(env, model, min_epsilon, action_space_size):   \n",
    "        while True:\n",
    "            env_prep.render(mode=env_render_mode)\n",
    "            epsilon = 0\n",
    "            obs, reward, done, info = play_one_step(env, obs, None, min_epsilon, training = False)\n",
    "\n",
    "            if done:\n",
    "                print(\"DONE episode = {} number of steps = {} reward =  {}\".format( episode, step, cumulative_reward))\n",
    "                rewards.append(reward)\n",
    "                break\n",
    "\n",
    "        print('last episode = {} reward = {}'.format(episode, reward))\n",
    "\n",
    "        return episode, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364dba26-35cc-4d53-878a-6bf0496b5a24",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training\n",
    "The learnig step is executed with **Double Deep Q-networks** algorithm presented in the paper *\"Deep reinforcement learning with double Q-learning\"*.https://arxiv.org/pdf/1509.06461.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e16f49-345d-460c-922d-f529d9291a07",
   "metadata": {},
   "source": [
    "## Training parameters\n",
    "We adopt as optimizer the **Adam** implementation setting the learning rate equal to $6.25x10^{-5}$ and **clipping the gradient** norm at most to 10; the parameters are specified in the paper \"*Deep reinforcement learning with double Q-learning*\" (https://arxiv.org/pdf/1509.06461.pdf)\n",
    "To evaluate the loss score we use the `mean_squared_error`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4742cf2f-c7c6-403e-9da9-32d47e7717da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      2\u001b[0m buffer_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m\n\u001b[0;32m----> 3\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m \u001b[43mlosses\u001b[49m\u001b[38;5;241m.\u001b[39mmean_squared_error\n\u001b[1;32m      5\u001b[0m discount_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m\n\u001b[1;32m      6\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6.25e-5\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "# Network Parameters\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Environment info\n",
    "input_shape = env_prep.observation_space.shape\n",
    "actions_number = env_prep.action_space.n\n",
    "\n",
    "# Model persistent file\n",
    "primary_model_file_name = \"{}_dueling_model\".format(game_name)\n",
    "\n",
    "# Training Parameters\n",
    "loss_function = losses.mean_squared_error\n",
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "learning_rate = 6.25e-5\n",
    "episodes = 50\n",
    "clipping_value = 10\n",
    "\n",
    "# Dual DQN Training\n",
    "freq_replacement = 50\n",
    "\n",
    "# Replay buffer parameters\n",
    "buffer_size = 2000\n",
    "step_to_heapify = 50\n",
    "alpha = 0.5\n",
    "beta_max = 1\n",
    "beta_min = 0.5\n",
    "\n",
    "# Policy parameters\n",
    "min_epsilon = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538fe86-67f4-4292-b80c-0225a088b271",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Result plots\n",
    "We use this function to generate the plot representing the rewards or the steps for each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ef6ec3d-002c-4d30-9052-4e05db6839a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_result(max_episodes,returns):\n",
    "    plt.ylabel('Reward')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.plot(max_episodes, returns)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf7e90-0940-497c-8dc3-cafa5366bc3f",
   "metadata": {},
   "source": [
    "## Model creation or loading \n",
    "In this step we check wheter there is an already saved model and loading it in that case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e76e0d-1130-47d8-b055-dbaa73907926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Model creation\n",
    "file_primary = Path(primary_model_file_name)\n",
    "if file_primary.exists():\n",
    "    print(\"Found an existing model\")\n",
    "    model = load_model(primary_model_file_name)\n",
    "else:\n",
    "    print(\"Model not found, a new one will be crate\")\n",
    "    model = create_dueling_model(input_shape, actions_number)\n",
    "\n",
    "# Print a summary about the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8325f-d204-47a9-84d0-e0532a453eb5",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "Here we ran the training operation. After a training session we save two plot episode - rewards, episode - steps. Also we save a csv with two column: steps and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef554953-be5f-4c07-9537-7f61a548629a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 84, 84)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 20, 32)       21536       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 9, 64)        8256        ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 7, 64)        12352       ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 448)          0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          229888      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 9)            4617        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          229888      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  (None, 1)           0           ['dense_3[0][0]']                \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            513         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " tf.math.subtract (TFOpLambda)  (None, 9)            0           ['dense_3[0][0]',                \n",
      "                                                                  'tf.math.reduce_mean[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 9)           0           ['dense_1[0][0]',                \n",
      " da)                                                              'tf.math.subtract[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 507,050\n",
      "Trainable params: 507,050\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 17:44:39.878265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-14 17:44:39.970284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-14 17:44:39.970951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-14 17:44:39.972894: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-14 17:44:39.974488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-14 17:44:39.975131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-14 17:44:39.975555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-14 17:44:40.878711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-14 17:44:40.879279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-14 17:44:40.879449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-14 17:44:40.879564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3358 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PrioritizedExperienceReplayWithRank' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9215/3167259789.py\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Print a summary about the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmodel_primary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mpre_rank_based\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPrioritizedExperienceReplayWithRank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdouble_DQN_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_prep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_primary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_rank_based\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_epsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_before_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PrioritizedExperienceReplayWithRank' is not defined"
     ]
    }
   ],
   "source": [
    "# Setting the optimizer\n",
    "training = True\n",
    "if training:\n",
    "    model_target = create_dueling_model(input_shape, actions_number)\n",
    "    model_target.set_weights(model.get_weights())\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    policy_training = EpsilonGreedyPolicy(model, actions_number, episodes=episodes, min_epsilon=min_epsilon)\n",
    "    replay_buffer = PrioritizedExperienceReplayRankBased(buffer_size, step_to_heapify, alpha)\n",
    "    agent = DuelDQNAgent(env_prep, model, model_target, policy_training, optimizer, replay_buffer)\n",
    "    steps, rewards = agent.double_dqn_training(batch_size, loss_function, discount_factor, freq_replacement,\n",
    "                                               clipping_value, beta_min, beta_max, episodes)\n",
    "    env_prep.close()\n",
    "    model.save(primary_model_file_name)\n",
    "\n",
    "    ext = \"png\"\n",
    "    name_plot_eps_steps = \"{} Training Episodes Steps.{}\".format(game_name, ext)\n",
    "    name_plot_eps_rewards = \"{} Training Episodes Rewards.{}\".format(game_name, ext)\n",
    "    file_plot_1 = Path(name_plot_eps_steps)\n",
    "    file_plot_2 = Path(name_plot_eps_rewards)\n",
    "    i = 1\n",
    "    while file_plot_1.exists():\n",
    "        file_plot_1 = Path(name_plot_eps_steps)\n",
    "        name_plot_eps_steps = \"{} Training Episodes Steps_{}.{}\".format(game_name, i, ext)\n",
    "        name_plot_eps_rewards = \"{} Training Episodes Rewards_{}.{}\".format(game_name, i, ext)\n",
    "        i += 1\n",
    "\n",
    "    plot_result(\"Episode\", \"Steps\", range(1, episodes+1), steps, name_plot_eps_steps)\n",
    "    plot_result(\"Episode\", \"Steps\", range(1, episodes+1), rewards, name_plot_eps_rewards)\n",
    "\n",
    "\n",
    "    csv_name = \"{}.csv\".format(game_name)\n",
    "    dict = {'steps': steps, 'rewards': rewards}\n",
    "    df = pd.DataFrame(dict)\n",
    "    df.to_csv(csv_name, mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0525ad1-afdc-4a71-9bba-0041a3abad5d",
   "metadata": {},
   "source": [
    "# Run play\n",
    "Here we play a game (one episode), save two plot episode - rewards, episode - steps. Also we save a csv with two column: steps and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b03c22-dc33-4db8-b3e1-a4408d017e01",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (4,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20822/1856718355.py\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mname_plot_eps_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Training Episodes Rewards\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mplot_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Steps\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_plot_eps_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mplot_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Steps\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_plot_eps_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20822/1856718355.py\u001b[0m in \u001b[0;36mplot_result\u001b[0;34m(x_label, y_label, x, y, name)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2766\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2767\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2768\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2769\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \"\"\"\n\u001b[1;32m   1634\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1635\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1636\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    499\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (4,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Play\n",
    "play = False\n",
    "if play:\n",
    "    policy_play = EpsilonGreedyPolicy(model, actions_number, min_epsilon=min_epsilon)\n",
    "    agent = DuelDQNAgent(env_prep, model, None, policy_play, None, None)\n",
    "    steps, reward = agent.play()\n",
    "\n",
    "    ext = \"png\"\n",
    "    name_plot_steps_rewards = \"{} Play Steps Rewards.{}\".format(game_name, ext)\n",
    "    file_plot_1 = Path(name_plot_steps_rewards)\n",
    "    i = 1\n",
    "    while file_plot_1.exists():\n",
    "        file_plot_1 = Path(name_plot_steps_rewards)\n",
    "        name_plot_steps_rewards = \"{} Play Steps Rewards_{}.{}\".format(game_name, i, ext)\n",
    "        i += 1\n",
    "    plot_result(\"Episode\", \"Steps\", steps, reward, name_plot_steps_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf7f35-90b3-47f2-8f02-6c3433194441",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b8394d-9a2e-4c06-af51-50750fb41af5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62287f1e-7723-4187-acf0-6e8ba70785f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
