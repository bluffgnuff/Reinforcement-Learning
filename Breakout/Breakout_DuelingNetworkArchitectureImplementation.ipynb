{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae84dfbf-263e-4d9a-ad88-2eaf271e4421",
   "metadata": {
    "id": "ae84dfbf-263e-4d9a-ad88-2eaf271e4421"
   },
   "source": [
    "# Dueling Network Architecture Implementation\n",
    "The Duelling network is an artificial neural network architecture that has improved the state of the art in the DQN area used in combination with Dual DQN and Prioritized Experience Replay. This approach splits the action value calculation using a combination of state value function and advantage function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a96bf-c613-4ec6-bf0c-0d90f01b4b8c",
   "metadata": {},
   "source": [
    "# Controls\n",
    "The flag below allows to custumize the agent behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1542dee-b62a-465a-aaec-2fcec30496e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "let_training = True  # @param {type:\"boolean\"}\n",
    "let_play =  False # @param {type:\"boolean\"}\n",
    "terminal_on_life_loss = False # @param {type:\"boolean\"}\n",
    "games_to_play = 10 # @param {type:\"number\"}\n",
    "rec_video = False  # @param {type:\"boolean\"}\n",
    "episodes = 4500 # @param {type:\"integer\"}\n",
    "decay_rate = 1.80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8f93a-897c-4712-a81f-545e8efedc37",
   "metadata": {},
   "source": [
    "# System Setting\n",
    "The code below permits to set the system to exec the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "TDL4T4160P-8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21905,
     "status": "ok",
     "timestamp": 1665042566743,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "TDL4T4160P-8",
    "outputId": "f5f0c89a-3413-4a14-b0ac-91798038dadf"
   },
   "outputs": [],
   "source": [
    "# To setting up Google Colab\n",
    "#!pip install gym[atari,accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Lab36Cm53hT3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1666077971727,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "Lab36Cm53hT3",
    "outputId": "685e3000-67fa-4bb3-e1ec-7a50773a17f0"
   },
   "outputs": [],
   "source": [
    "# !pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f1d5fad-136c-4376-b5b0-879077510380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 23:18:51.519831: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "/home/ste/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 23:18:52.732431: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-11-24 23:18:52.733020: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-11-24 23:18:52.758134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 23:18:52.758258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1050 Ti computeCapability: 6.1\n",
      "coreClock: 1.4175GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\n",
      "2022-11-24 23:18:52.758272: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-11-24 23:18:52.759573: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-11-24 23:18:52.759600: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-11-24 23:18:52.760727: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-11-24 23:18:52.760918: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-11-24 23:18:52.762226: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-11-24 23:18:52.762966: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-11-24 23:18:52.765710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-11-24 23:18:52.765833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 23:18:52.765989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 23:18:52.766070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "# Check that tensor flow is able to use the GPU\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc578666-0c50-4681-8b90-bc48ece391d6",
   "metadata": {
    "id": "fc578666-0c50-4681-8b90-bc48ece391d6"
   },
   "source": [
    "# Searching for available environments\n",
    "We want to test the performance of our architecture with the Atari game 'Asterix'.\n",
    "Here we check which kind of versions of this game are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82d78d38-4889-44d7-a8cd-49d3c9059bad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1539,
     "status": "ok",
     "timestamp": 1665042568266,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "82d78d38-4889-44d7-a8cd-49d3c9059bad",
    "outputId": "9d44fd58-f58f-4b95-cfa9-f00f7d2a04e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALE/Breakout-ram-v5\n",
      "ALE/Breakout-v5\n",
      "Breakout-ram-v0\n",
      "Breakout-ram-v4\n",
      "Breakout-ramDeterministic-v0\n",
      "Breakout-ramDeterministic-v4\n",
      "Breakout-ramNoFrameskip-v0\n",
      "Breakout-ramNoFrameskip-v4\n",
      "Breakout-v0\n",
      "Breakout-v4\n",
      "BreakoutDeterministic-v0\n",
      "BreakoutDeterministic-v4\n",
      "BreakoutNoFrameskip-v0\n",
      "BreakoutNoFrameskip-v4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ste/anaconda3/lib/python3.9/site-packages/gym/envs/registration.py:423: UserWarning: \u001b[33mWARN: Custom namespace `ALE` is being overridden by namespace `ALE`. If you are developing a plugin you shouldn't specify a namespace in `register` calls. The namespace is specified through the entry point package metadata.\u001b[0m\n",
      "  logger.warn(\n",
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "from gym import envs\n",
    "\n",
    "# Searching for available environments\n",
    "game_name = \"Breakout\"\n",
    "all_envs = envs.registry.values()\n",
    "env_ids = [env_spec.id for env_spec in all_envs]\n",
    "\n",
    "for id in sorted(env_ids):\n",
    "    if game_name in id:\n",
    "        print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5b1aa-6e92-4ac3-9079-c49121fafaef",
   "metadata": {
    "id": "12c5b1aa-6e92-4ac3-9079-c49121fafaef"
   },
   "source": [
    "# Environment Configuration\n",
    "We select the version 4 of the environment with no frame skipping and select as render mode human. The no frame skipping is used to make this environment compatible with the optimization made by *AtariPreprocessing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cecf335-3193-4ce8-90f5-dfe1c3a82e5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1665042568266,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "3cecf335-3193-4ce8-90f5-dfe1c3a82e5f",
    "outputId": "28ea8b12-3721-41e2-a222-156e9e23bd33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.wrappers import AtariPreprocessing\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# Make Parameters:\n",
    "game_mode = \"NoFrameskip\"  # [Deterministic | NoFrameskip | ram | ramDeterministic | ramNoFrameskip ]\n",
    "game_version = \"v4\"  # [v0 | v4 | v5]\n",
    "env_name = '{}{}-{}'.format(game_name, game_mode, game_version)\n",
    "env_render_mode = 'human'  # [human | rgb_array]\n",
    "env_frame_skip = 4\n",
    "\n",
    "env = gym.make(env_name, render_mode=env_render_mode)\n",
    "#env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b68bf0-d09a-432b-9fe9-9991f0d8a84a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1665042568267,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "23b68bf0-d09a-432b-9fe9-9991f0d8a84a",
    "outputId": "1921b05a-a9fa-47f2-9246-26adafe63578"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb4423dc0d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env.reset() \n",
    "state = env.render(mode=\"rgb_array\")\n",
    "plt.imshow(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04326819-e929-40fe-9a26-ecfc317f7882",
   "metadata": {
    "id": "04326819-e929-40fe-9a26-ecfc317f7882"
   },
   "source": [
    "## Enviornment Observations\n",
    "Below we have a first look at the environment characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "143cf671-a4e9-46a5-b06c-5f66f83fdc22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1665042568267,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "143cf671-a4e9-46a5-b06c-5f66f83fdc22",
    "outputId": "ffd6528b-5a7d-4fbf-d9e5-c4aad9150b7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "401a5ee2-bf0f-4721-be41-32c498caf4f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1665042568268,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "401a5ee2-bf0f-4721-be41-32c498caf4f3",
    "outputId": "ef5b5156-1509-47c2-98e8-2ffa96ea7495"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c5513e8-9f39-47bd-b667-a7436383795e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1665042568268,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "2c5513e8-9f39-47bd-b667-a7436383795e",
    "outputId": "f6792f4c-9ef4-485e-ef92-4875a0e28914"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad4449-5a13-4032-9645-53f78265617b",
   "metadata": {
    "id": "39ad4449-5a13-4032-9645-53f78265617b"
   },
   "source": [
    "## Environment Optimization\n",
    "We optimize the environment by adding the frame skipping, changing its observation in greyscale and following the experiment done in the paper we set to at most 30 the no-op actions; to get this we use the AtariPreprocessing wrapper.\n",
    "We use Framestack to create observations of 4 frames to give the idea of movement to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63a89f89-355e-407a-a036-d1d14bfe3fba",
   "metadata": {
    "id": "63a89f89-355e-407a-a036-d1d14bfe3fba"
   },
   "outputs": [],
   "source": [
    "env = AtariPreprocessing(env, frame_skip=env_frame_skip, grayscale_obs=True, terminal_on_life_loss=terminal_on_life_loss, noop_max=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43876bf8-8f34-4118-990a-dba1bbbc67fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "executionInfo": {
     "elapsed": 894,
     "status": "ok",
     "timestamp": 1665042569152,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "43876bf8-8f34-4118-990a-dba1bbbc67fd",
    "outputId": "bead8004-41c7-4082-d536-f1d6bce913c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb4402161c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = FrameStack(env, 4)\n",
    "state = env.reset()\n",
    "_, axarr = plt.subplots(2,2, figsize=(10,10))\n",
    "axarr[0, 0].imshow(state[0])\n",
    "axarr[0, 1].imshow(state[1])\n",
    "axarr[1, 0].imshow(state[2])\n",
    "axarr[1, 1].imshow(state[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51c6620c-52ee-48de-bc6b-4caba64b8f1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1665042569153,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "51c6620c-52ee-48de-bc6b-4caba64b8f1f",
    "outputId": "4c23526d-3fe5-4795-bba8-e84ad50f9222"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0735fbbc-11c3-418f-8026-f3eba9dc5cb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1665042569153,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "0735fbbc-11c3-418f-8026-f3eba9dc5cb9",
    "outputId": "ce6cfa59-1f25-4917-d5c2-b5aa4a49c4c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dccc025-b2ae-48c8-a70d-0e146d5c9d7c",
   "metadata": {
    "id": "9dccc025-b2ae-48c8-a70d-0e146d5c9d7c"
   },
   "source": [
    "# Network configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579e3d9-51f6-4612-bf77-25db75a98d4a",
   "metadata": {
    "id": "e579e3d9-51f6-4612-bf77-25db75a98d4a"
   },
   "source": [
    "## Policy\n",
    "The policy is the component that chooses the action to perform; using an $\\epsilon$-gready policy the action chosen can be random with probability $\\epsilon$ or an action suggested by the ANN with probability $1 - \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c3213c2-3be1-4337-bf89-5a32f9dbf2c8",
   "metadata": {
    "id": "7c3213c2-3be1-4337-bf89-5a32f9dbf2c8"
   },
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class EpsilonGreedyPolicy:\n",
    "\n",
    "    def __init__(self, model, action_space_size, episodes=1, min_epsilon=0, decay_rate=1.35):\n",
    "        self.model = model\n",
    "        self.action_space_size = action_space_size\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.episode = 1\n",
    "        self.episodes = episodes\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "    def get_action(self, state):\n",
    "        epsilon_decay = (self.episode / self.episodes)*self.decay_rate\n",
    "        epsilon = max(1 - epsilon_decay, self.min_epsilon)\n",
    "        rnd = random.random()\n",
    "        # print(random, epsilon)\n",
    "        if rnd < epsilon:\n",
    "            action = random.randint(self.action_space_size)\n",
    "            return action\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = self.model(state_tensor)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "            return action\n",
    "\n",
    "    def next_episode(self):\n",
    "        self.episode += 1\n",
    "\n",
    "    def reset_episodes(self):\n",
    "        self.episode = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c4fef-7fd8-4e38-9eac-b7623174d588",
   "metadata": {
    "id": "255c4fef-7fd8-4e38-9eac-b7623174d588"
   },
   "source": [
    "## Replication Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f6dd9-3076-4c01-bf7f-01b9400c6fc2",
   "metadata": {
    "id": "6d9f6dd9-3076-4c01-bf7f-01b9400c6fc2"
   },
   "source": [
    "### Prioritized experience replay\n",
    "The Prioritized experience replay was introduced in the paper \"Prioritized experience replay\" (https://arxiv.org/abs/1511.05952), it consists in an evolution of the replay buffer that orders the experiences to replay by priority. In this experiment we adopt the **rank-based** variant where the experience sampling from the buffer it's done with probability $ P(i) = \\frac{p_{i}^{\\alpha}}{\\sum_k{p_{k}^{\\alpha}}} $ and $p(i)=\\frac{1}{rank(i)}$ where $rank(i)$ is the rank of the transition *i*, $\\alpha$ is called **priority exponent**. It is necessary to compute the importance sampling weights as $w_j = \\frac{(N * P(j))^{-\\beta}}{max_i{w_i}} $ and $w_i = (\\frac{1}{N} . \\frac{1}{P(i)})^\\beta$ to avoid overfitting for the experiences with more priority.\n",
    "\n",
    "In both the paper the parameters are setted as follows: **priority exponent** $\\alpha= 0.7$,  the **importance sampling exponent** $\\beta = [0.5, 1]$.\n",
    "In the paper a heap array structure is proposed to implement the buffer. Due to the particular structure and the amount of property of the replay buffer in the Prioritized Experience Replay we choose to describe it as a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e8897eb-7fea-48e8-93d4-cfd169a6278a",
   "metadata": {
    "id": "9e8897eb-7fea-48e8-93d4-cfd169a6278a"
   },
   "outputs": [],
   "source": [
    "# import heapq as heap\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import heapq\n",
    "\n",
    "class PrioritizedExperienceReplayRankBased:\n",
    "    \"\"\"\n",
    "    replay_buffer       - contains the tuples (TD_error, transaction_id, experience)\n",
    "    max_buffer_size     - it's the max size of the buffer, over which before add an experience one is remove\n",
    "    alpha               - the alpha parameter used to calculate the probability of the i-th element P(i) to be sampled\n",
    "    self.max_td_error   - max td in the buffer\n",
    "\n",
    "    Old\n",
    "    time_to_haepify - time steps before sort the structure\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_buffer_size, alpha):\n",
    "        self.max_buffer_size = max_buffer_size\n",
    "        # (TD, experience)\n",
    "        self.replay_buffer = []\n",
    "        self.alpha = alpha\n",
    "        # The experience added has the maximum priority but once it sampled it will be updated with a more correct\n",
    "        # value.\n",
    "        self.max_td_error = 0\n",
    "        # self.heapify_threshold = step_to_heapify  # here we stock the threshold to sort the buffer\n",
    "        # self.step_to_heapify = step_to_heapify  # number of next steps before heapify\n",
    "\n",
    "    def set_alpha(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # Add experience in the buffer mapping it with its last TD_error.\n",
    "    # Heapq structure try to sort the elements of different tuple comparing from the first element of the tuple and\n",
    "    # continuing with next element until the two tuples have an element different. We are interested in sorting by TD,\n",
    "    # we don't care to sort on states, actions, or rewards. So, we use the transaction id to sort the transaction\n",
    "    # that is older in the buffer.\n",
    "    # The transaction_id of a transaction could be the -ith frame number of the whole training representing\n",
    "    # when the transaction happened.\n",
    "    # NB This approach avoids headppush fails when try to compare two states.\n",
    "    def add_experience(self, transaction_id, experience):\n",
    "        if len(self.replay_buffer) == self.max_buffer_size:\n",
    "            self.remove_experience()\n",
    "\n",
    "        # New experiences where td_error is unknown are set with the max td_error\n",
    "        if len(self.replay_buffer) > 0:\n",
    "            self.max_td_error = self.replay_buffer[0][0]\n",
    "        heapq.heappush(self.replay_buffer, (-self.max_td_error, transaction_id, experience))\n",
    "\n",
    "    # Remove experience from the buffer\n",
    "    def remove_experience(self, index=-1):\n",
    "        self.replay_buffer.pop(index)\n",
    "\n",
    "    @staticmethod\n",
    "    def zip_f_sampling(alpha, n):\n",
    "        x = np.arange(1, n + 1)\n",
    "        weights = x ** (-alpha)\n",
    "        weights /= weights.sum()\n",
    "        zipf = stats.rv_discrete(values=(x, weights))\n",
    "        return zipf.rvs() - 1\n",
    "\n",
    "    # Get batch_size samples from the buffer; using the beta parameter to compute the importance sampling weight\n",
    "    # Beta value can change while training we can delegate its control outside\n",
    "    def sample_experience(self, batch_size, beta):\n",
    "        experiences = []\n",
    "        importance_sampling_weights = []\n",
    "        n = len(self.replay_buffer) - 1\n",
    "        indexes = []\n",
    "        transaction_id = []\n",
    "\n",
    "        for i in range(0, batch_size):\n",
    "            # Sample index and check the experience is not already present in the batch\n",
    "            index = self.zip_f_sampling(self.alpha, n)\n",
    "            while index in indexes:\n",
    "                index = self.zip_f_sampling(self.alpha, n)\n",
    "            indexes.append(index)\n",
    "            # importance sampling weights computation\n",
    "            rank = index + 1\n",
    "            pj = 1 / rank\n",
    "            importance_sampling_weights.append(((n * pj) ** (-beta)))\n",
    "            transaction_id.append(self.replay_buffer[index][1])\n",
    "            experiences.append(self.replay_buffer[index][2])\n",
    "\n",
    "        # Normalization step\n",
    "        max_weight = max(importance_sampling_weights)\n",
    "        importance_sampling_weights_normalized = np.divide(importance_sampling_weights, max_weight)\n",
    "        return indexes, transaction_id, experiences, importance_sampling_weights_normalized\n",
    "\n",
    "    def update_td_error(self, indexes, td_errors):\n",
    "        # TODO update tupla e dopo ordina\n",
    "        for index, td_error in zip(indexes, td_errors):\n",
    "            my_list = list(self.replay_buffer[index])\n",
    "            my_list[0] = -abs(td_error)\n",
    "            self.replay_buffer[index] = tuple(my_list)\n",
    "        heapq.heapify(self.replay_buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb99df-922b-4ef1-9865-8d653c135eb8",
   "metadata": {
    "id": "5ddb99df-922b-4ef1-9865-8d653c135eb8"
   },
   "source": [
    "## Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e58cc1cd-e7dc-4f40-ba18-b5127abc5fff",
   "metadata": {
    "id": "e58cc1cd-e7dc-4f40-ba18-b5127abc5fff"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras as krs\n",
    "\n",
    "input_shape = env.observation_space.shape\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d096c-c9d8-4012-9400-25075d3779cc",
   "metadata": {
    "id": "257d096c-c9d8-4012-9400-25075d3779cc"
   },
   "source": [
    "## Neural Network Creation\n",
    "The network architecture proposed follows the structure used in *\"Dueling Network Architectures for Deep Reinforcement Learning\"* https://arxiv.org/abs/1511.06581 composed of 3 convolutional layers and 2 fully connected layers for each stream (advantage, value).\n",
    "It's possible to create a dueling network using the `DQNAgent` of `rl.agents.dqn` setting `enable_dueling_network=True` in the constructor, but the purpose of this experiment is to show how to develop it manually so it is not used.\n",
    "\n",
    "The output of the value stream and the output of the advantage stream are merged to obtain the action-value function in the last module of the network using the following formula:\n",
    "$$ Q(s, a; \\theta, \\alpha, \\beta)^\\pi = V(s; \\theta, \\beta)^\\pi + (A(s, a; \\theta, \\alpha)^\\pi - \\frac{1}{|A|}\\sum_{a'}A(s, a'; \\theta, \\alpha)^\\pi) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a878b21-2040-4ca8-aeff-3563499bf9f9",
   "metadata": {
    "id": "0a878b21-2040-4ca8-aeff-3563499bf9f9"
   },
   "outputs": [],
   "source": [
    "from tensorflow import math\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "\n",
    "backend.set_image_data_format('channels_first')\n",
    "def create_dueling_model(input_shape, number_actions):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    value_stream_1 = layers.Dense(512)(layer4)\n",
    "    value_stream_2 = layers.Dense(1)(value_stream_1)  # scalar output size\n",
    "\n",
    "    advantage_stream_1 = layers.Dense(512)(layer4)\n",
    "    advantage_stream_2 = layers.Dense(number_actions)(advantage_stream_1)  # output size equal to the actions available\n",
    "\n",
    "    # Combination of the streams: a Q value for each state\n",
    "    q_values = value_stream_2 + math.subtract(advantage_stream_2, math.reduce_mean(advantage_stream_2, axis=1,\n",
    "                                                                                   keepdims=True))\n",
    "    # Alternative q_value\n",
    "    # q_value = value_stream_2 + (advantage_stream_2 - backend.max(advantage_stream_2, axis=1, keepdims=True))\n",
    "    return Model(inputs=[inputs], outputs=[q_values])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b2cfe-111c-4afe-a3c5-391066124fe3",
   "metadata": {
    "id": "ff1b2cfe-111c-4afe-a3c5-391066124fe3"
   },
   "source": [
    "# Agent \n",
    "Here we define a custom agent to perfom action in the environment using a DoubleDQN training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391a27a-28ac-4d3d-9c8f-b42bb303bd76",
   "metadata": {
    "id": "e391a27a-28ac-4d3d-9c8f-b42bb303bd76"
   },
   "source": [
    "## Play one step\n",
    "With this function we want to ask the policy what action must be chosen and perform it on the enironment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec3a88-954a-49f5-96b9-a6d34c29a16d",
   "metadata": {
    "id": "bdec3a88-954a-49f5-96b9-a6d34c29a16d"
   },
   "source": [
    "## Gradient \n",
    "In our scenario the gradient that is backpropageted to the last convolutional layer must be rescaled by $\\frac{1}{\\sqrt{2}}$. Furthermore we have to realize by hand the gradient clipping that is not realized by the optimizer since we are using a custom loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bd489-2d61-4334-b692-51a88f5ecb29",
   "metadata": {
    "id": "c45bd489-2d61-4334-b692-51a88f5ecb29"
   },
   "source": [
    "## Double DQN Training\n",
    "Double DQN algorithm uses a second network, beyond the network used for the prediction. So in the training process the main network is used to choose an action and another to evaluate it, this permits to mitigate the overfitting present in the classic DQN. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gg9foUcDcp1r",
   "metadata": {
    "id": "gg9foUcDcp1r"
   },
   "source": [
    "## DQN Agent code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bad3e87-4bed-4e08-a34e-8a2e2fc06a83",
   "metadata": {
    "id": "6bad3e87-4bed-4e08-a34e-8a2e2fc06a83"
   },
   "outputs": [],
   "source": [
    "import math as mt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DuelDQNAgent:\n",
    "\n",
    "    # We keep the creation model outside the agent to ensure a fine-grained control on it\n",
    "    def __init__(self, env, model, policy, model_target=None, optimizer=None, replay_buffer=None):\n",
    "        self.env = env\n",
    "        self.model_primary = model\n",
    "        self.model_target = model_target\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "    def set_policy(self, policy):\n",
    "        self.policy = policy\n",
    "\n",
    "    # Execs one action receiving in input the current state\n",
    "    def play_one_step(self, state):\n",
    "        action = self.policy.get_action(state)\n",
    "        # print(\"action {}\".format(action))\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        return action, reward, next_state, done, info\n",
    "\n",
    "    # Play\n",
    "    def play(self):\n",
    "        state = self.env.reset()\n",
    "        steps = 0\n",
    "        cumulative_reward = 0\n",
    "        while True:\n",
    "            action, reward, next_state, done, info = self.play_one_step(state)\n",
    "            cumulative_reward += reward\n",
    "            if done:\n",
    "                print(\"DONE number of steps: {} reward:  {}\".format(steps, cumulative_reward))\n",
    "                break\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "        return steps, cumulative_reward\n",
    "\n",
    "    # Double DQN Training\n",
    "    #     @tf.function\n",
    "    @staticmethod\n",
    "    def gradient_clipping(gradients, clipping_value):\n",
    "        clipped_gradients = [(tf.clip_by_norm(grad, clipping_value)) for grad in gradients]\n",
    "        return clipped_gradients\n",
    "\n",
    "    #     @tf.function\n",
    "    def weighted_gradient(self, best_on_target_q_values, importance_sampling_weights, states, loss_function, mask):\n",
    "        with tf.GradientTape() as tape:\n",
    "            batch_size = len(states)\n",
    "            tape.watch(importance_sampling_weights)\n",
    "            best_on_target_q_values = tf.expand_dims(best_on_target_q_values, 1)\n",
    "            all_q_values = self.model_primary(states)\n",
    "            q_values = tf.reduce_sum(all_q_values * mask, axis=1)\n",
    "            q_values = tf.expand_dims(q_values, 1)\n",
    "            # The target and the predicted values has been expanded following the instruction in:\n",
    "            # https://www.tensorflow.org/api_docs/python/tf/compat/v1/losses/mean_squared_error\n",
    "            loss_values = loss_function(y_true=best_on_target_q_values, y_pred=q_values,\n",
    "                                        sample_weight=importance_sampling_weights)\n",
    "            loss_value = tf.reduce_sum(loss_values) / batch_size\n",
    "\n",
    "        grads = tape.gradient(loss_value, self.model_primary.trainable_variables)\n",
    "        return grads, loss_values.numpy()\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_grad(gradients, rescale_value, index):\n",
    "        tensor_to_scale = gradients[index]\n",
    "        rescaled_tensor = tensor_to_scale * rescale_value\n",
    "        gradients[index] = rescaled_tensor\n",
    "        return gradients\n",
    "\n",
    "    # Collects samples of the previous experiences from the replay buffer\n",
    "    # and use them to improve the weights update of the Neural Network.\n",
    "    def double_dqn_training_step(self, batch_size, loss_function, discount_factor, clipping_value, beta, step_size=1):\n",
    "        indexes, transaction_ids, experiences, importance_sampling_weights = self.replay_buffer.sample_experience(\n",
    "            batch_size, beta)\n",
    "        states, actions, rewards, next_states, dones = [np.array([experience[field_index] for experience in experiences]\n",
    "                                                                 ) for field_index in range(5)]\n",
    "\n",
    "        action_space = self.env.action_space.n\n",
    "        # Predict using the primary network\n",
    "        next_q_values = self.model_primary.predict(next_states)\n",
    "        next_q_values_target = self.model_target.predict(next_states)\n",
    "\n",
    "        # Select the action that lead us to the higher next Q value\n",
    "        best_actions = np.argmax(next_q_values, axis=1)\n",
    "        best_action_mask = tf.one_hot(best_actions, action_space)\n",
    "\n",
    "        next_q_value_target = tf.reduce_sum(next_q_values_target * best_action_mask, axis=1)\n",
    "        best_on_target_q_values = (rewards + (1 - dones) * discount_factor * next_q_value_target)\n",
    "        # Punishment\n",
    "        best_on_target_q_values = best_on_target_q_values * (1-dones) - (dones)\n",
    "\n",
    "        mask = tf.one_hot(actions, action_space)\n",
    "        importance_sampling_weights = tf.convert_to_tensor(importance_sampling_weights, tf.float32)\n",
    "        weighted_gradient, loss_values = self.weighted_gradient(best_on_target_q_values, importance_sampling_weights,\n",
    "                                                                states, loss_function, mask)\n",
    "        self.replay_buffer.update_td_error(indexes, loss_values)\n",
    "\n",
    "        # We rescale the last convolutional layer to 1/sqrt(2) to balance the double backpropagation\n",
    "        # The index of the last sequential layer\n",
    "        rescale_value = (1 / mt.sqrt(2))\n",
    "        index_gradient_to_rescale = 4\n",
    "        rescaled_grads = self.rescale_grad(weighted_gradient, rescale_value, index_gradient_to_rescale)\n",
    "\n",
    "        # Since we are in a custom loop we have to clip the gradient by hand, we can't delegate it to the optimizer\n",
    "        clipped_gradients = self.gradient_clipping(rescaled_grads, clipping_value)\n",
    "        # Application gradient descent trough optimizer\n",
    "        self.optimizer.apply_gradients(zip(clipped_gradients, self.model_primary.trainable_variables))\n",
    "\n",
    "    # We use the training step just when there is enough samples on the replay buffer\n",
    "    def double_dqn_training(self, batch_size, loss_function, discount_factor, freq_replacement, training_freq,\n",
    "                            clipping_value, beta_min, beta_max, max_episodes=600, max_steps=10800):\n",
    "        rewards_stock = []\n",
    "        steps_stock = []\n",
    "        cumulative_steps = 0\n",
    "\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            state = self.env.reset()\n",
    "            rewards = 0\n",
    "            steps = 0\n",
    "            beta = max(beta_min, (beta_max * episode / max_episodes))\n",
    "\n",
    "            while True:\n",
    "                action, reward, next_state, done, info = self.play_one_step(state)\n",
    "                experience = [state, action, reward, next_state, done]\n",
    "                rewards += reward\n",
    "                self.replay_buffer.add_experience(cumulative_steps, experience)\n",
    "                cumulative_steps += 1\n",
    "                steps += 1\n",
    "\n",
    "                if len(self.replay_buffer.replay_buffer) > batch_size and (cumulative_steps % training_freq) == 0:\n",
    "                    self.double_dqn_training_step(batch_size, loss_function, discount_factor, clipping_value, beta)\n",
    "                if (cumulative_steps % freq_replacement) == 0:\n",
    "                    self.model_target.set_weights(self.model_primary.get_weights())\n",
    "                if steps == max_steps:\n",
    "                    print(\n",
    "                        \"ABORTED episode = {} number of steps = {} reward = {}\".format(episode, steps, rewards))\n",
    "                if done:\n",
    "                    print(\n",
    "                        \"DONE episode = {} number of steps = {} reward = {}\".format(episode, steps, rewards))\n",
    "                if done or steps == max_steps:\n",
    "                    rewards_stock.append(rewards)\n",
    "                    steps_stock.append(steps)\n",
    "                    break\n",
    "                state = next_state\n",
    "\n",
    "            self.policy.next_episode()\n",
    "\n",
    "        return steps_stock, rewards_stock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538fe86-67f4-4292-b80c-0225a088b271",
   "metadata": {
    "id": "9538fe86-67f4-4292-b80c-0225a088b271"
   },
   "source": [
    "## Result plots\n",
    "We use this function to generate the plot representing the rewards or the steps for each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ef6ec3d-002c-4d30-9052-4e05db6839a0",
   "metadata": {
    "id": "6ef6ec3d-002c-4d30-9052-4e05db6839a0"
   },
   "outputs": [],
   "source": [
    "def plot_result(x_label, y_label, x, y, name):\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.plot(x, y)\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364dba26-35cc-4d53-878a-6bf0496b5a24",
   "metadata": {
    "id": "364dba26-35cc-4d53-878a-6bf0496b5a24"
   },
   "source": [
    "# Training\n",
    "The learning step is executed with the **Double Deep Q-networks** algorithm presented in the paper *\"Deep reinforcement learning with double Q-learning\"*.https://arxiv.org/pdf/1509.06461.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e16f49-345d-460c-922d-f529d9291a07",
   "metadata": {
    "id": "23e16f49-345d-460c-922d-f529d9291a07"
   },
   "source": [
    "## Training parameters\n",
    "We adopt as optimizer the **Adam** implementation setting the learning rate equal to $6.25x10^{-5}$ and **clipping the gradient** norm at most to 10; the parameters are specified in the paper \"*Deep reinforcement learning with double Q-learning*\" (https://arxiv.org/pdf/1509.06461.pdf)\n",
    "To evaluate the loss score we use the `mean_squared_error`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4742cf2f-c7c6-403e-9da9-32d47e7717da",
   "metadata": {
    "id": "4742cf2f-c7c6-403e-9da9-32d47e7717da"
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Environment info\n",
    "input_shape = env.observation_space.shape\n",
    "actions_number = env.action_space.n\n",
    "\n",
    "# Model persistent file\n",
    "primary_model_file_name = \"{}_dueling_model\".format(game_name)\n",
    "\n",
    "# Training Parameters\n",
    "loss_function = losses.MeanSquaredError(reduction=losses.Reduction.NONE)\n",
    "batch_size = 32 # @param {type:\"integer\"}\n",
    "discount_factor = 0.95 # @param {type:\"number\"}\n",
    "learning_rate = 6.25e-5 # @param {type:\"number\"}\n",
    "clipping_value = 10 # @param {type:\"number\"}\n",
    "training_freq = 4 # @param {type:\"integer\"}\n",
    "\n",
    "# Dual DQN Training\n",
    "freq_replacement = 1000 # @param {type:\"integer\"}\n",
    "\n",
    "# Replay buffer parameters\n",
    "buffer_size = 100000 # @param {type:\"integer\"}\n",
    "# step_to_heapify = 200 # @param {type:\"integer\"}\n",
    "alpha = 0.7 # @param {type:\"number\"}\n",
    "beta_max = 1 # @param {type:\"number\"}\n",
    "beta_min = 0.5 # @param {type:\"number\"}\n",
    "\n",
    "# Policy parameters\n",
    "min_epsilon = 0.01 # @param {type:\"number\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf7e90-0940-497c-8dc3-cafa5366bc3f",
   "metadata": {
    "id": "5fbf7e90-0940-497c-8dc3-cafa5366bc3f"
   },
   "source": [
    "## Model creation / loading \n",
    "In this step we check whether there is an already saved model and load it in that case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44e76e0d-1130-47d8-b055-dbaa73907926",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3653,
     "status": "ok",
     "timestamp": 1665042575453,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "44e76e0d-1130-47d8-b055-dbaa73907926",
    "outputId": "49ca5eb3-67e4-44d7-a61a-02e830afe86d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found an existing model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 23:18:55.243977: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-24 23:18:55.244363: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-11-24 23:18:55.244494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 23:18:55.244618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1050 Ti computeCapability: 6.1\n",
      "coreClock: 1.4175GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\n",
      "2022-11-24 23:18:55.244638: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-11-24 23:18:55.244660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-11-24 23:18:55.244673: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-11-24 23:18:55.244683: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-11-24 23:18:55.244695: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-11-24 23:18:55.244706: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-11-24 23:18:55.244717: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-11-24 23:18:55.244728: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-11-24 23:18:55.244782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 23:18:55.244903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 23:18:55.244986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-11-24 23:18:55.245009: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-11-24 23:18:55.608096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-11-24 23:18:55.608114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-11-24 23:18:55.608122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-11-24 23:18:55.608527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 23:18:55.608735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 23:18:55.608992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 23:18:55.609196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3454 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4, 84, 84)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 20, 20)   8224        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 9, 9)     32832       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 7, 7)     36928       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3136)         0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          1606144     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            2052        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          1606144     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean (TFOpLambda (None, 1)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            513         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda)   (None, 4)            0           dense_3[0][0]                    \n",
      "                                                                 tf.math.reduce_mean[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 4)            0           dense_1[0][0]                    \n",
      "                                                                 tf.math.subtract[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 3,292,837\n",
      "Trainable params: 3,292,837\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Model creation\n",
    "file_primary = Path(primary_model_file_name)\n",
    "if file_primary.exists():\n",
    "    print(\"Found an existing model\")\n",
    "    model = load_model(primary_model_file_name)\n",
    "else:\n",
    "    print(\"Model not found, a new one will be crate\")\n",
    "    model = create_dueling_model(input_shape, actions_number)\n",
    "\n",
    "# Print a summary about the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8325f-d204-47a9-84d0-e0532a453eb5",
   "metadata": {
    "id": "3cf8325f-d204-47a9-84d0-e0532a453eb5"
   },
   "source": [
    "## Training\n",
    "Here we ran the training operation. After a training session we save two plot episodes - rewards, episodes - steps. Also we save a csv with two columns: steps and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef554953-be5f-4c07-9537-7f61a548629a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 32149848,
     "status": "error",
     "timestamp": 1665074725297,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "ef554953-be5f-4c07-9537-7f61a548629a",
    "outputId": "d26f332c-d658-4041-b41b-06a1d68fdd00"
   },
   "outputs": [],
   "source": [
    "def training():\n",
    "    try:\n",
    "        model_target = create_dueling_model(input_shape, actions_number)\n",
    "        model_target.set_weights(model.get_weights())\n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "        policy_training = EpsilonGreedyPolicy(model, actions_number, episodes=episodes, min_epsilon=min_epsilon, decay_rate=decay_rate)\n",
    "        replay_buffer = PrioritizedExperienceReplayRankBased(buffer_size, alpha)\n",
    "        agent = DuelDQNAgent(env, model, policy_training, model_target, optimizer, replay_buffer)\n",
    "        steps, rewards = agent.double_dqn_training(batch_size, loss_function, discount_factor, freq_replacement,\n",
    "                                                   training_freq, clipping_value, beta_min, beta_max, episodes)\n",
    "\n",
    "        ext = \"png\"\n",
    "        name_plot_eps_steps = \"{} Training Episodes Steps.{}\".format(game_name, ext)\n",
    "        name_plot_eps_rewards = \"{} Training Episodes Rewards.{}\".format(game_name, ext)\n",
    "        file_plot_1 = Path(name_plot_eps_steps)\n",
    "        i = 1\n",
    "        while file_plot_1.exists():\n",
    "            i += 1\n",
    "            name_plot_eps_steps = \"{} Training Episodes Steps_{}.{}\".format(game_name, i, ext)\n",
    "            name_plot_eps_rewards = \"{} Training Episodes Rewards_{}.{}\".format(game_name, i, ext)\n",
    "            file_plot_1 = Path(name_plot_eps_steps)\n",
    "\n",
    "        plot_result(\"Episode\", \"Steps\", range(1, episodes + 1), steps, name_plot_eps_steps)\n",
    "        plot_result(\"Episode\", \"Rewards\", range(1, episodes + 1), rewards, name_plot_eps_rewards)\n",
    "\n",
    "        csv_name = \"{}.csv\".format(game_name)\n",
    "        dict = {'steps': steps, 'rewards': rewards}\n",
    "        df = pd.DataFrame(dict)\n",
    "        df.to_csv(csv_name, mode='a', header=False)\n",
    "\n",
    "    finally:\n",
    "        model.save(primary_model_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a05b7-819e-4b6d-ad07-bce40bd6fe01",
   "metadata": {},
   "source": [
    "# Play\n",
    "Here we play a game (one episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96580b5d-b9c7-418c-bdbe-a780b87ce2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play():    \n",
    "    policy_play = EpsilonGreedyPolicy(model, actions_number, min_epsilon=min_epsilon)\n",
    "    agent = DuelDQNAgent(env, model, policy_play)\n",
    "    steps, reward = agent.play()\n",
    "    return reward, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0525ad1-afdc-4a71-9bba-0041a3abad5d",
   "metadata": {
    "id": "b0525ad1-afdc-4a71-9bba-0041a3abad5d"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b03c22-dc33-4db8-b3e1-a4408d017e01",
   "metadata": {
    "id": "28b03c22-dc33-4db8-b3e1-a4408d017e01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 23:18:58.592929: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-11-24 23:18:58.609124: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200660000 Hz\n",
      "2022-11-24 23:18:58.692512: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-11-24 23:18:58.814678: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE episode = 1 number of steps = 128 reward = 0.0\n",
      "DONE episode = 2 number of steps = 248 reward = 2.0\n",
      "DONE episode = 3 number of steps = 220 reward = 2.0\n",
      "DONE episode = 4 number of steps = 138 reward = 0.0\n",
      "DONE episode = 5 number of steps = 136 reward = 0.0\n",
      "DONE episode = 6 number of steps = 139 reward = 0.0\n",
      "DONE episode = 7 number of steps = 133 reward = 0.0\n",
      "DONE episode = 8 number of steps = 175 reward = 1.0\n",
      "DONE episode = 9 number of steps = 281 reward = 4.0\n",
      "DONE episode = 10 number of steps = 229 reward = 3.0\n",
      "DONE episode = 11 number of steps = 182 reward = 1.0\n",
      "DONE episode = 12 number of steps = 144 reward = 0.0\n",
      "DONE episode = 13 number of steps = 166 reward = 1.0\n",
      "DONE episode = 14 number of steps = 189 reward = 1.0\n",
      "DONE episode = 15 number of steps = 177 reward = 1.0\n",
      "DONE episode = 16 number of steps = 262 reward = 3.0\n",
      "DONE episode = 17 number of steps = 168 reward = 1.0\n",
      "DONE episode = 18 number of steps = 169 reward = 1.0\n",
      "DONE episode = 19 number of steps = 278 reward = 3.0\n",
      "DONE episode = 20 number of steps = 149 reward = 0.0\n",
      "DONE episode = 21 number of steps = 135 reward = 0.0\n",
      "DONE episode = 22 number of steps = 135 reward = 0.0\n",
      "DONE episode = 23 number of steps = 295 reward = 5.0\n",
      "DONE episode = 24 number of steps = 189 reward = 1.0\n",
      "DONE episode = 25 number of steps = 304 reward = 4.0\n",
      "DONE episode = 26 number of steps = 131 reward = 0.0\n",
      "DONE episode = 27 number of steps = 309 reward = 4.0\n",
      "DONE episode = 28 number of steps = 302 reward = 4.0\n",
      "DONE episode = 29 number of steps = 207 reward = 2.0\n",
      "DONE episode = 30 number of steps = 180 reward = 1.0\n",
      "DONE episode = 31 number of steps = 185 reward = 1.0\n",
      "DONE episode = 32 number of steps = 215 reward = 2.0\n",
      "DONE episode = 33 number of steps = 144 reward = 0.0\n",
      "DONE episode = 34 number of steps = 200 reward = 1.0\n",
      "DONE episode = 35 number of steps = 262 reward = 3.0\n",
      "DONE episode = 36 number of steps = 258 reward = 3.0\n",
      "DONE episode = 37 number of steps = 135 reward = 0.0\n",
      "DONE episode = 38 number of steps = 135 reward = 0.0\n",
      "DONE episode = 39 number of steps = 208 reward = 2.0\n",
      "DONE episode = 40 number of steps = 241 reward = 2.0\n",
      "DONE episode = 41 number of steps = 147 reward = 0.0\n",
      "DONE episode = 42 number of steps = 130 reward = 0.0\n",
      "DONE episode = 43 number of steps = 337 reward = 6.0\n",
      "DONE episode = 44 number of steps = 275 reward = 3.0\n",
      "DONE episode = 45 number of steps = 322 reward = 4.0\n",
      "DONE episode = 46 number of steps = 271 reward = 4.0\n",
      "DONE episode = 47 number of steps = 163 reward = 1.0\n",
      "DONE episode = 48 number of steps = 178 reward = 1.0\n",
      "DONE episode = 49 number of steps = 177 reward = 1.0\n",
      "DONE episode = 50 number of steps = 214 reward = 2.0\n",
      "DONE episode = 51 number of steps = 131 reward = 0.0\n",
      "DONE episode = 52 number of steps = 277 reward = 3.0\n",
      "DONE episode = 53 number of steps = 232 reward = 2.0\n",
      "DONE episode = 54 number of steps = 166 reward = 1.0\n",
      "DONE episode = 55 number of steps = 132 reward = 0.0\n",
      "DONE episode = 56 number of steps = 139 reward = 0.0\n",
      "DONE episode = 57 number of steps = 354 reward = 5.0\n",
      "DONE episode = 58 number of steps = 161 reward = 1.0\n",
      "DONE episode = 59 number of steps = 162 reward = 1.0\n",
      "DONE episode = 60 number of steps = 126 reward = 0.0\n",
      "DONE episode = 61 number of steps = 213 reward = 2.0\n",
      "DONE episode = 62 number of steps = 188 reward = 1.0\n",
      "DONE episode = 63 number of steps = 144 reward = 0.0\n",
      "DONE episode = 64 number of steps = 189 reward = 1.0\n",
      "DONE episode = 65 number of steps = 132 reward = 0.0\n",
      "DONE episode = 66 number of steps = 130 reward = 0.0\n",
      "DONE episode = 67 number of steps = 135 reward = 0.0\n",
      "DONE episode = 68 number of steps = 164 reward = 1.0\n",
      "DONE episode = 69 number of steps = 179 reward = 1.0\n",
      "DONE episode = 70 number of steps = 157 reward = 0.0\n",
      "DONE episode = 71 number of steps = 131 reward = 0.0\n",
      "DONE episode = 72 number of steps = 207 reward = 2.0\n",
      "DONE episode = 73 number of steps = 229 reward = 2.0\n",
      "DONE episode = 74 number of steps = 158 reward = 1.0\n",
      "DONE episode = 75 number of steps = 130 reward = 0.0\n",
      "DONE episode = 76 number of steps = 175 reward = 1.0\n",
      "DONE episode = 77 number of steps = 271 reward = 3.0\n",
      "DONE episode = 78 number of steps = 128 reward = 0.0\n",
      "DONE episode = 79 number of steps = 225 reward = 2.0\n",
      "DONE episode = 80 number of steps = 142 reward = 0.0\n",
      "DONE episode = 81 number of steps = 197 reward = 2.0\n",
      "DONE episode = 82 number of steps = 207 reward = 2.0\n",
      "DONE episode = 83 number of steps = 226 reward = 2.0\n",
      "DONE episode = 84 number of steps = 164 reward = 1.0\n",
      "DONE episode = 85 number of steps = 156 reward = 1.0\n",
      "DONE episode = 86 number of steps = 262 reward = 3.0\n",
      "DONE episode = 87 number of steps = 320 reward = 7.0\n",
      "DONE episode = 88 number of steps = 163 reward = 1.0\n",
      "DONE episode = 89 number of steps = 170 reward = 1.0\n",
      "DONE episode = 90 number of steps = 305 reward = 4.0\n",
      "DONE episode = 91 number of steps = 312 reward = 5.0\n",
      "DONE episode = 92 number of steps = 147 reward = 0.0\n",
      "DONE episode = 93 number of steps = 167 reward = 1.0\n",
      "DONE episode = 94 number of steps = 188 reward = 1.0\n",
      "DONE episode = 95 number of steps = 186 reward = 1.0\n",
      "DONE episode = 96 number of steps = 245 reward = 3.0\n",
      "DONE episode = 97 number of steps = 136 reward = 0.0\n",
      "DONE episode = 98 number of steps = 294 reward = 3.0\n",
      "DONE episode = 99 number of steps = 132 reward = 0.0\n",
      "DONE episode = 100 number of steps = 160 reward = 1.0\n",
      "DONE episode = 101 number of steps = 206 reward = 2.0\n",
      "DONE episode = 102 number of steps = 230 reward = 2.0\n",
      "DONE episode = 103 number of steps = 204 reward = 2.0\n",
      "DONE episode = 104 number of steps = 173 reward = 1.0\n",
      "DONE episode = 105 number of steps = 192 reward = 1.0\n",
      "DONE episode = 106 number of steps = 266 reward = 3.0\n",
      "DONE episode = 107 number of steps = 157 reward = 1.0\n",
      "DONE episode = 108 number of steps = 136 reward = 0.0\n",
      "DONE episode = 109 number of steps = 162 reward = 1.0\n",
      "DONE episode = 110 number of steps = 182 reward = 1.0\n",
      "DONE episode = 111 number of steps = 479 reward = 10.0\n",
      "DONE episode = 112 number of steps = 207 reward = 2.0\n",
      "DONE episode = 113 number of steps = 273 reward = 4.0\n",
      "DONE episode = 114 number of steps = 175 reward = 1.0\n",
      "DONE episode = 115 number of steps = 185 reward = 1.0\n",
      "DONE episode = 116 number of steps = 199 reward = 2.0\n",
      "DONE episode = 117 number of steps = 129 reward = 0.0\n",
      "DONE episode = 118 number of steps = 270 reward = 3.0\n",
      "DONE episode = 119 number of steps = 190 reward = 1.0\n",
      "DONE episode = 120 number of steps = 307 reward = 4.0\n",
      "DONE episode = 121 number of steps = 135 reward = 0.0\n",
      "DONE episode = 122 number of steps = 182 reward = 2.0\n",
      "DONE episode = 123 number of steps = 341 reward = 7.0\n",
      "DONE episode = 124 number of steps = 156 reward = 1.0\n",
      "DONE episode = 125 number of steps = 221 reward = 2.0\n",
      "DONE episode = 126 number of steps = 245 reward = 3.0\n",
      "DONE episode = 127 number of steps = 265 reward = 3.0\n",
      "DONE episode = 128 number of steps = 227 reward = 2.0\n",
      "DONE episode = 129 number of steps = 207 reward = 2.0\n",
      "DONE episode = 130 number of steps = 288 reward = 3.0\n",
      "DONE episode = 131 number of steps = 165 reward = 1.0\n",
      "DONE episode = 132 number of steps = 153 reward = 1.0\n",
      "DONE episode = 133 number of steps = 164 reward = 1.0\n",
      "DONE episode = 134 number of steps = 235 reward = 3.0\n",
      "DONE episode = 135 number of steps = 243 reward = 3.0\n",
      "DONE episode = 136 number of steps = 158 reward = 1.0\n",
      "DONE episode = 137 number of steps = 128 reward = 0.0\n",
      "DONE episode = 138 number of steps = 220 reward = 2.0\n",
      "DONE episode = 139 number of steps = 155 reward = 1.0\n",
      "DONE episode = 140 number of steps = 127 reward = 0.0\n",
      "DONE episode = 141 number of steps = 166 reward = 1.0\n",
      "DONE episode = 142 number of steps = 314 reward = 5.0\n",
      "DONE episode = 143 number of steps = 128 reward = 0.0\n",
      "DONE episode = 144 number of steps = 130 reward = 0.0\n",
      "DONE episode = 145 number of steps = 170 reward = 1.0\n",
      "DONE episode = 146 number of steps = 125 reward = 0.0\n",
      "DONE episode = 147 number of steps = 177 reward = 1.0\n",
      "DONE episode = 148 number of steps = 175 reward = 1.0\n",
      "DONE episode = 149 number of steps = 247 reward = 3.0\n",
      "DONE episode = 150 number of steps = 161 reward = 1.0\n",
      "DONE episode = 151 number of steps = 224 reward = 2.0\n",
      "DONE episode = 152 number of steps = 159 reward = 1.0\n",
      "DONE episode = 153 number of steps = 182 reward = 1.0\n",
      "DONE episode = 154 number of steps = 160 reward = 1.0\n",
      "DONE episode = 155 number of steps = 166 reward = 1.0\n",
      "DONE episode = 156 number of steps = 240 reward = 2.0\n",
      "DONE episode = 157 number of steps = 143 reward = 0.0\n",
      "DONE episode = 158 number of steps = 238 reward = 3.0\n",
      "DONE episode = 159 number of steps = 204 reward = 2.0\n",
      "DONE episode = 160 number of steps = 152 reward = 1.0\n",
      "DONE episode = 161 number of steps = 209 reward = 2.0\n",
      "DONE episode = 162 number of steps = 189 reward = 1.0\n",
      "DONE episode = 163 number of steps = 128 reward = 0.0\n",
      "DONE episode = 164 number of steps = 230 reward = 2.0\n",
      "DONE episode = 165 number of steps = 133 reward = 0.0\n",
      "DONE episode = 166 number of steps = 133 reward = 0.0\n",
      "DONE episode = 167 number of steps = 166 reward = 1.0\n",
      "DONE episode = 168 number of steps = 277 reward = 6.0\n",
      "DONE episode = 169 number of steps = 284 reward = 3.0\n",
      "DONE episode = 170 number of steps = 167 reward = 1.0\n",
      "DONE episode = 171 number of steps = 187 reward = 1.0\n",
      "DONE episode = 172 number of steps = 167 reward = 1.0\n",
      "DONE episode = 173 number of steps = 156 reward = 1.0\n",
      "DONE episode = 174 number of steps = 138 reward = 0.0\n",
      "DONE episode = 175 number of steps = 133 reward = 0.0\n",
      "DONE episode = 176 number of steps = 130 reward = 0.0\n",
      "DONE episode = 177 number of steps = 134 reward = 0.0\n",
      "DONE episode = 178 number of steps = 128 reward = 0.0\n",
      "DONE episode = 179 number of steps = 185 reward = 1.0\n",
      "DONE episode = 180 number of steps = 176 reward = 1.0\n",
      "DONE episode = 181 number of steps = 159 reward = 1.0\n",
      "DONE episode = 182 number of steps = 137 reward = 0.0\n",
      "DONE episode = 183 number of steps = 179 reward = 1.0\n",
      "DONE episode = 184 number of steps = 129 reward = 0.0\n",
      "DONE episode = 185 number of steps = 127 reward = 0.0\n",
      "DONE episode = 186 number of steps = 170 reward = 1.0\n",
      "DONE episode = 187 number of steps = 176 reward = 1.0\n",
      "DONE episode = 188 number of steps = 155 reward = 1.0\n",
      "DONE episode = 189 number of steps = 151 reward = 0.0\n",
      "DONE episode = 190 number of steps = 200 reward = 2.0\n",
      "DONE episode = 191 number of steps = 245 reward = 3.0\n",
      "DONE episode = 192 number of steps = 182 reward = 2.0\n",
      "DONE episode = 193 number of steps = 278 reward = 3.0\n",
      "DONE episode = 194 number of steps = 217 reward = 2.0\n",
      "DONE episode = 195 number of steps = 130 reward = 0.0\n",
      "DONE episode = 196 number of steps = 174 reward = 1.0\n",
      "DONE episode = 197 number of steps = 177 reward = 1.0\n",
      "DONE episode = 198 number of steps = 172 reward = 1.0\n",
      "DONE episode = 199 number of steps = 136 reward = 0.0\n",
      "DONE episode = 200 number of steps = 305 reward = 4.0\n",
      "DONE episode = 201 number of steps = 205 reward = 2.0\n",
      "DONE episode = 202 number of steps = 234 reward = 3.0\n",
      "DONE episode = 203 number of steps = 169 reward = 1.0\n",
      "DONE episode = 204 number of steps = 130 reward = 0.0\n",
      "DONE episode = 205 number of steps = 194 reward = 2.0\n",
      "DONE episode = 206 number of steps = 317 reward = 4.0\n",
      "DONE episode = 207 number of steps = 259 reward = 3.0\n",
      "DONE episode = 208 number of steps = 188 reward = 2.0\n",
      "DONE episode = 209 number of steps = 155 reward = 1.0\n",
      "DONE episode = 210 number of steps = 160 reward = 1.0\n",
      "DONE episode = 211 number of steps = 281 reward = 4.0\n",
      "DONE episode = 212 number of steps = 281 reward = 4.0\n",
      "DONE episode = 213 number of steps = 163 reward = 1.0\n",
      "DONE episode = 214 number of steps = 325 reward = 4.0\n",
      "DONE episode = 215 number of steps = 232 reward = 3.0\n",
      "DONE episode = 216 number of steps = 133 reward = 0.0\n",
      "DONE episode = 217 number of steps = 264 reward = 3.0\n",
      "DONE episode = 218 number of steps = 213 reward = 2.0\n",
      "DONE episode = 219 number of steps = 156 reward = 1.0\n",
      "DONE episode = 220 number of steps = 398 reward = 7.0\n",
      "DONE episode = 221 number of steps = 142 reward = 0.0\n",
      "DONE episode = 222 number of steps = 196 reward = 2.0\n",
      "DONE episode = 223 number of steps = 131 reward = 0.0\n",
      "DONE episode = 224 number of steps = 133 reward = 0.0\n",
      "DONE episode = 225 number of steps = 180 reward = 1.0\n",
      "DONE episode = 226 number of steps = 228 reward = 2.0\n",
      "DONE episode = 227 number of steps = 209 reward = 2.0\n",
      "DONE episode = 228 number of steps = 151 reward = 1.0\n",
      "DONE episode = 229 number of steps = 168 reward = 1.0\n",
      "DONE episode = 230 number of steps = 132 reward = 0.0\n",
      "DONE episode = 231 number of steps = 132 reward = 0.0\n",
      "DONE episode = 232 number of steps = 184 reward = 1.0\n",
      "DONE episode = 233 number of steps = 219 reward = 2.0\n",
      "DONE episode = 234 number of steps = 137 reward = 0.0\n",
      "DONE episode = 235 number of steps = 258 reward = 3.0\n",
      "DONE episode = 236 number of steps = 183 reward = 1.0\n",
      "DONE episode = 237 number of steps = 172 reward = 1.0\n",
      "DONE episode = 238 number of steps = 174 reward = 1.0\n",
      "DONE episode = 239 number of steps = 405 reward = 6.0\n",
      "DONE episode = 240 number of steps = 171 reward = 1.0\n",
      "DONE episode = 241 number of steps = 227 reward = 2.0\n",
      "DONE episode = 242 number of steps = 273 reward = 3.0\n",
      "DONE episode = 243 number of steps = 180 reward = 1.0\n",
      "DONE episode = 244 number of steps = 187 reward = 2.0\n",
      "DONE episode = 245 number of steps = 211 reward = 2.0\n",
      "DONE episode = 246 number of steps = 352 reward = 4.0\n",
      "DONE episode = 247 number of steps = 158 reward = 1.0\n",
      "DONE episode = 248 number of steps = 223 reward = 2.0\n",
      "DONE episode = 249 number of steps = 244 reward = 2.0\n",
      "DONE episode = 250 number of steps = 146 reward = 0.0\n",
      "DONE episode = 251 number of steps = 154 reward = 1.0\n",
      "DONE episode = 252 number of steps = 158 reward = 1.0\n",
      "DONE episode = 253 number of steps = 257 reward = 3.0\n",
      "DONE episode = 254 number of steps = 215 reward = 3.0\n",
      "DONE episode = 255 number of steps = 129 reward = 0.0\n",
      "DONE episode = 256 number of steps = 205 reward = 2.0\n",
      "DONE episode = 257 number of steps = 127 reward = 0.0\n",
      "DONE episode = 258 number of steps = 221 reward = 2.0\n",
      "DONE episode = 259 number of steps = 235 reward = 3.0\n",
      "DONE episode = 260 number of steps = 305 reward = 4.0\n",
      "DONE episode = 261 number of steps = 156 reward = 1.0\n",
      "DONE episode = 262 number of steps = 238 reward = 3.0\n",
      "DONE episode = 263 number of steps = 160 reward = 1.0\n",
      "DONE episode = 264 number of steps = 125 reward = 0.0\n",
      "DONE episode = 265 number of steps = 137 reward = 0.0\n",
      "DONE episode = 266 number of steps = 331 reward = 4.0\n",
      "DONE episode = 267 number of steps = 231 reward = 2.0\n",
      "DONE episode = 268 number of steps = 180 reward = 1.0\n",
      "DONE episode = 269 number of steps = 230 reward = 3.0\n",
      "DONE episode = 270 number of steps = 308 reward = 4.0\n",
      "DONE episode = 271 number of steps = 181 reward = 1.0\n",
      "DONE episode = 272 number of steps = 289 reward = 4.0\n",
      "DONE episode = 273 number of steps = 270 reward = 4.0\n",
      "DONE episode = 274 number of steps = 275 reward = 3.0\n",
      "DONE episode = 275 number of steps = 267 reward = 3.0\n",
      "DONE episode = 276 number of steps = 156 reward = 1.0\n",
      "DONE episode = 277 number of steps = 219 reward = 2.0\n",
      "DONE episode = 278 number of steps = 187 reward = 2.0\n",
      "DONE episode = 279 number of steps = 252 reward = 3.0\n",
      "DONE episode = 280 number of steps = 136 reward = 0.0\n",
      "DONE episode = 281 number of steps = 193 reward = 2.0\n",
      "DONE episode = 282 number of steps = 131 reward = 0.0\n",
      "DONE episode = 283 number of steps = 144 reward = 0.0\n",
      "DONE episode = 284 number of steps = 180 reward = 2.0\n",
      "DONE episode = 285 number of steps = 176 reward = 1.0\n",
      "DONE episode = 286 number of steps = 245 reward = 3.0\n",
      "DONE episode = 287 number of steps = 174 reward = 1.0\n",
      "DONE episode = 288 number of steps = 127 reward = 0.0\n",
      "DONE episode = 289 number of steps = 156 reward = 1.0\n",
      "DONE episode = 290 number of steps = 266 reward = 3.0\n",
      "DONE episode = 291 number of steps = 130 reward = 0.0\n",
      "DONE episode = 292 number of steps = 222 reward = 2.0\n",
      "DONE episode = 293 number of steps = 230 reward = 3.0\n",
      "DONE episode = 294 number of steps = 153 reward = 1.0\n",
      "DONE episode = 295 number of steps = 240 reward = 3.0\n",
      "DONE episode = 296 number of steps = 128 reward = 0.0\n",
      "DONE episode = 297 number of steps = 187 reward = 2.0\n",
      "DONE episode = 298 number of steps = 129 reward = 0.0\n",
      "DONE episode = 299 number of steps = 264 reward = 4.0\n",
      "DONE episode = 300 number of steps = 316 reward = 5.0\n",
      "DONE episode = 301 number of steps = 132 reward = 0.0\n",
      "DONE episode = 302 number of steps = 158 reward = 1.0\n",
      "DONE episode = 303 number of steps = 180 reward = 1.0\n",
      "DONE episode = 304 number of steps = 160 reward = 1.0\n",
      "DONE episode = 305 number of steps = 314 reward = 5.0\n",
      "DONE episode = 306 number of steps = 134 reward = 0.0\n",
      "DONE episode = 307 number of steps = 204 reward = 2.0\n",
      "DONE episode = 308 number of steps = 135 reward = 0.0\n",
      "DONE episode = 309 number of steps = 133 reward = 0.0\n",
      "DONE episode = 310 number of steps = 280 reward = 3.0\n",
      "DONE episode = 311 number of steps = 134 reward = 0.0\n",
      "DONE episode = 312 number of steps = 213 reward = 2.0\n",
      "DONE episode = 313 number of steps = 144 reward = 0.0\n",
      "DONE episode = 314 number of steps = 204 reward = 2.0\n",
      "DONE episode = 315 number of steps = 127 reward = 0.0\n",
      "DONE episode = 316 number of steps = 339 reward = 5.0\n",
      "DONE episode = 317 number of steps = 139 reward = 0.0\n",
      "DONE episode = 318 number of steps = 293 reward = 4.0\n",
      "DONE episode = 319 number of steps = 183 reward = 1.0\n",
      "DONE episode = 320 number of steps = 254 reward = 3.0\n",
      "DONE episode = 321 number of steps = 125 reward = 0.0\n",
      "DONE episode = 322 number of steps = 347 reward = 5.0\n",
      "DONE episode = 323 number of steps = 350 reward = 8.0\n",
      "DONE episode = 324 number of steps = 225 reward = 2.0\n",
      "DONE episode = 325 number of steps = 156 reward = 1.0\n",
      "DONE episode = 326 number of steps = 153 reward = 1.0\n",
      "DONE episode = 327 number of steps = 206 reward = 2.0\n",
      "DONE episode = 328 number of steps = 202 reward = 2.0\n",
      "DONE episode = 329 number of steps = 174 reward = 1.0\n",
      "DONE episode = 330 number of steps = 220 reward = 2.0\n",
      "DONE episode = 331 number of steps = 349 reward = 8.0\n",
      "DONE episode = 332 number of steps = 154 reward = 1.0\n",
      "DONE episode = 333 number of steps = 159 reward = 1.0\n",
      "DONE episode = 334 number of steps = 151 reward = 1.0\n",
      "DONE episode = 335 number of steps = 127 reward = 0.0\n",
      "DONE episode = 336 number of steps = 236 reward = 3.0\n",
      "DONE episode = 337 number of steps = 170 reward = 1.0\n",
      "DONE episode = 338 number of steps = 219 reward = 2.0\n",
      "DONE episode = 339 number of steps = 183 reward = 1.0\n",
      "DONE episode = 340 number of steps = 256 reward = 3.0\n",
      "DONE episode = 341 number of steps = 178 reward = 1.0\n",
      "DONE episode = 342 number of steps = 273 reward = 4.0\n",
      "DONE episode = 343 number of steps = 214 reward = 3.0\n",
      "DONE episode = 344 number of steps = 126 reward = 0.0\n",
      "DONE episode = 345 number of steps = 240 reward = 3.0\n",
      "DONE episode = 346 number of steps = 232 reward = 3.0\n",
      "DONE episode = 347 number of steps = 181 reward = 1.0\n",
      "DONE episode = 348 number of steps = 176 reward = 1.0\n",
      "DONE episode = 349 number of steps = 155 reward = 1.0\n",
      "DONE episode = 350 number of steps = 308 reward = 4.0\n",
      "DONE episode = 351 number of steps = 307 reward = 7.0\n",
      "DONE episode = 352 number of steps = 235 reward = 2.0\n",
      "DONE episode = 353 number of steps = 130 reward = 0.0\n",
      "DONE episode = 354 number of steps = 256 reward = 3.0\n",
      "DONE episode = 355 number of steps = 206 reward = 2.0\n",
      "DONE episode = 356 number of steps = 308 reward = 4.0\n",
      "DONE episode = 357 number of steps = 253 reward = 3.0\n",
      "DONE episode = 358 number of steps = 134 reward = 0.0\n",
      "DONE episode = 359 number of steps = 163 reward = 1.0\n",
      "DONE episode = 360 number of steps = 220 reward = 2.0\n",
      "DONE episode = 361 number of steps = 275 reward = 3.0\n",
      "DONE episode = 362 number of steps = 184 reward = 1.0\n",
      "DONE episode = 363 number of steps = 232 reward = 3.0\n",
      "DONE episode = 364 number of steps = 235 reward = 2.0\n",
      "DONE episode = 365 number of steps = 259 reward = 3.0\n",
      "DONE episode = 366 number of steps = 249 reward = 3.0\n",
      "DONE episode = 367 number of steps = 130 reward = 0.0\n",
      "DONE episode = 368 number of steps = 146 reward = 0.0\n",
      "DONE episode = 369 number of steps = 325 reward = 5.0\n",
      "DONE episode = 370 number of steps = 209 reward = 2.0\n",
      "DONE episode = 371 number of steps = 170 reward = 1.0\n",
      "DONE episode = 372 number of steps = 175 reward = 1.0\n",
      "DONE episode = 373 number of steps = 193 reward = 2.0\n",
      "DONE episode = 374 number of steps = 258 reward = 3.0\n",
      "DONE episode = 375 number of steps = 303 reward = 4.0\n",
      "DONE episode = 376 number of steps = 252 reward = 3.0\n",
      "DONE episode = 377 number of steps = 171 reward = 1.0\n",
      "DONE episode = 378 number of steps = 265 reward = 3.0\n",
      "DONE episode = 379 number of steps = 411 reward = 6.0\n",
      "DONE episode = 380 number of steps = 154 reward = 1.0\n",
      "DONE episode = 381 number of steps = 141 reward = 0.0\n",
      "DONE episode = 382 number of steps = 200 reward = 2.0\n",
      "DONE episode = 383 number of steps = 256 reward = 3.0\n",
      "DONE episode = 384 number of steps = 193 reward = 2.0\n",
      "DONE episode = 385 number of steps = 300 reward = 5.0\n",
      "DONE episode = 386 number of steps = 132 reward = 0.0\n",
      "DONE episode = 387 number of steps = 382 reward = 6.0\n",
      "DONE episode = 388 number of steps = 171 reward = 1.0\n",
      "DONE episode = 389 number of steps = 257 reward = 3.0\n",
      "DONE episode = 390 number of steps = 126 reward = 0.0\n",
      "DONE episode = 391 number of steps = 126 reward = 0.0\n",
      "DONE episode = 392 number of steps = 240 reward = 2.0\n",
      "DONE episode = 393 number of steps = 134 reward = 0.0\n",
      "DONE episode = 394 number of steps = 227 reward = 2.0\n",
      "DONE episode = 395 number of steps = 174 reward = 1.0\n",
      "DONE episode = 396 number of steps = 267 reward = 3.0\n",
      "DONE episode = 397 number of steps = 234 reward = 3.0\n",
      "DONE episode = 398 number of steps = 269 reward = 4.0\n",
      "DONE episode = 399 number of steps = 252 reward = 3.0\n",
      "DONE episode = 400 number of steps = 262 reward = 4.0\n",
      "DONE episode = 401 number of steps = 127 reward = 0.0\n",
      "DONE episode = 402 number of steps = 181 reward = 1.0\n",
      "DONE episode = 403 number of steps = 281 reward = 4.0\n",
      "DONE episode = 404 number of steps = 169 reward = 1.0\n",
      "DONE episode = 405 number of steps = 125 reward = 0.0\n",
      "DONE episode = 406 number of steps = 193 reward = 2.0\n",
      "DONE episode = 407 number of steps = 208 reward = 2.0\n",
      "DONE episode = 408 number of steps = 179 reward = 1.0\n",
      "DONE episode = 409 number of steps = 255 reward = 3.0\n",
      "DONE episode = 410 number of steps = 221 reward = 3.0\n",
      "DONE episode = 411 number of steps = 162 reward = 1.0\n",
      "DONE episode = 412 number of steps = 156 reward = 1.0\n",
      "DONE episode = 413 number of steps = 212 reward = 2.0\n",
      "DONE episode = 414 number of steps = 217 reward = 3.0\n",
      "DONE episode = 415 number of steps = 178 reward = 1.0\n",
      "DONE episode = 416 number of steps = 300 reward = 4.0\n",
      "DONE episode = 417 number of steps = 205 reward = 2.0\n",
      "DONE episode = 418 number of steps = 181 reward = 1.0\n",
      "DONE episode = 419 number of steps = 171 reward = 1.0\n",
      "DONE episode = 420 number of steps = 183 reward = 2.0\n",
      "DONE episode = 421 number of steps = 257 reward = 3.0\n",
      "DONE episode = 422 number of steps = 267 reward = 4.0\n",
      "DONE episode = 423 number of steps = 157 reward = 1.0\n",
      "DONE episode = 424 number of steps = 158 reward = 1.0\n",
      "DONE episode = 425 number of steps = 213 reward = 2.0\n",
      "DONE episode = 426 number of steps = 287 reward = 4.0\n",
      "DONE episode = 427 number of steps = 237 reward = 3.0\n",
      "DONE episode = 428 number of steps = 188 reward = 2.0\n",
      "DONE episode = 429 number of steps = 132 reward = 0.0\n",
      "DONE episode = 430 number of steps = 190 reward = 2.0\n",
      "DONE episode = 431 number of steps = 167 reward = 1.0\n",
      "DONE episode = 432 number of steps = 213 reward = 2.0\n",
      "DONE episode = 433 number of steps = 276 reward = 3.0\n",
      "DONE episode = 434 number of steps = 284 reward = 5.0\n",
      "DONE episode = 435 number of steps = 155 reward = 1.0\n",
      "DONE episode = 436 number of steps = 261 reward = 4.0\n",
      "DONE episode = 437 number of steps = 271 reward = 4.0\n",
      "DONE episode = 438 number of steps = 171 reward = 1.0\n",
      "DONE episode = 439 number of steps = 156 reward = 1.0\n",
      "DONE episode = 440 number of steps = 300 reward = 5.0\n",
      "DONE episode = 441 number of steps = 248 reward = 3.0\n",
      "DONE episode = 442 number of steps = 270 reward = 4.0\n",
      "DONE episode = 443 number of steps = 186 reward = 2.0\n",
      "DONE episode = 444 number of steps = 203 reward = 2.0\n",
      "DONE episode = 445 number of steps = 220 reward = 3.0\n",
      "DONE episode = 446 number of steps = 188 reward = 2.0\n",
      "DONE episode = 447 number of steps = 177 reward = 1.0\n",
      "DONE episode = 448 number of steps = 265 reward = 3.0\n",
      "DONE episode = 449 number of steps = 278 reward = 3.0\n",
      "DONE episode = 450 number of steps = 225 reward = 2.0\n",
      "DONE episode = 451 number of steps = 316 reward = 5.0\n",
      "DONE episode = 452 number of steps = 208 reward = 2.0\n",
      "DONE episode = 453 number of steps = 203 reward = 2.0\n",
      "DONE episode = 454 number of steps = 421 reward = 7.0\n",
      "DONE episode = 455 number of steps = 255 reward = 3.0\n",
      "DONE episode = 456 number of steps = 175 reward = 1.0\n",
      "DONE episode = 457 number of steps = 278 reward = 4.0\n",
      "DONE episode = 458 number of steps = 425 reward = 7.0\n",
      "DONE episode = 459 number of steps = 231 reward = 3.0\n",
      "DONE episode = 460 number of steps = 174 reward = 1.0\n",
      "DONE episode = 461 number of steps = 293 reward = 4.0\n",
      "DONE episode = 462 number of steps = 310 reward = 5.0\n",
      "DONE episode = 463 number of steps = 350 reward = 5.0\n",
      "DONE episode = 464 number of steps = 184 reward = 2.0\n",
      "DONE episode = 465 number of steps = 185 reward = 1.0\n",
      "DONE episode = 466 number of steps = 165 reward = 1.0\n",
      "DONE episode = 467 number of steps = 186 reward = 2.0\n",
      "DONE episode = 468 number of steps = 300 reward = 5.0\n",
      "DONE episode = 469 number of steps = 250 reward = 3.0\n",
      "DONE episode = 470 number of steps = 152 reward = 1.0\n",
      "DONE episode = 471 number of steps = 233 reward = 3.0\n",
      "DONE episode = 472 number of steps = 279 reward = 4.0\n",
      "DONE episode = 473 number of steps = 160 reward = 1.0\n",
      "DONE episode = 474 number of steps = 183 reward = 2.0\n",
      "DONE episode = 475 number of steps = 190 reward = 2.0\n",
      "DONE episode = 476 number of steps = 156 reward = 1.0\n",
      "DONE episode = 477 number of steps = 305 reward = 4.0\n",
      "DONE episode = 478 number of steps = 255 reward = 3.0\n",
      "DONE episode = 479 number of steps = 247 reward = 3.0\n",
      "DONE episode = 480 number of steps = 176 reward = 1.0\n",
      "DONE episode = 481 number of steps = 301 reward = 4.0\n",
      "DONE episode = 482 number of steps = 231 reward = 3.0\n",
      "DONE episode = 483 number of steps = 297 reward = 4.0\n",
      "DONE episode = 484 number of steps = 299 reward = 4.0\n",
      "DONE episode = 485 number of steps = 127 reward = 0.0\n",
      "DONE episode = 486 number of steps = 265 reward = 3.0\n",
      "DONE episode = 487 number of steps = 230 reward = 3.0\n",
      "DONE episode = 488 number of steps = 172 reward = 1.0\n",
      "DONE episode = 489 number of steps = 249 reward = 3.0\n",
      "DONE episode = 490 number of steps = 208 reward = 2.0\n",
      "DONE episode = 491 number of steps = 256 reward = 3.0\n",
      "DONE episode = 492 number of steps = 173 reward = 1.0\n",
      "DONE episode = 493 number of steps = 157 reward = 1.0\n",
      "DONE episode = 494 number of steps = 133 reward = 0.0\n",
      "DONE episode = 495 number of steps = 275 reward = 4.0\n",
      "DONE episode = 496 number of steps = 224 reward = 2.0\n",
      "DONE episode = 497 number of steps = 208 reward = 2.0\n",
      "DONE episode = 498 number of steps = 226 reward = 3.0\n",
      "DONE episode = 499 number of steps = 156 reward = 1.0\n",
      "DONE episode = 500 number of steps = 235 reward = 3.0\n",
      "DONE episode = 501 number of steps = 306 reward = 4.0\n",
      "DONE episode = 502 number of steps = 263 reward = 3.0\n",
      "DONE episode = 503 number of steps = 249 reward = 3.0\n",
      "DONE episode = 504 number of steps = 151 reward = 1.0\n",
      "DONE episode = 505 number of steps = 229 reward = 2.0\n",
      "DONE episode = 506 number of steps = 248 reward = 3.0\n",
      "DONE episode = 507 number of steps = 153 reward = 1.0\n",
      "DONE episode = 508 number of steps = 331 reward = 5.0\n",
      "DONE episode = 509 number of steps = 181 reward = 1.0\n",
      "DONE episode = 510 number of steps = 266 reward = 4.0\n",
      "DONE episode = 511 number of steps = 354 reward = 8.0\n",
      "DONE episode = 512 number of steps = 125 reward = 0.0\n",
      "DONE episode = 513 number of steps = 226 reward = 2.0\n",
      "DONE episode = 514 number of steps = 196 reward = 2.0\n",
      "DONE episode = 515 number of steps = 285 reward = 3.0\n",
      "DONE episode = 516 number of steps = 139 reward = 0.0\n",
      "DONE episode = 517 number of steps = 155 reward = 1.0\n",
      "DONE episode = 518 number of steps = 200 reward = 2.0\n",
      "DONE episode = 519 number of steps = 184 reward = 2.0\n",
      "DONE episode = 520 number of steps = 269 reward = 4.0\n",
      "DONE episode = 521 number of steps = 123 reward = 0.0\n",
      "DONE episode = 522 number of steps = 166 reward = 1.0\n",
      "DONE episode = 523 number of steps = 327 reward = 5.0\n",
      "DONE episode = 524 number of steps = 204 reward = 2.0\n",
      "DONE episode = 525 number of steps = 124 reward = 0.0\n",
      "DONE episode = 526 number of steps = 251 reward = 3.0\n",
      "DONE episode = 527 number of steps = 126 reward = 0.0\n",
      "DONE episode = 528 number of steps = 230 reward = 3.0\n",
      "DONE episode = 529 number of steps = 124 reward = 0.0\n",
      "DONE episode = 530 number of steps = 172 reward = 1.0\n",
      "DONE episode = 531 number of steps = 154 reward = 1.0\n",
      "DONE episode = 532 number of steps = 273 reward = 3.0\n",
      "DONE episode = 533 number of steps = 162 reward = 1.0\n",
      "DONE episode = 534 number of steps = 248 reward = 3.0\n",
      "DONE episode = 535 number of steps = 155 reward = 1.0\n",
      "DONE episode = 536 number of steps = 160 reward = 1.0\n",
      "DONE episode = 537 number of steps = 253 reward = 3.0\n",
      "DONE episode = 538 number of steps = 179 reward = 1.0\n",
      "DONE episode = 539 number of steps = 279 reward = 4.0\n",
      "DONE episode = 540 number of steps = 171 reward = 1.0\n",
      "DONE episode = 541 number of steps = 205 reward = 2.0\n",
      "DONE episode = 542 number of steps = 210 reward = 3.0\n",
      "DONE episode = 543 number of steps = 216 reward = 3.0\n",
      "DONE episode = 544 number of steps = 174 reward = 1.0\n",
      "DONE episode = 545 number of steps = 155 reward = 1.0\n",
      "DONE episode = 546 number of steps = 310 reward = 5.0\n",
      "DONE episode = 547 number of steps = 327 reward = 5.0\n",
      "DONE episode = 548 number of steps = 161 reward = 1.0\n",
      "DONE episode = 549 number of steps = 205 reward = 2.0\n",
      "DONE episode = 550 number of steps = 293 reward = 5.0\n",
      "DONE episode = 551 number of steps = 181 reward = 2.0\n",
      "DONE episode = 552 number of steps = 321 reward = 4.0\n",
      "DONE episode = 553 number of steps = 296 reward = 4.0\n",
      "DONE episode = 554 number of steps = 221 reward = 2.0\n",
      "DONE episode = 555 number of steps = 202 reward = 2.0\n",
      "DONE episode = 556 number of steps = 252 reward = 3.0\n",
      "DONE episode = 557 number of steps = 316 reward = 4.0\n",
      "DONE episode = 558 number of steps = 328 reward = 4.0\n",
      "DONE episode = 559 number of steps = 310 reward = 4.0\n",
      "DONE episode = 560 number of steps = 186 reward = 1.0\n",
      "DONE episode = 561 number of steps = 398 reward = 6.0\n",
      "DONE episode = 562 number of steps = 214 reward = 2.0\n",
      "DONE episode = 563 number of steps = 134 reward = 0.0\n",
      "DONE episode = 564 number of steps = 240 reward = 3.0\n",
      "DONE episode = 565 number of steps = 198 reward = 2.0\n",
      "DONE episode = 566 number of steps = 307 reward = 5.0\n",
      "DONE episode = 567 number of steps = 378 reward = 6.0\n",
      "DONE episode = 568 number of steps = 160 reward = 1.0\n",
      "DONE episode = 569 number of steps = 183 reward = 1.0\n",
      "DONE episode = 570 number of steps = 259 reward = 3.0\n",
      "DONE episode = 571 number of steps = 305 reward = 4.0\n",
      "DONE episode = 572 number of steps = 202 reward = 2.0\n",
      "DONE episode = 573 number of steps = 353 reward = 6.0\n",
      "DONE episode = 574 number of steps = 298 reward = 4.0\n",
      "DONE episode = 575 number of steps = 152 reward = 1.0\n",
      "DONE episode = 576 number of steps = 202 reward = 2.0\n",
      "DONE episode = 577 number of steps = 193 reward = 1.0\n",
      "DONE episode = 578 number of steps = 221 reward = 3.0\n",
      "DONE episode = 579 number of steps = 284 reward = 4.0\n",
      "DONE episode = 580 number of steps = 299 reward = 4.0\n",
      "DONE episode = 581 number of steps = 250 reward = 3.0\n",
      "DONE episode = 582 number of steps = 253 reward = 3.0\n",
      "DONE episode = 583 number of steps = 253 reward = 3.0\n",
      "DONE episode = 584 number of steps = 223 reward = 2.0\n",
      "DONE episode = 585 number of steps = 221 reward = 2.0\n",
      "DONE episode = 586 number of steps = 186 reward = 1.0\n",
      "DONE episode = 587 number of steps = 252 reward = 3.0\n",
      "DONE episode = 588 number of steps = 443 reward = 8.0\n",
      "DONE episode = 589 number of steps = 265 reward = 4.0\n",
      "DONE episode = 590 number of steps = 212 reward = 2.0\n",
      "DONE episode = 591 number of steps = 157 reward = 1.0\n",
      "DONE episode = 592 number of steps = 253 reward = 3.0\n",
      "DONE episode = 593 number of steps = 204 reward = 2.0\n",
      "DONE episode = 594 number of steps = 286 reward = 4.0\n",
      "DONE episode = 595 number of steps = 198 reward = 2.0\n",
      "DONE episode = 596 number of steps = 156 reward = 1.0\n",
      "DONE episode = 597 number of steps = 319 reward = 5.0\n",
      "DONE episode = 598 number of steps = 204 reward = 2.0\n",
      "DONE episode = 599 number of steps = 154 reward = 1.0\n",
      "DONE episode = 600 number of steps = 212 reward = 3.0\n",
      "DONE episode = 601 number of steps = 232 reward = 3.0\n",
      "DONE episode = 602 number of steps = 257 reward = 3.0\n",
      "DONE episode = 603 number of steps = 127 reward = 0.0\n",
      "DONE episode = 604 number of steps = 187 reward = 2.0\n",
      "DONE episode = 605 number of steps = 187 reward = 2.0\n",
      "DONE episode = 606 number of steps = 153 reward = 1.0\n",
      "DONE episode = 607 number of steps = 155 reward = 1.0\n",
      "DONE episode = 608 number of steps = 172 reward = 1.0\n",
      "DONE episode = 609 number of steps = 217 reward = 3.0\n",
      "DONE episode = 610 number of steps = 352 reward = 5.0\n",
      "DONE episode = 611 number of steps = 303 reward = 4.0\n",
      "DONE episode = 612 number of steps = 154 reward = 1.0\n",
      "DONE episode = 613 number of steps = 201 reward = 2.0\n",
      "DONE episode = 614 number of steps = 204 reward = 2.0\n",
      "DONE episode = 615 number of steps = 236 reward = 3.0\n",
      "DONE episode = 616 number of steps = 264 reward = 4.0\n",
      "DONE episode = 617 number of steps = 299 reward = 4.0\n",
      "DONE episode = 618 number of steps = 154 reward = 1.0\n",
      "DONE episode = 619 number of steps = 393 reward = 6.0\n",
      "DONE episode = 620 number of steps = 236 reward = 3.0\n",
      "DONE episode = 621 number of steps = 298 reward = 4.0\n",
      "DONE episode = 622 number of steps = 342 reward = 5.0\n",
      "DONE episode = 623 number of steps = 174 reward = 1.0\n",
      "DONE episode = 624 number of steps = 189 reward = 2.0\n",
      "DONE episode = 625 number of steps = 177 reward = 1.0\n",
      "DONE episode = 626 number of steps = 208 reward = 2.0\n",
      "DONE episode = 627 number of steps = 453 reward = 8.0\n",
      "DONE episode = 628 number of steps = 284 reward = 4.0\n",
      "DONE episode = 629 number of steps = 156 reward = 1.0\n",
      "DONE episode = 630 number of steps = 236 reward = 3.0\n",
      "DONE episode = 631 number of steps = 352 reward = 8.0\n",
      "DONE episode = 632 number of steps = 251 reward = 3.0\n",
      "DONE episode = 633 number of steps = 346 reward = 5.0\n",
      "DONE episode = 634 number of steps = 283 reward = 4.0\n",
      "DONE episode = 635 number of steps = 300 reward = 4.0\n",
      "DONE episode = 636 number of steps = 176 reward = 1.0\n",
      "DONE episode = 637 number of steps = 242 reward = 3.0\n",
      "DONE episode = 638 number of steps = 250 reward = 3.0\n",
      "DONE episode = 639 number of steps = 307 reward = 4.0\n",
      "DONE episode = 640 number of steps = 261 reward = 3.0\n",
      "DONE episode = 641 number of steps = 252 reward = 3.0\n",
      "DONE episode = 642 number of steps = 218 reward = 3.0\n",
      "DONE episode = 643 number of steps = 162 reward = 1.0\n",
      "DONE episode = 644 number of steps = 267 reward = 4.0\n",
      "DONE episode = 645 number of steps = 192 reward = 2.0\n",
      "DONE episode = 646 number of steps = 211 reward = 3.0\n",
      "DONE episode = 647 number of steps = 358 reward = 5.0\n",
      "DONE episode = 648 number of steps = 208 reward = 2.0\n",
      "DONE episode = 649 number of steps = 327 reward = 5.0\n",
      "DONE episode = 650 number of steps = 253 reward = 3.0\n",
      "DONE episode = 651 number of steps = 202 reward = 2.0\n",
      "DONE episode = 652 number of steps = 202 reward = 2.0\n",
      "DONE episode = 653 number of steps = 179 reward = 1.0\n",
      "DONE episode = 654 number of steps = 172 reward = 1.0\n",
      "DONE episode = 655 number of steps = 259 reward = 3.0\n",
      "DONE episode = 656 number of steps = 270 reward = 4.0\n",
      "DONE episode = 657 number of steps = 153 reward = 1.0\n",
      "DONE episode = 658 number of steps = 206 reward = 2.0\n",
      "DONE episode = 659 number of steps = 376 reward = 6.0\n",
      "DONE episode = 660 number of steps = 224 reward = 3.0\n",
      "DONE episode = 661 number of steps = 214 reward = 3.0\n",
      "DONE episode = 662 number of steps = 222 reward = 2.0\n",
      "DONE episode = 663 number of steps = 152 reward = 1.0\n",
      "DONE episode = 664 number of steps = 200 reward = 2.0\n",
      "DONE episode = 665 number of steps = 302 reward = 4.0\n",
      "DONE episode = 666 number of steps = 184 reward = 2.0\n",
      "DONE episode = 667 number of steps = 340 reward = 6.0\n",
      "DONE episode = 668 number of steps = 447 reward = 8.0\n",
      "DONE episode = 669 number of steps = 233 reward = 3.0\n",
      "DONE episode = 670 number of steps = 217 reward = 2.0\n",
      "DONE episode = 671 number of steps = 291 reward = 4.0\n",
      "DONE episode = 672 number of steps = 205 reward = 2.0\n",
      "DONE episode = 673 number of steps = 169 reward = 1.0\n",
      "DONE episode = 674 number of steps = 173 reward = 1.0\n",
      "DONE episode = 675 number of steps = 197 reward = 2.0\n",
      "DONE episode = 676 number of steps = 157 reward = 1.0\n",
      "DONE episode = 677 number of steps = 170 reward = 1.0\n",
      "DONE episode = 678 number of steps = 343 reward = 5.0\n",
      "DONE episode = 679 number of steps = 227 reward = 2.0\n",
      "DONE episode = 680 number of steps = 128 reward = 0.0\n",
      "DONE episode = 681 number of steps = 232 reward = 3.0\n",
      "DONE episode = 682 number of steps = 153 reward = 1.0\n",
      "DONE episode = 683 number of steps = 252 reward = 3.0\n",
      "DONE episode = 684 number of steps = 204 reward = 2.0\n",
      "DONE episode = 685 number of steps = 262 reward = 4.0\n",
      "DONE episode = 686 number of steps = 402 reward = 6.0\n",
      "DONE episode = 687 number of steps = 198 reward = 2.0\n",
      "DONE episode = 688 number of steps = 126 reward = 0.0\n",
      "DONE episode = 689 number of steps = 300 reward = 7.0\n",
      "DONE episode = 690 number of steps = 213 reward = 2.0\n",
      "DONE episode = 691 number of steps = 364 reward = 5.0\n",
      "DONE episode = 692 number of steps = 354 reward = 6.0\n",
      "DONE episode = 693 number of steps = 232 reward = 3.0\n",
      "DONE episode = 694 number of steps = 249 reward = 3.0\n",
      "DONE episode = 695 number of steps = 238 reward = 3.0\n",
      "DONE episode = 696 number of steps = 283 reward = 4.0\n",
      "DONE episode = 697 number of steps = 168 reward = 1.0\n",
      "DONE episode = 698 number of steps = 522 reward = 13.0\n",
      "DONE episode = 699 number of steps = 127 reward = 0.0\n",
      "DONE episode = 700 number of steps = 369 reward = 6.0\n",
      "DONE episode = 701 number of steps = 390 reward = 7.0\n",
      "DONE episode = 702 number of steps = 352 reward = 6.0\n",
      "DONE episode = 703 number of steps = 252 reward = 2.0\n",
      "DONE episode = 704 number of steps = 190 reward = 2.0\n",
      "DONE episode = 705 number of steps = 126 reward = 0.0\n",
      "DONE episode = 706 number of steps = 205 reward = 2.0\n",
      "DONE episode = 707 number of steps = 198 reward = 2.0\n",
      "DONE episode = 708 number of steps = 295 reward = 4.0\n",
      "DONE episode = 709 number of steps = 203 reward = 2.0\n",
      "DONE episode = 710 number of steps = 232 reward = 3.0\n",
      "DONE episode = 711 number of steps = 208 reward = 2.0\n",
      "DONE episode = 712 number of steps = 234 reward = 3.0\n",
      "DONE episode = 713 number of steps = 318 reward = 5.0\n",
      "DONE episode = 714 number of steps = 154 reward = 1.0\n",
      "DONE episode = 715 number of steps = 206 reward = 2.0\n",
      "DONE episode = 716 number of steps = 154 reward = 1.0\n",
      "DONE episode = 717 number of steps = 279 reward = 4.0\n",
      "DONE episode = 718 number of steps = 249 reward = 3.0\n",
      "DONE episode = 719 number of steps = 381 reward = 6.0\n",
      "DONE episode = 720 number of steps = 396 reward = 6.0\n",
      "DONE episode = 721 number of steps = 255 reward = 3.0\n",
      "DONE episode = 722 number of steps = 231 reward = 3.0\n",
      "DONE episode = 723 number of steps = 273 reward = 4.0\n",
      "DONE episode = 724 number of steps = 308 reward = 4.0\n",
      "DONE episode = 725 number of steps = 155 reward = 1.0\n",
      "DONE episode = 726 number of steps = 223 reward = 2.0\n",
      "DONE episode = 727 number of steps = 277 reward = 4.0\n",
      "DONE episode = 728 number of steps = 376 reward = 7.0\n",
      "DONE episode = 729 number of steps = 176 reward = 1.0\n",
      "DONE episode = 730 number of steps = 201 reward = 2.0\n",
      "DONE episode = 731 number of steps = 370 reward = 6.0\n",
      "DONE episode = 732 number of steps = 324 reward = 4.0\n",
      "DONE episode = 733 number of steps = 210 reward = 2.0\n",
      "DONE episode = 734 number of steps = 257 reward = 4.0\n",
      "DONE episode = 735 number of steps = 267 reward = 4.0\n",
      "DONE episode = 736 number of steps = 238 reward = 3.0\n",
      "DONE episode = 737 number of steps = 159 reward = 1.0\n",
      "DONE episode = 738 number of steps = 203 reward = 2.0\n",
      "DONE episode = 739 number of steps = 248 reward = 3.0\n",
      "DONE episode = 740 number of steps = 270 reward = 3.0\n",
      "DONE episode = 741 number of steps = 172 reward = 1.0\n",
      "DONE episode = 742 number of steps = 255 reward = 4.0\n",
      "DONE episode = 743 number of steps = 174 reward = 1.0\n",
      "DONE episode = 744 number of steps = 177 reward = 1.0\n",
      "DONE episode = 745 number of steps = 279 reward = 3.0\n",
      "DONE episode = 746 number of steps = 312 reward = 5.0\n",
      "DONE episode = 747 number of steps = 207 reward = 2.0\n",
      "DONE episode = 748 number of steps = 198 reward = 2.0\n",
      "DONE episode = 749 number of steps = 350 reward = 5.0\n",
      "DONE episode = 750 number of steps = 471 reward = 8.0\n",
      "DONE episode = 751 number of steps = 207 reward = 2.0\n",
      "DONE episode = 752 number of steps = 349 reward = 5.0\n",
      "DONE episode = 753 number of steps = 289 reward = 5.0\n",
      "DONE episode = 754 number of steps = 353 reward = 6.0\n",
      "DONE episode = 755 number of steps = 203 reward = 2.0\n",
      "DONE episode = 756 number of steps = 133 reward = 0.0\n",
      "DONE episode = 757 number of steps = 236 reward = 3.0\n",
      "DONE episode = 758 number of steps = 221 reward = 2.0\n",
      "DONE episode = 759 number of steps = 256 reward = 3.0\n",
      "DONE episode = 760 number of steps = 409 reward = 7.0\n",
      "DONE episode = 761 number of steps = 262 reward = 3.0\n",
      "DONE episode = 762 number of steps = 250 reward = 3.0\n",
      "DONE episode = 763 number of steps = 185 reward = 2.0\n",
      "DONE episode = 764 number of steps = 206 reward = 2.0\n",
      "DONE episode = 765 number of steps = 259 reward = 3.0\n",
      "DONE episode = 766 number of steps = 250 reward = 3.0\n",
      "DONE episode = 767 number of steps = 235 reward = 3.0\n",
      "DONE episode = 768 number of steps = 176 reward = 1.0\n",
      "DONE episode = 769 number of steps = 262 reward = 4.0\n",
      "DONE episode = 770 number of steps = 215 reward = 3.0\n",
      "DONE episode = 771 number of steps = 377 reward = 6.0\n",
      "DONE episode = 772 number of steps = 264 reward = 3.0\n",
      "DONE episode = 773 number of steps = 288 reward = 4.0\n",
      "DONE episode = 774 number of steps = 309 reward = 4.0\n",
      "DONE episode = 775 number of steps = 224 reward = 2.0\n",
      "DONE episode = 776 number of steps = 271 reward = 3.0\n",
      "DONE episode = 777 number of steps = 156 reward = 1.0\n",
      "DONE episode = 778 number of steps = 224 reward = 2.0\n",
      "DONE episode = 779 number of steps = 156 reward = 1.0\n",
      "DONE episode = 780 number of steps = 260 reward = 4.0\n",
      "DONE episode = 781 number of steps = 308 reward = 5.0\n",
      "DONE episode = 782 number of steps = 327 reward = 5.0\n",
      "DONE episode = 783 number of steps = 226 reward = 2.0\n",
      "DONE episode = 784 number of steps = 279 reward = 4.0\n",
      "DONE episode = 785 number of steps = 181 reward = 2.0\n",
      "DONE episode = 786 number of steps = 171 reward = 1.0\n",
      "DONE episode = 787 number of steps = 395 reward = 7.0\n",
      "DONE episode = 788 number of steps = 284 reward = 4.0\n",
      "DONE episode = 789 number of steps = 252 reward = 3.0\n",
      "DONE episode = 790 number of steps = 299 reward = 4.0\n",
      "DONE episode = 791 number of steps = 158 reward = 1.0\n",
      "DONE episode = 792 number of steps = 254 reward = 3.0\n",
      "DONE episode = 793 number of steps = 221 reward = 2.0\n",
      "DONE episode = 794 number of steps = 185 reward = 2.0\n",
      "DONE episode = 795 number of steps = 351 reward = 5.0\n",
      "DONE episode = 796 number of steps = 229 reward = 3.0\n",
      "DONE episode = 797 number of steps = 230 reward = 3.0\n",
      "DONE episode = 798 number of steps = 276 reward = 4.0\n",
      "DONE episode = 799 number of steps = 224 reward = 2.0\n",
      "DONE episode = 800 number of steps = 155 reward = 1.0\n",
      "DONE episode = 801 number of steps = 152 reward = 1.0\n",
      "DONE episode = 802 number of steps = 158 reward = 1.0\n",
      "DONE episode = 803 number of steps = 390 reward = 13.0\n",
      "DONE episode = 804 number of steps = 153 reward = 1.0\n",
      "DONE episode = 805 number of steps = 204 reward = 2.0\n",
      "DONE episode = 806 number of steps = 283 reward = 4.0\n",
      "DONE episode = 807 number of steps = 179 reward = 2.0\n",
      "DONE episode = 808 number of steps = 261 reward = 4.0\n",
      "DONE episode = 809 number of steps = 206 reward = 2.0\n",
      "DONE episode = 810 number of steps = 264 reward = 4.0\n",
      "DONE episode = 811 number of steps = 226 reward = 2.0\n",
      "DONE episode = 812 number of steps = 283 reward = 4.0\n",
      "DONE episode = 813 number of steps = 246 reward = 4.0\n",
      "DONE episode = 814 number of steps = 270 reward = 3.0\n",
      "DONE episode = 815 number of steps = 330 reward = 5.0\n",
      "DONE episode = 816 number of steps = 306 reward = 4.0\n",
      "DONE episode = 817 number of steps = 256 reward = 3.0\n",
      "DONE episode = 818 number of steps = 279 reward = 5.0\n",
      "DONE episode = 819 number of steps = 323 reward = 6.0\n",
      "DONE episode = 820 number of steps = 183 reward = 2.0\n",
      "DONE episode = 821 number of steps = 304 reward = 4.0\n",
      "DONE episode = 822 number of steps = 294 reward = 4.0\n",
      "DONE episode = 823 number of steps = 181 reward = 1.0\n",
      "DONE episode = 824 number of steps = 275 reward = 3.0\n",
      "DONE episode = 825 number of steps = 295 reward = 4.0\n",
      "DONE episode = 826 number of steps = 396 reward = 6.0\n",
      "DONE episode = 827 number of steps = 206 reward = 2.0\n",
      "DONE episode = 828 number of steps = 258 reward = 4.0\n",
      "DONE episode = 829 number of steps = 251 reward = 3.0\n",
      "DONE episode = 830 number of steps = 331 reward = 5.0\n",
      "DONE episode = 831 number of steps = 273 reward = 3.0\n",
      "DONE episode = 832 number of steps = 413 reward = 7.0\n",
      "DONE episode = 833 number of steps = 400 reward = 6.0\n",
      "DONE episode = 834 number of steps = 333 reward = 5.0\n",
      "DONE episode = 835 number of steps = 331 reward = 5.0\n",
      "DONE episode = 836 number of steps = 293 reward = 5.0\n",
      "DONE episode = 837 number of steps = 224 reward = 3.0\n",
      "DONE episode = 838 number of steps = 189 reward = 2.0\n",
      "DONE episode = 839 number of steps = 424 reward = 8.0\n",
      "DONE episode = 840 number of steps = 153 reward = 1.0\n",
      "DONE episode = 841 number of steps = 173 reward = 1.0\n",
      "DONE episode = 842 number of steps = 340 reward = 6.0\n",
      "DONE episode = 843 number of steps = 277 reward = 4.0\n",
      "DONE episode = 844 number of steps = 253 reward = 3.0\n",
      "DONE episode = 845 number of steps = 255 reward = 3.0\n",
      "DONE episode = 846 number of steps = 233 reward = 3.0\n",
      "DONE episode = 847 number of steps = 235 reward = 3.0\n",
      "DONE episode = 848 number of steps = 321 reward = 4.0\n",
      "DONE episode = 849 number of steps = 183 reward = 2.0\n",
      "DONE episode = 850 number of steps = 202 reward = 2.0\n",
      "DONE episode = 851 number of steps = 252 reward = 3.0\n",
      "DONE episode = 852 number of steps = 351 reward = 5.0\n",
      "DONE episode = 853 number of steps = 282 reward = 4.0\n",
      "DONE episode = 854 number of steps = 250 reward = 3.0\n",
      "DONE episode = 855 number of steps = 356 reward = 6.0\n",
      "DONE episode = 856 number of steps = 191 reward = 2.0\n",
      "DONE episode = 857 number of steps = 250 reward = 3.0\n",
      "DONE episode = 858 number of steps = 392 reward = 6.0\n",
      "DONE episode = 859 number of steps = 266 reward = 4.0\n",
      "DONE episode = 860 number of steps = 125 reward = 0.0\n",
      "DONE episode = 861 number of steps = 171 reward = 1.0\n",
      "DONE episode = 862 number of steps = 305 reward = 5.0\n",
      "DONE episode = 863 number of steps = 532 reward = 10.0\n",
      "DONE episode = 864 number of steps = 318 reward = 5.0\n",
      "DONE episode = 865 number of steps = 347 reward = 6.0\n",
      "DONE episode = 866 number of steps = 260 reward = 4.0\n",
      "DONE episode = 867 number of steps = 128 reward = 0.0\n",
      "DONE episode = 868 number of steps = 240 reward = 3.0\n",
      "DONE episode = 869 number of steps = 335 reward = 5.0\n",
      "DONE episode = 870 number of steps = 230 reward = 3.0\n",
      "DONE episode = 871 number of steps = 192 reward = 2.0\n",
      "DONE episode = 872 number of steps = 297 reward = 4.0\n",
      "DONE episode = 873 number of steps = 218 reward = 2.0\n",
      "DONE episode = 874 number of steps = 236 reward = 3.0\n",
      "DONE episode = 875 number of steps = 232 reward = 2.0\n",
      "DONE episode = 876 number of steps = 389 reward = 7.0\n",
      "DONE episode = 877 number of steps = 204 reward = 2.0\n",
      "DONE episode = 878 number of steps = 313 reward = 5.0\n",
      "DONE episode = 879 number of steps = 298 reward = 4.0\n",
      "DONE episode = 880 number of steps = 200 reward = 2.0\n",
      "DONE episode = 881 number of steps = 296 reward = 4.0\n",
      "DONE episode = 882 number of steps = 378 reward = 6.0\n",
      "DONE episode = 883 number of steps = 306 reward = 4.0\n",
      "DONE episode = 884 number of steps = 293 reward = 4.0\n",
      "DONE episode = 885 number of steps = 265 reward = 4.0\n",
      "DONE episode = 886 number of steps = 260 reward = 4.0\n",
      "DONE episode = 887 number of steps = 354 reward = 8.0\n",
      "DONE episode = 888 number of steps = 181 reward = 2.0\n",
      "DONE episode = 889 number of steps = 376 reward = 7.0\n",
      "DONE episode = 890 number of steps = 199 reward = 2.0\n",
      "DONE episode = 891 number of steps = 424 reward = 7.0\n",
      "DONE episode = 892 number of steps = 327 reward = 5.0\n",
      "DONE episode = 893 number of steps = 216 reward = 3.0\n",
      "DONE episode = 894 number of steps = 127 reward = 0.0\n",
      "DONE episode = 895 number of steps = 361 reward = 6.0\n",
      "DONE episode = 896 number of steps = 248 reward = 3.0\n",
      "DONE episode = 897 number of steps = 246 reward = 3.0\n",
      "DONE episode = 898 number of steps = 383 reward = 7.0\n",
      "DONE episode = 899 number of steps = 267 reward = 4.0\n",
      "DONE episode = 900 number of steps = 314 reward = 4.0\n",
      "DONE episode = 901 number of steps = 231 reward = 3.0\n",
      "DONE episode = 902 number of steps = 262 reward = 4.0\n",
      "DONE episode = 903 number of steps = 398 reward = 6.0\n",
      "DONE episode = 904 number of steps = 298 reward = 5.0\n",
      "DONE episode = 905 number of steps = 305 reward = 5.0\n",
      "DONE episode = 906 number of steps = 335 reward = 5.0\n",
      "DONE episode = 907 number of steps = 234 reward = 3.0\n",
      "DONE episode = 908 number of steps = 320 reward = 5.0\n",
      "DONE episode = 909 number of steps = 186 reward = 2.0\n",
      "DONE episode = 910 number of steps = 229 reward = 3.0\n",
      "DONE episode = 911 number of steps = 283 reward = 5.0\n",
      "DONE episode = 912 number of steps = 227 reward = 3.0\n",
      "DONE episode = 913 number of steps = 229 reward = 3.0\n",
      "DONE episode = 914 number of steps = 180 reward = 2.0\n",
      "DONE episode = 915 number of steps = 362 reward = 6.0\n",
      "DONE episode = 916 number of steps = 375 reward = 8.0\n",
      "DONE episode = 917 number of steps = 183 reward = 2.0\n",
      "DONE episode = 918 number of steps = 326 reward = 7.0\n",
      "DONE episode = 919 number of steps = 230 reward = 3.0\n",
      "DONE episode = 920 number of steps = 302 reward = 4.0\n",
      "DONE episode = 921 number of steps = 279 reward = 4.0\n",
      "DONE episode = 922 number of steps = 227 reward = 3.0\n",
      "DONE episode = 923 number of steps = 187 reward = 2.0\n",
      "DONE episode = 924 number of steps = 322 reward = 4.0\n",
      "DONE episode = 925 number of steps = 186 reward = 2.0\n",
      "DONE episode = 926 number of steps = 321 reward = 4.0\n",
      "DONE episode = 927 number of steps = 411 reward = 7.0\n",
      "DONE episode = 928 number of steps = 308 reward = 5.0\n",
      "DONE episode = 929 number of steps = 179 reward = 1.0\n",
      "DONE episode = 930 number of steps = 296 reward = 4.0\n",
      "DONE episode = 931 number of steps = 230 reward = 3.0\n",
      "DONE episode = 932 number of steps = 335 reward = 6.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if let_training:\n",
    "    training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c509d-475a-46f4-ad95-b556fd1e7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "if let_play:\n",
    "    tot_rewards = 0\n",
    "    tot_steps = 0\n",
    "    max_reward = 0\n",
    "    rewards = []\n",
    "    steps_arr = []\n",
    "    \n",
    "    if rec_video:\n",
    "        \n",
    "        path_v = \"{}_video\".format(game_name)\n",
    "        index_v = 0\n",
    "\n",
    "        directory_n = \"{}\".format(path_v)\n",
    "        directory = Path(\"./{}\".format(directory_n))\n",
    "        while directory.exists():\n",
    "            index_v += 1\n",
    "            directory_n = \"{}-{}\".format(path_v, index_v)\n",
    "            directory = Path(\"./{}\".format(directory_n))\n",
    "        \n",
    "        env = RecordVideo(env, directory)\n",
    "    for i in range(0, games_to_play):\n",
    "        reward, steps = play()\n",
    "        tot_rewards += reward\n",
    "        tot_steps += steps\n",
    "        rewards.append(reward)\n",
    "        steps_arr.append(steps)\n",
    "        \n",
    "        if max_reward < reward:\n",
    "            max_reward = reward\n",
    "    \n",
    "    ext = \"png\"\n",
    "    name_plot_steps = \"{} Playing Episodes Steps.{}\".format(game_name, ext)\n",
    "    name_plot_rewards = \"{} Playing Episodes Rewards.{}\".format(game_name, ext)\n",
    "    file_plot_1 = Path(name_plot_steps)\n",
    "    i = 1\n",
    "    while file_plot_1.exists():\n",
    "        i += 1\n",
    "        name_plot_steps = \"{} Playing Episodes Steps_{}.{}\".format(game_name, i, ext)\n",
    "        name_plot_rewards = \"{} Playing Episodes Rewards_{}.{}\".format(game_name, i, ext)\n",
    "        file_plot_1 = Path(name_plot_steps)\n",
    "\n",
    "    plot_result(\"Game\", \"Steps\", range(1, games_to_play + 1), steps_arr, name_plot_steps)\n",
    "    plot_result(\"Game\", \"Rewards\", range(1, games_to_play + 1), rewards, name_plot_rewards)\n",
    "    \n",
    "    print(\"Max REWARD {}\".format(max_reward))\n",
    "    print(\"Average REWARD {}\".format(tot_rewards/games_to_play))\n",
    "    print(\"Average STEPS {}\".format(tot_steps/games_to_play))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1591b57d-fb25-42cc-a552-6602dfd4d161",
   "metadata": {},
   "source": [
    "# Video demostration of the agent's ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391deda-eabc-427a-9c34-8ed1532f1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "video = \"./asterix_video/rl-video-episode-0.mp4\".format(game_name) \n",
    "Video(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77KRHlFFhhNG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1665074736231,
     "user": {
      "displayName": "stefano romeo",
      "userId": "04091680816877563929"
     },
     "user_tz": -120
    },
    "id": "77KRHlFFhhNG",
    "outputId": "dba6b184-6bec-447d-abef-369969bbf0ad"
   },
   "outputs": [],
   "source": [
    "# To download the weights from Google Colab\n",
    "#!rm -r ./sample_data\n",
    "#!zip -r /content/Asterix_dueling.zip /content\n",
    "#from google.colab import files\n",
    "#files.download('Asterix_dueling.zip')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
